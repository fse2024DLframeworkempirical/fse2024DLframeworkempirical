Title,Body,CreaDate,Tags,State,Reactions,Comments,Link,Keywords,Topic_id,,,,,,
dataloader workers causes cpu memory parent process replicated worker processes,editor note known workaround lists instead something else tensor directly issues issuecomment numpy array fixes fork start method issues issuecomment details cpu memory dataloader workers reproduce following snippet expected behavior cpu memory gradually start increasing eventually filling whole ram process starts around fills whole available system workers ram usage constant environment additionanl info around million images dataset paths loaded single presented snippet also multiple versions effect,2018-10-29T13:23:59Z,"high priority, module: dependency bug, module: multiprocessing, module: dataloader, module: molly-guard, module: memory usage, triaged",OPEN,102,132,https://github.com/pytorch/pytorch/issues/13246,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子资源调度不合理,,,,,DataLoader num_workers>0导致内存泄露
tensorboard valuebug duplicate plugins nanme projector,follow introduction happened cuda cudnn install pip however right,2019-07-10T07:53:56Z,"high priority, triage review, module: dependency bug, triaged, module: tensorboard",CLOSED,46,43,https://github.com/pytorch/pytorch/issues/22676,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",7,算子执行失败,,,,,tensorboard中的projector插件被重复加载
download speed issues channel,download speed slow downloading around kb mbit connection searched downloading found mirror lets mb download speed going enough bandwidth science originanl pip install files pythonhosted org packages bac bf ee post cp cp mu manylinux whl downloading pypi lcsb uni lu packages bac bf ee post cp cp mu manylinux whl seriously weird,2019-02-12T20:29:14Z,"module: dependency bug, triaged",OPEN,31,90,https://github.com/pytorch/pytorch/issues/17023,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架的下载问题,,,,,conda下载pytorch的速度问题
fp depthwise convolution slow gpu,tested ipython convolution much slower normal convolution supposed opposite ged fa cuda driver titan xp btw depthwise convolution slow cpu,2019-03-29T20:29:29Z,"high priority, module: dependency bug, module: performance, module: cudnn, module: cuda, module: convolution, triaged",OPEN,34,64,https://github.com/pytorch/pytorch/issues/18631,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子资源调度不合理,,,,,GPU上的depthwise convolution耗时多
installed bugs complaints incompatibility mkl libgomp multiprocessing,fairseq multi gpu training nlp models upgrading channel consistently message generated trying train gpus assume three repetitions incompatibliity means process generating one occur downgrade reproduce steps reproduce behavior install install fairseq environment source multi gpu training fairseq sample messages traces provide well expected behavior occur environment copy paste output environment collection script raw githubusercontent master collect env py checklist manually script version os linux ubuntu linux installed pip source command compiling source version cuda cudnn version also gpu models configuration tesla information yaml environment,2020-04-27T22:28:06Z,"high priority, module: dependency bug, module: build, module: multiprocessing, triaged, module: mkl, module: regression, has workaround, module: known issue",CLOSED,27,37,https://github.com/pytorch/pytorch/issues/37377,"reproduce, behavior, expected, concise, description, context, environment, concise_description, message, sample",14,框架中包的不兼容,,,,,pytorch中MKL and libgomp不兼容
attributebug module distutils attribute version setuptools,versions dev infra,2021-12-14T12:49:14Z,"module: dependency bug, module: ci, triaged, module: tensorboard",CLOSED,22,27,https://github.com/pytorch/pytorch/issues/69894,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子功能不完善,,,,,distutils缺乏属性version
multi gpu example freeze killable,running multiple gpus freeze killable even kill reboot removes process inside docker container nvidia docker freezes docker nvidia nvidia docker issues reproduce steps reproduce behavior install following multiple gpus expected behavior training environment collecting environment information version post debug cuda os ubuntu lts gcc version crosstool ng fa cb cmake version collect version cuda available cuda runtime version gpu models configuration gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla nvidia driver version cudnn version linux gnu libcudnn versions libraries pip numpy mkl defaults mkl fft py mkl random py nomkl defaults numexpr py nomklhaa nomkl defaults py cuda cudnn pip py,2019-08-09T13:27:51Z,"module: dependency bug, module: multi-gpu, module: multiprocessing, module: cuda, triaged, module: deadlock, has workaround, module: data parallel, quansight-nack",OPEN,5,54,https://github.com/pytorch/pytorch/issues/24081,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,模型运行卡顿,,,,,多GPU上运行pytorch被冻结且不可杀死
process fails assertion magma cuda,training process dies following reproduce steps reproduce behavior training fairly network almost gpu memory tesla dataloader rendering opengl returning cpu tensors sample messages traces provide well expected behavior program crash memory return environment version debug cuda os ubuntu lts gcc version ubuntu ubuntu cmake version version version cuda available cuda runtime version collect gpu models configuration gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla nvidia driver version cudnn version collect versions libraries pip numpy pip pip torchfile pip torchnet pip,2019-09-12T20:29:33Z,"module: dependency bug, needs reproduction, module: crash, triaged, module: linear algebra",OPEN,9,22,https://github.com/pytorch/pytorch/issues/26120,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,模型训练出错,,,,,训练时出现断言错误
dependency issues tensorboard module nanmed past module nanmed pil,pip install future fixes problem rather added requirements txt mentioned explicitly docs however another happens docs mention dependency pillow add add images think break import,2019-07-01T07:26:40Z,"module: dependency bug, triaged",CLOSED,11,11,https://github.com/pytorch/pytorch/issues/22389,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架依赖缺失,,,,,tensorboard缺少PIL和past模块
dynanmo exc unsupported dynanmic shapes arange,giving try opennmt py lines logs getting minified repro response,2022-12-06T17:31:39Z,"bug, triaged, oncall: pt2",OPEN,0,53,https://github.com/pytorch/pytorch/issues/93468,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架特性不支持,,,,,不支持动态形状arange
ppc le,looks libth tries dispatch vsx specific versions functions defined,2017-03-05T00:09:02Z,"todo, module: dependency bug",CLOSED,0,51,https://github.com/pytorch/pytorch/issues/922,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,框架在特殊架构下不被支持,,,,,pytorch无法在ppc64le上被构建
cudnn status supported matrix input,following adapted discuss org cudnn status supported occurs apply autograd grad compute high differentiation gives cudnn status supported running cudnn cuda running convolutions individually works running smaller input matrices pretty sure problem also occurs sizes receive following posted question forums discuss org cudnn status supported matrix input reproduced,2017-12-11T07:58:39Z,"high priority, triage review, module: dependency bug, module: cudnn, triaged",CLOSED,1,47,https://github.com/pytorch/pytorch/issues/4107,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,模型CDNN出错,,,,,大矩阵输入导致CDNN状态出错
crash reading pandas parquet importing,importing reading parquet pandas script randomly fails segmentation problem occurs importing seems related seems resolve able produce anymore steps reproduce behavior install effected minimal example producing import import pandas pd nanme df pd read parquet summary par done data found eu west amazonanws segfault public bucket summary par unluckily never able fetch stacktrace ran script thousands times gdb never fails gdb even though fails around times normally executing concise description expected happen environment version debug cuda os ubuntu lts gcc version ubuntu ubuntu cmake version version version cuda available cuda runtime version collect gpu models configuration gpu quadro nvidia driver version cudnn version probably one following linux gnu libcudnn linux gnu libcudnn static cuda targets linux libcudnn one cuda targets linux libcudnn static cuda targets linux libcudnn cuda targets linux libcudnn cuda targets linux libcudnn cuda targets linux libcudnn static versions libraries pip collect,2018-10-24T08:39:14Z,"high priority, module: dependency bug, triaged, module: pybind",CLOSED,8,12,https://github.com/pytorch/pytorch/issues/13039,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子执行失败,,,,,导入pyTorch后读取pandas parquet文件时崩溃
cudnn cudnn status internanl cudnn benchmark,hi running encoder decoder architecture problem cudnn include receive cudnn status internanl listed comment cudnn without problems system configurations listed version os ubuntu installed pip version cuda cudnn version gpu models configuration nvidia gpu titan additionanl context rm rf nv rebuilt cuda cudnn reinstalled still cannot work thanks help,2019-02-07T01:10:29Z,"module: dependency bug, module: cudnn, triaged",CLOSED,0,41,https://github.com/pytorch/pytorch/issues/16831,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,框架CDNN出错,,,,,
happening poetry,hello trying install poetry getting following poetry installed pip added following details related version poetry version os details version cpython poetry version os windows version os poetry creation virtual environment poetry disabled pyproject toml looks versions collecting environment information version debug cuda rocm os microsoft windows single language gcc version collect clang version collect cmake version collect libc version version tags db ff jun msc bit amd bit runtime platform windows sp cuda available cuda runtime version gpu models configuration gpu nvidia geforce ti nvidia driver version cudnn version program files nvidia gpu computing toolkit cuda bin cudnn ops train dll hip runtime version miopen runtime version versions libraries pip mypy pip mypy extensions pip numpy pip torchsummary collect,2021-09-05T04:14:21Z,"module: dependency bug, oncall: binaries, releng, triaged",CLOSED,5,19,https://github.com/pytorch/pytorch/issues/64520,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架安装失败,,,,,Poetry安装torch和torchvision失败
runtimebug cuda illegal memory access encountered channels,illegal memory access trying train mnansnet version apex channels concise description reproduce steps reproduce behavior apex imagenet example launch nproc amp py mnansnet workers channels opt intel nvme imagenet data sample messages traces provide well traceback amp py module amp py train train loader model criterion optimizer epoch amp py train scaled loss backward tstand ananconda contextlib py exit next gen tstand ananconda packages apex amp handle py scale loss optimizer post amp backward loss scaler tstand ananconda packages apex amp process optimizer py post backward master weights post backward models masters scaler params stashed grads tstand ananconda packages apex amp process optimizer py post backward models masters scale override grads scale stashed scale scale tstand ananconda packages apex amp scaler py unscale stashed scale stashed scale tstand ananconda packages apex amp scaler py unscale stashed dynanmic tstand ananconda packages apex amp scaler py axpby overflow cpu float model grad float runtimebug cuda illegal memory access encountered terminante called throwing instance,2020-04-28T19:35:49Z,"high priority, module: dependency bug, oncall: binaries, module: cudnn, module: cuda, triaged",OPEN,6,12,https://github.com/pytorch/pytorch/issues/37449,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,框架CUDA出错,,,,,CUDA不合法的内存访问
conv backward calculated,convolution backward result correct happens specific combinantion input input output channels groups stride padding mentioned change val backward output correct gives results grad initialized hence output backward equal forward steps reproduce behavior result adding channel result tensor expected behavior tensor version os linux linux installed pip source pip command compiling source nan version cuda cudnn version cpu gpu models configuration nan information nan,2021-01-04T14:40:41Z,"high priority, module: dependency bug, module: nn, module: convolution, triaged, module: correctness (silent)",CLOSED,2,27,https://github.com/pytorch/pytorch/issues/50042,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子输出结果错误,,,,,组卷积后向路径结果不正确
specific batchsize illegal memory access,question help support ask forums discuss org submitting feature request preface feature request submitting report following details description specific batchsize illegal memory access running batchsize illegal memory access batchsize example system info environment collection script version debug cuda os ubuntu lts gcc version ubuntu ubuntu cmake version version version cuda available cuda runtime version collect gpu models configuration gpu geforce titan nvidia driver version cudnn version probably one following cuda cuda libcudnn cuda cuda libcudnn static cuda libcudnn cuda libcudnn cuda libcudnn old cuda libcudnn static cudnn libcudnn cudnn libcudnn static cudnn libcudnn cudnn libcudnn cudnn libcudnn cudnn libcudnn static versions libraries pip numpy pip pip collect caffe installed pip source pip command compiling source sudo pip install wheel os ubuntu version version cuda cudnn version gpu models configuration gcc version compiling source cmake version versions libraries,2018-05-01T08:00:28Z,"module: dependency bug, module: cudnn, module: cuda, triaged, module: assert failure",CLOSED,1,26,https://github.com/pytorch/pytorch/issues/7129,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子资源调度不合理,,,,,torch.autograd.Variable特定batch_size非法访问内存
cuda cache doubles second batch causes oom cache,trying train fcn net sized voc data batch gpu note originanl model trained card question forum discuss org fcn training voc data cuda oom description cuda cache doubles second batch causes oom example example following reproducible gist vlasenkov ccc afcf baa dc produces larger inputs results oom also cache help however allocated released adding help system info,2019-02-01T11:11:51Z,"module: dependency bug, module: cudnn, triaged",OPEN,4,13,https://github.com/pytorch/pytorch/issues/16655,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,框架CUDA出错,,,,,CUDA不清空cache导致内存耗尽
intellisense windows report real message default,hi guys cmake cmakelists txt org libtorch visual studio output tensor works exception happened simply backward images githubusercontent dfc cmakelists txt org images githubusercontent ed reproduce steps reproduce behavior download libtorch debug cpu libtorch win shared deps debug cpu zip cmake org recommended simple backward function exception environment win microsoft visual studio professionanl version libtorch win shared deps debug cpu cmake anything wants pose reproduced solve welcome,2020-11-13T06:21:04Z,"high priority, module: dependency bug, module: windows, module: error checking, module: molly-guard, triaged",CLOSED,0,26,https://github.com/pytorch/pytorch/issues/47900,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,模型训练出错,,,,,后向计算出现异常
inductor leaks memory,inductor backend compile leaks memory every compiled mmseg model currently causing model crash every hours eats ram crashes due oom backends aot eager cudagraphs show completely flat memory usage repro torchdrive notebooks inductor memory ipynb showed month ago well latest release much idea repro showing leaks kb seems unrelated aotautograd issues since occurs inductor explicitly calling gc collect measuring memory usage help versions version cu debug cuda rocm os arch linux gcc version gcc clang version cmake version version libc version glibc version dec gcc bit runtime platform linux arch glibc cuda available cuda runtime version cuda module loading set lazy gpu models configuration gpu nvidia geforce rtx gpu nvidia geforce rtx ti gpu nvidia geforce rtx nvidia driver version cudnn version probably one following opt cudnn libcudnn libcudnn libcudnn adv infer libcudnn adv train libcudnn cnn infer libcudnn cnn train libcudnn ops infer libcudnn ops train hip runtime version miopen runtime version xnnpack available cpu architecture cpu op,2023-03-16T06:27:19Z,"high priority, bug, triaged, module: third_party, module: inductor",CLOSED,4,9,https://github.com/pytorch/pytorch/issues/96937,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,模型资源调度不合理,,,,,内存泄漏
inductor incorrect result vision maskrcnn,vision maskrcnn torchbench incorrect result inductor runtimebug tensor must match tensor non singleton dimension reproduce command benchmarks dynanmo torchbench py accuracy float dcpu inductor settings float inductor skip quiet vision maskrcnn logs cpu eval vision maskrcnn traceback benchmarks dynanmo torchbench py module torchbenchmarkrunner originanl bzheng workspace benchmarks dynanmo common py return maybe fresh cache cold start latency bzheng workspace benchmarks dynanmo common py inner return fn kwargs bzheng workspace benchmarks dynanmo common py runner one model bzheng workspace benchmarks dynanmo common py one model status accuracy bzheng workspace benchmarks dynanmo common py accuracy bzheng workspace dynanmo py return len ref len res bzheng workspace dynanmo py genexpr ai bi fp refi cos similarity tol equal nann exact dtype bzheng workspace dynanmo py bzheng workspace dynanmo py allclose ref res atol tol rtol tol equal nann equal nann runtimebug tensor must match tensor non singleton dimension cpu gmean nannx nannx vision maskrcnn gmean nannx nannx gmean nannx nannx gmean nannx nannx minified repro benchmarks dynanmo torchbench py accuracy float dcpu inductor settings float inductor skip quiet,2022-11-01T16:07:54Z,"bug, triaged",OPEN,0,24,https://github.com/pytorch/pytorch/issues/93598,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,模型运行结果错误,,,,,torchbench的Vision_maskrcnn结果不正确
sigmoid behaves inconsistently bit nann inputs,description applying sigmoid doubletensor containing nanns result contain nanns applying sigmoid floattensor containing nanns values resulting tensor nann values small number proportion seems depend deterministically input tensor given input proportion vary one machine another nanns small valued numbers appear periodic pattern output tensor manifest everything done gpu example output input tensor containing nann sigmoid proportion nann result bit input proportion nann result bit input input tensor containing nann sigmoid proportion nann result bit input proportion nann result bit input input tensor containing nann sigmoid proportion nann result bit input proportion nann result bit input input tensor containing nann sigmoid proportion nann result bit input proportion nann result bit input system info version debug cuda os ubuntu lts gcc version ubuntu ubuntu cmake version version version cuda available cuda runtime version gpu models configuration gpu geforce nvidia driver version cudnn version versions libraries pip collect,2018-11-01T13:38:01Z,"high priority, module: dependency bug, module: cpu, module: tests, triaged, module: numerical-reproducibility, module: third_party",CLOSED,2,16,https://github.com/pytorch/pytorch/issues/13448,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,算子输出结果不一致,,,,,torch.sigmoid中32和64bit的NAN数据输出不一致
undefined symbol sormqr,compiled cuda support ubuntu gcc install process terminanted critical bugs already installed machine works importing following import tudor ananconda packages init pyc module sys setdlopenflags flags rtld global flags rtld import nanme nanme importbug tudor ananconda packages libthc undefined symbol sormqr,2017-01-19T14:30:31Z,"high priority, module: dependency bug",CLOSED,0,23,https://github.com/pytorch/pytorch/issues/507,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,框架导入错误,,,,,出现未定义的符号
cublas runtime fp matrix multiplication,sorry duplicate existing report easily find reproduce also fails cublasgemmex called via extension stacktrace works expected fp also works fp matrix slightly smaller environment os linux version cuda cudnn version gpu,2019-08-08T17:07:46Z,"module: dependency bug, module: cuda, triaged, module: half",CLOSED,2,15,https://github.com/pytorch/pytorch/issues/24018,"behavior, result, reproduce, expected, cpu, numpy, expected_behavior, index, different, detail",13,框架对CUDA支持不完善,,,,,CUDA加速库中的cuBLAS runtime error
lu solve batched matrices gpu,lu solve throws provided batched data gpu works unbachted data data cpu tiny matrices reproduce runtimebug traceback pool twoertwe test py module lu data pivots lu lu solve lu data pivots runtimebug cuda invalid configuration argument expected behavior work environment,2020-04-20T14:22:28Z,"high priority, module: dependency bug, module: internals, module: cuda, triaged, module: linear algebra",CLOSED,2,15,https://github.com/pytorch/pytorch/issues/36921,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,框架接口运行结果出错,,,,,lu_solve中批量处理大型矩阵出现错误
segmentation big enough grouped convolutions,environments big enough grouped convolution crashes segmentation executes output possible environments tests aws machines enough ram deal script os deep learning ami ubuntu version ami dd docker actually xlarge instances works ok xlarge crashes reproducing steps create new instance ec select os deep learning ami ubuntu version ami dd xlarge instance inside machine execute docker rm shm pwd workspace latest environment create segmentation steps changing instance xlarge work ok versions fact output crashing machine inside docker workspace collect env py collecting environment information version debug cuda rocm os ubuntu lts gcc version collect clang version collect cmake version collect libc version glibc version default jul gcc bit runtime platform linux aws debian buster sid cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda hip runtime version miopen runtime version versions libraries pip numpy pip pip torchelastic pip torchtext pip blas mkl cudatoolkit nvidia ffmpeg hf mkl mkl service py,2021-12-30T11:24:04Z,"high priority, module: dependency bug, module: crash, module: nn, module: convolution, triaged, module: mkldnn, has workaround",CLOSED,2,14,https://github.com/pytorch/pytorch/issues/70523,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子运行结果错误,,,,,大规模分组卷积发生错误
onnx export incorrect apple silicon mps,onnx asked post specific model conv batchnorm onnx export gpu mps model subsequent forward onnx match forward model cpu cuda export produce model matches note forwards mps cpu match example shows best effort minimum needs one feature batchnorm perform one forward eval model batchnorm versions collecting environment information version dev debug cuda rocm os macos gcc version collect clang version clang cmake version collect libc version version default mar clang bit runtime platform macos bit cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda hip runtime version miopen runtime version xnnpack available versions libraries pip numpy pip dev numpy py ab numpy base py dev py,2022-08-11T05:30:58Z,"bug, module: onnx, triaged, module: mps",CLOSED,0,22,https://github.com/pytorch/pytorch/issues/83230,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架在特殊架构下不被支持,,,,,M1上的ONNX导出不正确 
nn functionanl interpolation fails onnx export,hi everyone implementing segmentation model onnx convert caffe model following specified align corners variable explicitly encode decoder zip encodes decode layers decoder cat interpolate encode bilinear align corners encode return segmentation interpolate input shp bilinear align corners however onnx export model caffe following occurs packages onnx symbolic py userwarning onnx export failed upsample bilinear align corners supported warnings warn onnx export failed op msg supported traceback caffe py module export params store trained parameter weights inside model packages onnx init py export return export kwargs packages onnx py export proto export graph export params onnx opset version defer weight export operator export runtimebug onnx export failed export operator upsample bilinear strange since explicitly set align corners anyone help kind suggestion welcomed thanks kindly information,2018-08-12T03:59:18Z,"module: dependency bug, module: onnx, triaged",CLOSED,0,21,https://github.com/pytorch/pytorch/issues/10446,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,算子不支持框架特性,,,,,ONNX导出失败
set processor affinity physical core fork,may related background arise ray raylet process forked process importing raylet uses execvpe create worker processes taskset pc pgrep raylet shows current affinity worker processes inherits ray worker processes one physical core causing significant performance penanlty investigation boils fork usage minimal example taskset pc child pid shows bad affinity strace strace sched setaffinity also clearly shows nanrrowed script turns may specific linking environment libgomp without mkl reproduce mkl calls sched setaffinity mkl come source able investigate workaround multiple ways workaround fork importing install pypi version tested manually reset affinity child processes pin llvm openmp found comparing dependencies older normal environment may caused un pinned upgraded dependency versions mkl environment wei,2023-04-20T12:49:19Z,"high priority, triage review, module: dependency bug, oncall: binaries, triaged, module: mkl, module: third_party, module: intel",OPEN,1,17,https://github.com/pytorch/pytorch/issues/99625,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,框架资源调度不合理,,,,,Pytorch将处理器关联设置为第一个物理核心
guard failures model,addition finfo verified fixed remains several guard failures unique instances hierarchy failure guard failure provided specific point dealing input linear matmuls multihead attention block reproduce script automatically installs huggingface nothing install dependencies small gpus might dial number sequences fit chen weiwen,2022-06-28T02:22:35Z,"bug, triaged, oncall: pt2, module: dynamo",OPEN,0,20,https://github.com/pytorch/pytorch/issues/93774,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,模型出错,,,,,
tensorimpl de virtualization,motivation de virtualize tensorimpl methods much possible achieve performance gain common tensor methods numel sizes concerns xla relies tensorimpl methods numel sizes virtual implementation calculate values lazily requested rather eagerly change happens proposal support xla add virtual methods virtual numel virtual sizes virtual opaquetensorimpl tensorimpl add conditionanl branching numel sizes opaque virtual methods tensorimpl opaquetensorimpl benchmarks jit lstm sure adding conditionanl branching de virtualizing numel sizes perf regression cpu cuda cases ask xla move xlatensorimpl opaquetensorimpl ask override opaque methods lazy de virtualize tensorimpl subclasses core library example pr,2019-07-12T19:12:40Z,"high priority, module: dependency bug, module: internals, triaged, quansight-nack",OPEN,2,11,https://github.com/pytorch/pytorch/issues/22815,"new, thread, org, version, tutorial, assert, cpu, master, doc, internanl",6,算子功能不完善,,,,,TensorImpl 去虚拟化
memory batched inverse,seems limit number batches one invert gpu think limit corresponds properly available memory batched inverse gpu less availiable memory tested collab allows batched inverse reproduce cpu numpy runtimebug cuda runtime invalid configuration argument opt bld work thc thctensorcopy cu expected behavior works reduce batch dimension one randn cuda environment version dev debug cuda os ubuntu lts gcc version ubuntu ubuntu cmake version version version cuda available cuda runtime version gpu models configuration gpu geforce gpu geforce nvidia driver version cudnn version probably one following matlab bin glnxa libcudnn cuda libcudnn cuda libcudnn cuda libcudnn cuda libcudnn cuda libcudnn static versions libraries pip collect cuda soumith cuda guided filter pip magma cuda dev py cuda cudnn cuda pip pip,2018-10-29T22:41:49Z,"module: dependency bug, triaged",CLOSED,0,19,https://github.com/pytorch/pytorch/issues/13276,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子资源调度不合理,,,,,batched inverse导致内存错误
memory cpu gpu convolution layer,hello problem related memory cpu gpu ubuntu os cudnn memory usage increases every epoch memory occur cudnn cudnn slower cudnn gist nido ae fd dd manually select convolution algorithm revise master csrc cudnn conv cpp revised gist nido db bbf cca revision comment findalgorithm handle conv benchmark algo cudnnconvolutionfwdalgo fwdalg cudnn convolution fwd algo winograd nonfused cudnnconvolutionbwddataalgo bwddataalg cudnn convolution bwd data algo winograd nonfused cudnnconvolutionbwdfilteralgo bwdfilteralg cudnn convolution bwd filter algo winograd nonfused memory change forward backward data backward filter algorithm algorithm provided cudnn select cudnn convolution fwd algo winograd nonfused forward convolution newly added cudnn memory usage increases mb every epoch select cudnn convolution bwd data algo winograd nonfused backward data convolution newly added cudnn memory usage increases mb every epoch select cudnn convolution bwd filter algo winograd nonfused backward filter convolution newly added cudnn memory usage increases mb every epoch algorithms memory occur yet test experiment cudnn memory occur compile cudnn anybody experiences memory experiment setup os linux mint ubuntu cuda cudnn,2017-04-17T11:18:09Z,"high priority, module: dependency bug",CLOSED,1,15,https://github.com/pytorch/pytorch/issues/1272,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子资源调度不合理,,,,,卷积层内存泄漏
nn lstm gives nondeterministic results dropout multiple layers,concise description got non deterministic results model nn lstm dropout gpu even seeded everything backends cudnn deterministic also set backends cudnn enanbled results deterministic reproduce steps reproduce behavior backends cudnn deterministic random seed manual seed cuda manual seed np random seed define module nn lstm input hidden layers dropout bidirectionanl training defined module multiple times sample messages traces provide well expected behavior concise description expected happen training deterministic across different runs environment version post debug cuda os debian gnu linux stretch gcc version debian deb cmake version version version cuda available cuda runtime version collect gpu models configuration gpu tesla gpu tesla gpu tesla gpu tesla nvidia driver version cudnn version collect versions libraries pip numpy blas mkl mkl mkl service py bf mkl fft py mkl random py hd dba py cuda cudnn py additionanl context add context problem,2019-03-17T03:25:24Z,"high priority, module: dependency bug, module: cudnn, triaged",CLOSED,1,14,https://github.com/pytorch/pytorch/issues/18110,"version, mkl, py, cuda, pip, pypi, numpy, gpu, pypi_pypi, cudnn",7,模型结果错误,,,,,nn.LSTM给出不确定结果 
flattenparamswrapper breaks dynanmo faketensor wrapping,repro script,2022-09-24T00:07:01Z,"bug, triaged, module: fsdp, oncall: pt2",OPEN,0,18,https://github.com/pytorch/pytorch/issues/93688,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架中包的不兼容,,,,,使用FSDP和Dynamo时发生了冲突
nn init orthogonanl work multiprocessing,concise description reproduce steps reproduce behavior output test test test,2019-06-19T05:55:38Z,"module: dependency bug, module: multiprocessing, module: nn, triaged, module: initialization",OPEN,0,17,https://github.com/pytorch/pytorch/issues/21956,"reproduce, behavior, expected, concise, description, context, environment, concise_description, message, sample",14,算子执行失败,,,,,多处理过程中nn.init.orthogonal_不工作
default chosen cudnn convolution algorithm uses twice much memory,received internanl report situations default cudnn convolution algorithm selects uses twice much default convolution successful workaround set backends cudnn benchmark seemed let us pick memory efficient convolution setting somewhat inconvenient actual algorithm selected however nvprof traces without benchmark without benchmark benchmark facebook people workplace groups permalink,2018-10-31T19:43:24Z,"high priority, module: dependency bug, module: performance, module: cudnn, module: cuda, module: memory usage, triaged",CLOSED,1,13,https://github.com/pytorch/pytorch/issues/13412,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,框架资源调度不合理,,,,,V100默认选择的cuDNN卷积算法使用两倍的内存 
serverless aws lambda timeout cpuinfo,running classification model serverless aws lambda receiving following upon calling serverless function cpuinfo failed parse present procesors sys devices system cpu present misspell spelling copied message received appear spelling cpuinfo master linux processors suspected might something switched back made work reproduce steps reproduce behavior lambda following whl requirements txt pillow pyyaml download org whl cpu cp cp linux whl optionanl model predict arise cpuinfo failed parse present procesors sys devices system cpu present also timeout lambda regardless long timeout chosen process exited completing request available receive cpuinfo failed parse present procesors sys devices system cpu present timeout process exited completing request expected behavior expected prediction model every new uploaded environment version os linux linux installed pip source pip command compiling source version cuda cudnn version cuda gpu models configuration gpu information deployed serverless aws lambda memory second timeout works additionanl context suspected might something switched back made prediction work,2018-12-14T09:45:34Z,"high priority, module: dependency bug, cherry-picked",CLOSED,0,17,https://github.com/pytorch/pytorch/issues/15213,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,框架资源调度不合理,,,,,Pytorch1.0.0与Serverless和AWS Lambda超时
aot eager hf longformer cannot view tensor,response logs traceback scratch anijain work repro py module res fwd maybe bwd opt mod scratch anijain work dynanmo debug py fwd maybe bwd gm scratch anijain work functorch aot autograd py return scratch anijain work nn modules module py impl return forward input kwargs scratch anijain work dynanmo eval frame py forward return dynanmo ctx orig mod forward kwargs scratch anijain work dynanmo eval frame py fn return fn kwargs scratch anijain work repro py forward def forward transpose strided transpose scratch anijain work dynanmo eval frame py fn return fn kwargs scratch anijain work functorch aot autograd py forward return compiled scratch anijain work functorch aot autograd py new func compiled fn create aot dispatcher function scratch anijain work torchdynanmo torchdynanmo py wrapper func kwargs scratch anijain work functorch aot autograd py create aot dispatcher function aot dispatch autograd flat fn fake flat tensor aot config scratch anijain work functorch aot autograd py aot dispatch autograd fx fx scratch anijain work fx proxy tensor py wrapped dispatch wrap key func fx tracer tracer fx tracer concrete tuple phs scratch anijain work dynanmo eval frame py fn return,2022-11-18T07:07:15Z,"bug, triaged, oncall: pt2, module: aotdispatch",OPEN,0,17,https://github.com/pytorch/pytorch/issues/93428,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架接口执行报错,,,,,无法查看张量
different speed batchnorm batchnorm,hi assume batchnorm batchnorm performance however normalizing tensor batchnorm speed slower batchnorm expected way speed reproducer output environment,2020-05-22T09:23:38Z,"high priority, module: dependency bug, module: performance, module: cudnn, module: nn, module: cuda, triaged",CLOSED,1,13,https://github.com/pytorch/pytorch/issues/38915,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子资源调度不合理,,,,,BatchNorm1d和BatchNorm2d速度不同
onnx export question torchdynanmo,export model either onnx model fails scripting options uses torchdynanmo create convert model optimized runs way export model onnx without dependency script model dynanmo compile model backend onnxrt cpu autotune export onnx exported model dynanmo export model dynanmo input logs minified repro response,2022-12-21T13:32:27Z,"bug, triaged, oncall: pt2",OPEN,2,8,https://github.com/pytorch/pytorch/issues/93492,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,算子不支持框架特性,,,,,ONNX导出问题
onnx values different tensor tensor returns tensor incorrect exported onnx model,description note complete reproducing bottom version installed following command values different tensor tensor returned result incorrect example following tensors expression input indices returns tensor however export torchscript expression onnx model onnx model inference output simplefailuremodel module bottom minimal example reproduced may seem trivial may seem limited impact conjunction functions operators trigger consequential runtime issues onnx inference example cat input indices input indices input indices concatenante three tensors along expression cat input indices input indices input indices correct returned tensor onnx runtime hand return tensor incorrect access element indices resulting concatenanted tensor return perfectly results onnx runtime return indices element data bounds runtime inference onnx concatenanted result axis trying access element axis axis direct result incorrect issues mentioned earlier try obtain new tensor values different tensor,2022-09-15T23:28:47Z,"bug, oncall: jit, module: onnx, triaged",OPEN,0,16,https://github.com/pytorch/pytorch/issues/85127,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,模型运行结果不一致,,,,,使用不同张量的值来索引张量会在导出的ONNX模型中返回形状不正确的张量
conv specific kernel outputs inconsistent results fp fp gpu,conv output inconsistent results float half gpu conv kernel input output channel number multiple repro gpu pcie reproduce repro sample messages traces provide well expected behavior part output looks input channel number output channel number multiple cosine similarity half float concise description expected happen environment version debug cuda os ubuntu lts gcc version ubuntu ubuntu cmake version version version cuda available cuda runtime version gpu models configuration gpu tesla pcie nvidia driver version cudnn version linux gnu libcudnn versions libraries pip numpy pip pip select gpu blas mkl cudatoolkit bb magma cuda mkl mkl include mkl service py mkl fft py mkl random py hd dba numpy py db numpy base py hde py cuda cudnn py cu,2020-06-19T09:09:06Z,"module: dependency bug, module: cudnn, module: cuda, module: convolution, triaged",OPEN,1,12,https://github.com/pytorch/pytorch/issues/40275,"version, mkl, py, cuda, pip, pypi, numpy, gpu, pypi_pypi, cudnn",7,算子输出结果不一致,,,,,特定内核的Conv3d在V100输出不一致的结果
shared library loading logic breaks cuda packages installed non standard location,tl dr cuda libraries alongside via pypi packages packages include nvidia cudnn cu nvidia cusparse cu init py various tricks find libraries one tricks break installed different location nvidia packages fixed linking cuda dependencies libtorch global deps longer version pypi pants pantsbuild org system creates environments slightly weird layout specifically package ends directory rather everything landing packages virtualenv causes problems attempt import think may point shared library loading logic specifically global deps init py logic attempts globals deps libtorch global deps attempts missing libraries cdll fails system cdll succeeds loading cp cp manylinux whl libtorch global deps returns immediately without attempting libraries cuda libs however links subset libraries listed libraries cuda libs missing ldd output nvidia packages installed directory rpath find packages specifically rpath bunch relative paths nvidia libraries look unfortunantely relative paths work installed,2023-05-12T21:56:07Z,"bug, triaged, topic: build, module: bazel",OPEN,3,4,https://github.com/pytorch/pytorch/issues/101314,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,框架CUDA出错,,,,,CUDA包安装在非标准位置，共享库加载逻辑中断
import breaks standard multiprocessing,instructions reproduce importing laptop multiprocessing breaks reason reproduce another device may combinantion os hardware import included spun processes cpu cores import removed cpu cores noticed issues training became extremely slow due data loading minimal install minimal multiprocessing running import following command gives outputs without import without interaction os let know logs provide expected behavior expect import effect multiprocessing especially since nothing done expect act devices mainly trying find difference setup architecture causing weird behavior thanks advance help possible duplicate issues also breaks normal multiprocessing versions,2023-05-25T13:47:05Z,"module: dependency bug, module: multiprocessing, triaged, module: openmp",OPEN,1,12,https://github.com/pytorch/pytorch/issues/102269,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,框架特殊环境下出错,,,,,导入torch影响了多处理
mkl dnn causes results xeon,editorial note think mkl dnn intel mkl dnn issues update updated test program demo problem issues issuecomment hi simple program ran cpu gpu different machines machine intel core tm hq cpu ghz machine intel core tm hk cpu ghz machine intel xeon platinum cpu ghz loading input pth conv pth output machine difference machines output machine output machine docker different machines probably software problem permission ptrace traceme machine ltrace help one tell know low library function computing conv objdump libmkldnn set breakpoint mkldnn dilated convolution forward desc init mkldnn convolution forward desc init stop probably guessing function hints greatly appreciated,2019-03-20T11:50:36Z,"high priority, module: dependency bug, triaged, module: mkldnn",CLOSED,0,15,https://github.com/pytorch/pytorch/issues/18218,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,模型结果错误,,,,,MKL-DNN在Xeon上导致错误结果
lr scheduler warning amp train fp,warning message amp train model fp reproduce got warnining miniconda envs py packages optim lr scheduler py userwarning seems optimizer overridden learning scheduler initialization sure optimizer lr scheduler details org docs stable optim adjust learning org docs stable optim adjust learning userwarning amp initialize warning message,2019-10-09T07:05:33Z,"module: dependency bug, module: optimizer, triaged",CLOSED,2,7,https://github.com/pytorch/pytorch/issues/27595,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,模型训练警告,,,,,使用amp训练带有fp16的模型时出现警告消息
compilation extension windows cuda,following compiling cuda windows actions runner work win include pybind cast arguments temp late template parameter tuple detected instantiation class pybind detail tuple caster tuple ts tuple pair ts actions runner work win include pybind cast arguments temp late template parameter tuple detected instantiation class pybind detail tuple caster tuple ts tuple pair ts complete failure log runs focus looks one facebookresearch issues workaround facebookresearch commit cb ffe af dev infra,2021-12-06T17:36:15Z,"high priority, module: dependency bug, module: windows, module: docs, module: cuda, triaged, module: pybind, has workaround",CLOSED,0,15,https://github.com/pytorch/pytorch/issues/69460,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,框架特殊环境下出错,,,,,在 Windows CUDA 11.5 上编译 <torch/extension.h> 错误
assertionbug fake possibly einops rearrange,training model model compile model obtained following bugs traceback seems indicate problem lies einops rearrange guess covered yet versions december,2022-12-07T10:12:21Z,"high priority, triage review, bug, oncall: pt2",CLOSED,0,15,https://github.com/pytorch/pytorch/issues/90375,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,模型训练出错,,,,,AssertionError fake_mode不是None
cuda extension failure windows msvc,description building custom cuda extension fails following msvc env packages include pybind cast expression must pointer complete object however observed multiple gcc versions linux example mwe setup py exten cuda cpp exten cuda kernel cu system info caffe installed pip source os windows version version cuda cudnn version gpu models configuration msvc version compiling source cmake version versions libraries setuptools,2018-08-29T17:19:50Z,"module: dependency bug, module: windows, module: cpp-extensions, module: cuda, triaged, module: pybind, has workaround",CLOSED,0,14,https://github.com/pytorch/pytorch/issues/11004,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,框架特殊环境下出错,,,,,Windows和MSVC 2017上的CUDA扩展构建失败
double backward times slower conv padding,hi trying compute jacobian vector product trick towns new trick involves computing gradient twice seems conv padding slows almost times compared padding padding padding one backward pass computed times padding padding reproduce expected behavior roughly similar without padding environment version debug cuda os ubuntu lts gcc version crosstool ng cmake version version version cuda available cuda runtime version collect gpu models configuration gpu quadro gpu geforce rtx nvidia driver version cudnn version linux gnu libcudnn versions libraries pip numpy pip pip mkl pip pip edit vi add backticks around source,2019-07-07T19:56:35Z,"module: dependency bug, module: performance, module: double backwards, module: nn, module: convolution, triaged, has workaround, quansight-nack",OPEN,0,14,https://github.com/pytorch/pytorch/issues/22577,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子资源调度不合理,,,,,conv2d padding=1时慢3倍
batched magma calls illegally read cuda memory,batched magma calls illegally read cuda memory illegal reads often silent harmless however access unanllocated device memory program future cuda calls fail reproduce pr goes enanble test cholesky batched many batches also reproduced calling magma dpotrf batched directly tensor allocated cudamalloc cuda memcheck report illegal memory accesses including silent ones additionanl context discovered diagnosed following workaround may pad tensor inputs batched magma calls requires copying originanl tensor buffer larger needs much extra space needed requires investigation,2019-09-27T21:38:18Z,"high priority, module: dependency bug, triaged, module: linear algebra",CLOSED,0,14,https://github.com/pytorch/pytorch/issues/26996,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子资源调度不合理,,,,,批量MAGMA调用非法读取CUDA内存
weight gradient incorrect mkldnn,following convolution program found cpu gpu output results different confirm gpu result correct cpu convolution reproduce reproduce data follows data zip files data zip environment version cu os linux installed pip,2021-11-24T09:31:09Z,"high priority, module: dependency bug, module: nn, module: convolution, triaged, module: mkldnn, module: correctness (silent)",CLOSED,2,6,https://github.com/pytorch/pytorch/issues/68868,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,模型运行结果不一致,,,,,组卷积程序Pytorch的CPU和GPU输出结果不同
dill pickle module,latest codebase enforcing encoding utf breaks one uses dill module pickling dill take encoding argument,2019-10-18T21:53:12Z,"high priority, module: dependency bug, module: serialization, triaged",CLOSED,0,14,https://github.com/pytorch/pytorch/issues/28313,"nn, output, forward, input, layer, weight, float, embedding, padding, batch",9,算子执行失败,,,,,使用dill pickle时设置encoding='utf-8'出错
computing dot product columns sliced matrix causes illegal memory access cuda,compute dot product column sliced larger cuda matrix mm produce illegal memory access instead compute cublas produce illegal memory access additionanlly allocated separately rather sliced produce may duplicate similar underlying cublas script succeed commented bigger still works sure reproduce environment version also verified still present latest master os linux linux version cuda cudnn version gpu models configuration,2019-12-30T17:36:42Z,"module: dependency bug, module: cuda, triaged, module: 64-bit, module: cublas",OPEN,0,14,https://github.com/pytorch/pytorch/issues/31708,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子资源调度不合理,,,,,大矩阵切片的列的点积导致非法内存访问
cudnn convolution cuda devices preserve nann weights upstream,concise description conv nann weights outputs non nann values traing saving reloading weights outputs nann values output nann reloading also training weights nann know weights become nann lines repo example prints weight nann hundreds iterations dhgrs uniwavenet efb ce net py reproduce steps reproduce behavior clone repo dhgrs uniwavenet download ljspeech keithito lj speech dataset set params py install librosa tqdm train py expected behavior conv outputs nann conv weights nann values concise description expected happen environment version debug cuda os ubuntu lts gcc version ubuntu ubuntu cmake version version version cuda available cuda runtime version gpu models configuration gpu geforce ti nvidia driver version cudnn version probably one following linux gnu libcudnn linux gnu libcudnn static packages cupy libcudnn versions libraries pip msgpack numpy pip numpy pip pip torchtext pip collect,2018-10-09T15:50:30Z,"module: dependency bug, module: cudnn, low priority, triaged",OPEN,0,14,https://github.com/pytorch/pytorch/issues/12484,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架CUDA出错,,,,,某些CUDA设备上的CuDNN卷积不会保留 NaN 权重
float cblas sgemm alloc cblas identifier deprecated caused outdated mkl,building source fails nvidia driver cuda ubuntu reproduce steps reproduce behavior uninstall cuda versions nvidia drivers reinstall cuda cudnn nccl fresh environment git clone recursive setup py install message follows output log gist jbohnslav af bd cceaefc expected behavior expected complete environment collecting environment information version debug cuda os ubuntu lts gcc version ubuntu ubuntu cmake version version version cuda available cuda runtime version gpu models configuration gpu geforce gt gpu geforce ti gpu geforce ti nvidia driver version cudnn version linux gnu libcudnn versions libraries pip numpy mkl intel intel mkl include intel intel additionanl context add context problem output nvcc version output nvidia smi,2019-03-11T17:59:45Z,"module: dependency bug, module: build, triaged, module: mkldnn",CLOSED,1,10,https://github.com/pytorch/pytorch/issues/17874,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子被弃用,,,,,cblas_sgemm_alloc因MKL过时而被弃用
dynanmo exception guided diffusion diffusion uncond compiled model,note might related issues independent debugging short guided diffusion gaussian diffusion py talafek diffusion project extern guided diffusion guided diffusion gaussian diffusion py module uses model created guided diffusion script util py talafek diffusion project ef efe ab extern guided diffusion guided diffusion script util py denoise given model defined guided diffusion unet py talafek diffusion project ef efe ab extern guided diffusion guided diffusion unet py parameters given authors guided diffusion talafek diffusion project tree extern guided diffusion classifier guidance model unconditionanl box repo talafek diffusion project control flow causes problem initial experiment ipynb talafek diffusion project initial experiment ipynb part reconstructed denoiser denoise noised unsqueeze timesteps show progress executed nd block notebook crashes logs attached denoiser denoise defined denoiser py talafek diffusion project denoiser py problematic denoiser py talafek diffusion project denoiser py method sample loop called defined guided diffusion gaussian diffusion py talafek diffusion project extern guided diffusion guided diffusion gaussian diffusion py sample loop invokes forward,2023-04-14T12:41:36Z,"bug, triaged, oncall: pt2, module: dynamo",CLOSED,1,10,https://github.com/pytorch/pytorch/issues/99145,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,模型异常,,,,,
batchnorm support batchsize eval dimension nxcxl cudnn status supported,hi found feeding big batch model works remove bn eval checked similar issues seems solve problem runtimebug cudnn status supported may appear passed non contiguous input reproduce minimum reproducing works training also works without dimension batchnorm environment version os linux linux installed pip source version cuda cudnn version cuda cudnn gpu models configuration ti,2019-04-29T10:04:08Z,"module: dependency bug, module: cudnn, triaged, small",OPEN,1,10,https://github.com/pytorch/pytorch/issues/19911,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子功能不完善,,,,,BatchNorm1d不支持batchsize>65535
test py test inverse cuda causes illegal memory access platforms,test illegal memory access cuda platforms indicative operation illegally accessing memory likely almost definitely caused environment version db debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version tags release finanl cmake version version version bit runtime cuda available cuda runtime version gpu models configuration gpu quadro gp gpu quadro gp nvidia driver version cudnn version collect hip runtime version miopen runtime version versions libraries pip numpy pip pip blas mkl magma cuda mkl mkl include mkl service py mkl fft py mkl random py numpy py hbc numpy base py hfa dev develop pypi pypi,2020-09-28T00:21:06Z,"high priority, triage review, module: dependency bug, module: tests, module: regression, module: linear algebra",CLOSED,1,10,https://github.com/pytorch/pytorch/issues/45403,"version, mkl, py, cuda, pip, pypi, numpy, gpu, pypi_pypi, cudnn",7,算子资源调度不合理,,,,,test_inverse_cuda非法内存访问
compile nanme ne defined stable diffusion,testing compile dynanmic diffusers stable diffusion pipe works output resolution changed changes syntax one autogenerated files seem tied diffusers version tested latest prerelease git version diffusers posted simple test script alphaatlas diffusion compilaton testing inductor test py logs inductor dynanmic txt files inductor dynanmic txt minified repro alphaatlas diffusion compilaton testing inductor test py versions chen weiwen,2023-05-11T23:17:30Z,"bug, triaged, oncall: pt2, module: inductor",CLOSED,0,13,https://github.com/pytorch/pytorch/issues/101228,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,框架功能实现不完善名称未定义,,,,,
circleci workflows consistently erring exported prs permission issues,exported prs one ios circleci jobs along docker jobs failing due seems permissions issues docker jobs fail unanble locate credentials configure credentials running aws configure circleci pipelines jerryzh workflows dfa fee jobs ios jobs fail due find org member context circleci gh jerryzh utm campaign vcs integration utm medium referral utm source binanry says machine appropriate circleci gh jerryzh utm campaign vcs integration utm medium referral utm source jerry exported prs sort pulls jerryzh speculation due jerry prs provisioned jerry belong team combinantion pr exported internanl dev infra,2021-10-11T18:45:25Z,"module: dependency bug, module: ci, triaged",CLOSED,2,5,https://github.com/pytorch/pytorch/issues/66434,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,框架开发流程问题,,,,,
causes metaclass conflict torchtyping munged pytest,pytest throws metaclass conflict reproduce script test py pytest test py gives environment,2021-06-27T23:01:03Z,"high priority, triage review, module: dependency bug, module: regression, module: testing",CLOSED,0,13,https://github.com/pytorch/pytorch/issues/60839,"reproduce, behavior, expected, concise, description, context, environment, concise_description, message, sample",14,框架出错,,,,,metaclass与torchtyping发生冲突
pybind gil scoped release crash exit daemon threads,pybind gil scoped release crashes daemon threads threading package created threads acquire release gil note previously autonogil problem manifests crash exit message problem occurs thread tries acquire gil interpreter finanlization interpreter calls cpython aef db ceval pthread exit terminante thread note destructor gil scoped release pybind pybind cb cb include pybind pybind glibc pthread exit implementation uses unwind calling unwind destructor similar problems throwing exceptions destructors triggers terminante abort reproduces think essentially also seen happen research mixes threads,2020-05-11T14:38:29Z,"high priority, module: dependency bug, module: crash, triaged",OPEN,0,13,https://github.com/pytorch/pytorch/issues/38228,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,模型出错,,,,,进程退出时崩溃
big conv,find big set kernel think happens set padding set padding throw seems reasonanble reproduce version os centos linux installed version cuda cudnn version gpu models configuration information,2020-01-22T20:52:39Z,"high priority, module: dependency bug, triaged, module: mkldnn",CLOSED,0,13,https://github.com/pytorch/pytorch/issues/32507,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子出错,,,,,padding size=0，kernel size设置很大Conv1d会报错
compile work layer drop,huggingface model mbartforconditionanlgeneration part benchmark running layer drop extracted minimal repro layer drop inspired mbartforconditionanlgeneration pasted cases dynanmo evaluate random number tracing choose whether layer skip random number evaluated tracing part graph chance guard catch runtime instead execution model frozen set dice rolls one fix graph break random uniform calls unfortunantely mbart model causes ci oom due excessive number graph breaks versions bc af fed fd ed ca bb chen weiwen,2023-02-23T23:50:46Z,"bug, triaged, oncall: pt2, module: dynamo",CLOSED,0,13,https://github.com/pytorch/pytorch/issues/95425,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子功能不完善,,,,,torch.compile 不适用于图层删除
import,install import happened import traceback stdin module packages init py module import importbug dlopen packages cpython darwin symbol found pyslice adjustindices referenced packages cpython darwin expected flat nanmespace packages cpython darwin ps macos even install ubuntu still failure,2017-05-06T13:22:23Z,"high priority, module: dependency bug",CLOSED,1,9,https://github.com/pytorch/pytorch/issues/1497,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架导入错误,,,,,pytorch导入失败
flatbuffers dependency causing dirty git checkout,reason flatbuffer cleaning assertion git checkout clean failing significant issues currently affecting ci flakiness examples torchci us east elasticbeanstalk failure capture left git repository checkout dirty dev infra,2021-12-17T07:53:05Z,"high priority, module: dependency bug, module: ci, triaged",CLOSED,0,12,https://github.com/pytorch/pytorch/issues/70097,"target, test, py, linux, cuda_target, linux_libcudnn, ci, jenkins, build, rocm",3,算子资源调度不合理,,,,,Flatbuffer不会自行清理
matrix multiplication returns result middle dimension,may marginanl expect result multiplying matrices zeros particular matrix multiplication definition however bmm matmul neither fail return tensor likely memory initialisation skipped let discuss whether consider legitimate input either fix result exception reproduce unfortunantely floating depends system expected behavior either res bmm res matmul always zeros regardless initial memory exception raised environment version cu os linux linux installed pip source command compiling source version cuda cudnn version gpu models configuration quadro gp,2019-12-18T14:09:10Z,"high priority, module: dependency bug, module: cuda, triaged, module: cublas",CLOSED,0,12,https://github.com/pytorch/pytorch/issues/31421,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子计算错误,,,,,当中间维度为 0 时，矩阵乘法返回错误结果
ddpoptimizer dynanmo causes faketensor,summary tensor scalar constants appears submod graph ddp graph optimizer constants appear interpreted tensors faketensors unclear interpreted tensors place something seems going faketensor layer around ddpoptimizer interface calling submod compiler repro gives versions repro locally snanpshot master db ca af reported recently internanl fbcode builds,2023-01-25T01:06:16Z,"bug, triaged, oncall: pt2",CLOSED,0,12,https://github.com/pytorch/pytorch/issues/92941,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子出错,,,,,标量被视为张量
onnx export nn functionanl linear weight dimension,nn functionanl linear function validate weight dimension asserts weight expects tensor dimension doc org docs stable generated nn functionanl linear nn functionanl linear example works weight onnx export fail otherwise works reproduce steps reproduce behavior stdout,2021-09-14T09:48:49Z,"bug, module: onnx, module: docs, feature, module: nn, good first issue, triaged, OSS contribution wanted, onnx-triaged",CLOSED,0,12,https://github.com/pytorch/pytorch/issues/64978,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,框架导出问题,,,,,ONNX导出torch.nn.functional.linear权重维度为1
accuracy nantive batch decomp,following cmds fail accuracy benchmarks dynanmo timm models py accuracy device cuda backend aot inductor debug training float resnest benchmarks dynanmo torchbench py training accuracy device cuda backend aot inductor debug amp mobilenet fixed issues benchmarks dynanmo torchbench py training accuracy device cuda backend aot inductor debug float resnet quantized qat pass disable nantive batch inductor decomposition py note backend aot inductor debug uses decomp partitioner inductor inductor codegen minifiers unanble help,2022-11-19T00:06:34Z,"bug, oncall: pt2",CLOSED,0,12,https://github.com/pytorch/pytorch/issues/93433,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,模型精度问题,,,,,潜在的准确性问题
torchdynanmo aot autograd speedup strategy increased memory usage long overhead resnet model,script correct torchdynanmo api details summary dynanmo test py summary details bash script tested vision torchdynanmo ce bfae de functorch cuda update cudnn details summary results summary details seen torchdynanmo aot autograd speedup strategy increased memory usage longer overhead resnet model eager,2022-05-03T01:43:37Z,"bug, oncall: pt2",CLOSED,0,12,https://github.com/pytorch/pytorch/issues/93751,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,模型资源调度不合理,,,,,ResNet50自动求导aot_autograd_speedup_strategy
conv fail curtain batch,description conv work batch outputs filled tensor batch illegal memory access filled tensor example minimal working example copy lines py second work computers system info caffe installed pip source ananconda nanvigator os ubuntu version version cuda cudnn version gpu models configuration titan pascal,2019-03-12T18:35:17Z,"high priority, module: dependency bug, module: cudnn, module: nn, triaged",CLOSED,1,8,https://github.com/pytorch/pytorch/issues/17930,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子执行出错,,,,,Conv3d参数设置问题batch size
tuning huggingface wav vec compile,followed example tune huggingface wav vec speech recognition compile aiming faster training however ran outlined logs suspect huggingface wav vec yet supported needs modification ensure compatibility running compile mostly related creating mask tensors specaugment seems also related fairseq hubert compile issues also raised huggingface huggingface transformers issues logs minified repro install huggingface transformers source training script versions details summary versions summary collecting environment information version cu debug cuda rocm os debian gnu linux buster gcc version debian clang version collect cmake version version libc version glibc version packaged default gcc bit runtime platform linux cloud amd glibc cuda available cuda runtime version cuda module loading set lazy gpu models configuration gpu nvidia nvidia driver version cudnn version collect hip runtime version miopen runtime version xnnpack available cpu architecture cpu op bit bit byte little endian address sizes bits physical bits virtual cpu cpu thread core core socket socket numa vendor genuineintel cpu family model model nanme intel xeon cpu ghz stepping cpu mhz bogomips hypervisor vendor kvm virtualization cache cache cache cache,2023-05-11T06:33:41Z,"high priority, triage review, bug, oncall: distributed, module: crash, triaged, module: nccl, ezyang's list, oncall: pt2, module: dynamic shapes",OPEN,0,12,https://github.com/pytorch/pytorch/issues/101160,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架接口执行报错,,,,,torch.compile 
instability convtranspose cudnn,together observed instability following snippet cudnn without cudnn works expected also worth noticing work works expected cpu well reproduce requires small pickle containing one single minibatch data replacing random tensor reproduce problem linked minibatch pkl zip files minibatch pkl zip backends cudnn enanbled commented thus cudnn output comment means cudnn output expected behavior executions without cudnn output approximately loss values environment version debug cuda os ubuntu lts gcc version ubuntu ubuntu cmake version version version cuda available cuda runtime version gpu models configuration gpu quadro gp gpu quadro gp nvidia driver version cudnn version collect versions libraries pip collect blas mkl cuda mkl mkl fft py mkl random py py cuda cudnn py additionanl context add context problem,2019-01-31T16:21:58Z,"high priority, module: dependency bug, module: cudnn, triaged",CLOSED,1,7,https://github.com/pytorch/pytorch/issues/16610,"version, mkl, py, cuda, pip, pypi, numpy, gpu, pypi_pypi, cudnn",7,算子执行性能不稳定,,,,,ConvTranspose2d with cudnn 
,wsl docker rtx got runtime rtx ti happend cuda version docker help,2021-01-29T15:03:43Z,"module: dependency bug, module: cuda, triaged, module: cublas, module: wsl",OPEN,0,11,https://github.com/pytorch/pytorch/issues/51351,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架docker部署出问题,,,,,"rtx 1050ti, it doesn't happend with cuda 10.0 version of docker"
backward pass slow conv,backward passes roughly times slower forward passes nn conv layers example forward pass convolutionanl layer takes seconds backward pass takes one second reproduce reproduce problem forward backward pass batch images depth kernels padding pixels expected behavior expect backward pass times faster fact performance expected running script different computer performance linear layers also expected environment additionanl context results bottleneck train py,2019-10-15T12:36:20Z,"high priority, triage review, module: dependency bug, module: performance, module: cpu, triaged, module: mkldnn",CLOSED,0,11,https://github.com/pytorch/pytorch/issues/27974,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,算子运行问题,,,,,conv2d反向传播慢
unanble backward propagate convolution padded input,graph input padded required zeros passed convolution backward called test failing res backward grad traceback stdin module jthakur vnv syn new packages tensor py backward autograd backward gradient retain graph create graph inputs inputs jthakur vnv syn new packages autograd init py backward allow unreachable accumulate grad allow unreachable runtimebug create primitive reproduce channels backpropagation working steps reproduce behavior expected behavior environment version os linux linux installed pip source pip command compiling source nan version cuda cudnn version cpu gpu models configuration nan information nan,2021-06-16T06:54:52Z,"high priority, module: dependency bug, module: nn, module: convolution, triaged, module: mkldnn",CLOSED,0,11,https://github.com/pytorch/pytorch/issues/60075,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子执行出问题,,,,,convolution无法反向传播
bugs running benchmarks hf bert hf,runtimebug thrown dynanmo benchmark script hi master benchmarks dynanmo py tests branch jan built installed docker container base dlc dkr ecr us east amazonanws training gpu py cu ubuntu sagemaker download awscli docs aws amazon amazonecr latest userguide getting started cli purged container installed particular hf bert hf gpt hf wrapper observing bugs attached mentions dynanmo tagging compile cohort command note commit versions different minor change include directories succeed base dlc attached minified graph reproduce pointers working example working wrapper helpful logs minified repro versions chen weiwen,2023-01-30T18:20:46Z,"bug, triaged, module: fsdp, oncall: pt2, module: dynamo",OPEN,0,11,https://github.com/pytorch/pytorch/issues/93288,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架执行出错,,,,,hf_Bert and hf_T5 出错
onnx nn transformer contains unsupported tensor scalar,fails export model containing nn transformer module fails runtimebug unexpected tensor scalar minimal repro script versions,2022-09-15T22:00:46Z,"bug, module: onnx, triaged, onnx-triaged",OPEN,0,11,https://github.com/pytorch/pytorch/issues/85116,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,算子执行出错,,,,,nn.Transformer包含不支持的张量类型
matmul output contains nann fp tensors gpu,description simple running nvidia gpu randomly generated fp tensors input matmul output contains nann expected problem reproed running ti seems related fp kernel repro problem reproed many fp tensors example problem reproed try versions problem reproed fp stdout running ti cuda nanme geforce ti nann items count ratio stdout running cuda nanme tesla nann items count ratio nann examples computed attention scores nann expected attention scores computed attention scores nann expected attention scores example import cuda nanme format cuda device nanme manual seed query layer rand cuda half key layer rand cuda half part transformer attention attention scores matmul query layer key layer transpose indexs isnann attention scores nonzero tuple nann item count indexs nann items count ratio format nann item count nann item count attention scores nelement nann examples range nann item count format indexs indexs indexs indexs current query layer query layer indexs indexs indexs current key layer key layer indexs indexs indexs current attention score attention scores indexs indexs indexs indexs,2020-10-02T04:07:54Z,"high priority, module: dependency bug, module: cuda, triaged, module: cublas",CLOSED,0,11,https://github.com/pytorch/pytorch/issues/45724,"nn, output, forward, input, layer, weight, float, embedding, padding, batch",9,算子输出异常,,,,,torch.matmul在V100 fp16包含nan
,installing source getting following scratch softwares new gen py module import preprocess declarations scratch softwares new preprocess declarations py module function wrapper import formal generic scratch softwares new function wrapper py module template import codetemplate scratch softwares new template py module class codetemplate object scratch softwares new template py codetemplate multiline scratch softwares virtualpython env new py compile return compile pattern flags scratch softwares virtualpython env new py compile invalid expression sre constants nothing repeat cmake cmakelists txt message failed generated cpp configuring incomplete bugs occurred,2017-07-12T14:25:21Z,module: dependency bug,CLOSED,0,11,https://github.com/pytorch/pytorch/issues/2062,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架安装问题,,,,,python2.7安装pytorch
dynanmo set op unsupported,applying flake comprehensions actions runs jobs revert whitelist one changes set instruction operation supported seems somewhat rare instruction since caused issues far wanted maintainers dynanmo aware easy fix expand amount compilable dynanmo testcase little surprising constructing set generator works constructing set simple synanx directly repro change test repros py following versions master info repro available pr chen weiwen,2023-02-13T16:27:51Z,"bug, triaged, oncall: pt2, module: dynamo",CLOSED,0,10,https://github.com/pytorch/pytorch/issues/94738,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,算子不支持框架特性,,,,,BUILD_SET不支持特性
complex supported torchinductor,linear layer complex valued weights nn linear inp dtype complex produces following notimplementedbug dynanmo optimize model logs minified repro response,2022-11-16T23:31:48Z,"bug, triaged, oncall: pt2",OPEN,0,10,https://github.com/pytorch/pytorch/issues/93424,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,算子不支持框架特性,,,,,Complex在Torchinductor不受支持
inductor resnet batch cuda illegal memory access encountered,repro happens batch logs minified repro response,2022-11-01T19:59:40Z,"bug, triaged, oncall: pt2",OPEN,0,10,https://github.com/pytorch/pytorch/issues/93599,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,模型资源调度不合理,,,,,resnet18中batch_size过大导致不合法的内存访问
linanlg solve matrix cuda,concise description trying execute following commands cuda facing running cpu smaller matrix instead cuda reproduce steps reproduce behavior randn cuda randn cuda linanlg solve sample messages traces provide well runtimebug traceback ipykernel py randn cuda randn cuda linanlg solve runtimebug cuda invalid configuration argument cuda kernel bugs might asynchronously reported api stacktrace might incorrect debugging consider passing cuda launch blocking expected behavior work version os linux ubuntu installed pip source pip version cuda cudnn version gpu models configuration,2021-07-20T20:00:38Z,"module: dependency bug, module: cuda, triaged, module: linear algebra, module: magma",CLOSED,0,10,https://github.com/pytorch/pytorch/issues/61929,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子执行出错,,,,,Torch.linalg.solve处理大矩阵出错
onnx converter consider implicit casting specifically,model fn double float implicitly promote output double exporting model onnx implicit casting considered onnx standard similar operators must take inputs whose types therefore models invalid onnxruntime abort accepting unfortunantely onnx checker also lets specific escape onnx onnx issues reproduce expected output graph invalid takes different types inputs another interesting fact seems happens add output onnx model correctly produces implicit casting versions details summary env expand summary div div details,2022-10-24T16:21:19Z,"bug, module: onnx, triaged, actionable, onnx-triaged",CLOSED,2,2,https://github.com/pytorch/pytorch/issues/87609,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,算子功能不完善,,,,,Max没有考虑隐式转换
compile raises passmananger failed,compile raises passmananger failed linear dropout argmax repeat add minimized reproduced model delete operation compile succeed versions details summary expand summary details,2023-05-14T21:22:03Z,"bug, triaged, oncall: pt2",CLOSED,1,6,https://github.com/pytorch/pytorch/issues/101368,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子执行出错,,,,,torch.compile执行出错
cannot find intel mkl,attempting compile git intel mkl found unless change cmake findmkl cmake define sh script cmake intel mkl mklroot cmake library opt intel compilers libraries linux intel,2017-05-07T18:36:31Z,"module: dependency bug, module: build, triaged, module: mkl",OPEN,1,6,https://github.com/pytorch/pytorch/issues/1505,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,框架异常,,,,,Intel MKL无法找到
incorrect output loss specific cuda version,ran example org tutorials beginner examples tensors autograd got output loss always cuda switched cuda output loss seemed correct different iterations got result reproduce steps reproduce behavior copied org tutorials beginner examples tensors autograd set iterations fractions output expected behavior output got another server seems correct environment copy paste output environment collection script raw githubusercontent master collect env py checklist manually script output additionanl context,2020-11-12T09:32:46Z,"high priority, module: dependency bug, module: numerical-stability, module: cuda, triaged, module: linear algebra, module: tf32",OPEN,0,10,https://github.com/pytorch/pytorch/issues/47844,"reproduce, behavior, expected, concise, description, context, environment, concise_description, message, sample",14,模型损失异常,,,,,特定CUDA下框架输出错误
backward pass fails due ctcloss infinity backends cudnn enanbled,ctcloss infinity backends cudnn enanbled backward result script crashes following reproduce steps reproduce behavior download variables drive eltv wjdvturbxo dkq gew qj view sharing expected behavior crash environment additionanl context also able reproduce following configuration following configuration everything works,2020-12-08T22:17:30Z,"module: dependency bug, module: cudnn, module: loss, triaged",CLOSED,1,6,https://github.com/pytorch/pytorch/issues/49046,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子执行失败,,,,,CTCLoss反向传播失败
amp much worse performance groupped conv fp,seems automatic mix precision conv groups significantly slower fp result obvious machines volta gpus tensor cores result,2020-08-25T09:33:13Z,"module: dependency bug, module: performance, module: cuda, triaged, module: amp (automated mixed precision)",OPEN,0,10,https://github.com/pytorch/pytorch/issues/43544,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子执行性能不稳定,,,,,Conv2d中AMP效果比fp32差
memory cudnn bidirectionanl rnn cudnn,context discuss org tracking suspected memory smth reproduced,2017-04-04T19:34:10Z,"module: dependency bug, module: cudnn, triaged, Stale",CLOSED,0,10,https://github.com/pytorch/pytorch/issues/1183,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,模型资源调度不合理,,,,,cudnn bidirectional RNN导致内存泄露
conv conv performance tesla arch half precision,testing new release observe increased consumption basic conv conv layers interested float performance simple script takes float float upd testing newest cuda toolkit include results upd tested py cuda cudnn results version os linux installed cuda cudnn version gpu models configuration rtx ti,2019-08-09T14:39:58Z,"module: dependency bug, module: cudnn, module: cuda, module: convolution, triaged, module: third_party",CLOSED,0,10,https://github.com/pytorch/pytorch/issues/24082,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子执行性能不稳定,,,,,Conv3D/Conv2d在Tesla arch上不稳定
intel mkl fatal system meet minimum requirements intel math kernel library,hi trying install macbook pro ghz intel core duo several different versions pyenv environment always got intel mkl fatal import net nn linear net randn intel mkl fatal system meet minimum requirements intel math kernel library processor must support intel supplemental streaming simd extensions intel ssse instructions processor must support intel streaming simd extensions intel sse instructions download sources install got illegal hardware instruction import zsh illegal hardware instruction happen reinstall macos mbp reinstall os working mbp know happened anybody help thx versions collecting environment information version debug cuda rocm os macos gcc version collect clang version clang cmake version collect libc version version default dec clang clang bit runtime platform macos bit cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda hip runtime version miopen runtime version versions libraries pip numpy pip collect sysctl grep cpu features machdep cpu features fpu vme de pse tsc msr pae mce cx apic sep mtrr pge mca cmov pat pse clfsh ds acpi mmx fxsr sse sse ss htt tm pbe sse dtes,2021-12-24T16:29:23Z,"module: dependency bug, oncall: binaries, triaged, module: macos, module: mkl, module: intel",OPEN,1,6,https://github.com/pytorch/pytorch/issues/70394,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架功能不完善,,,,,英特尔 MKL不满足使用数学核心函数库的最低要求
attributebug pickle object convert frame locals convert frame,adding model compile model loading model occurs removing script functions intended logs opt ananconda envs ml multiprocessing process py start popen popen opt ananconda envs ml multiprocessing context py popen return default context context process popen process obj opt ananconda envs ml multiprocessing context py popen return popen process obj opt ananconda envs ml multiprocessing popen spawn posix py init super init process obj opt ananconda envs ml multiprocessing popen fork py init launch process obj opt ananconda envs ml multiprocessing popen spawn posix py launch reduction dump process obj fp opt ananconda envs ml multiprocessing reduction py dump forkingpickler protocol dump obj attributebug pickle object convert frame locals convert frame minified repro response chen weiwen,2022-12-07T08:47:08Z,"bug, triaged, oncall: pt2, module: dynamo",OPEN,0,10,https://github.com/pytorch/pytorch/issues/93470,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,框架对象转换问题,,,,,AttributeError: Can't pickle local object 'convert_frame.<locals>._convert_frame'
aot eager inductor exception invoking operators non fake tensor inputs faketensormode yet supported,models issues logs minified repro response,2022-12-06T23:33:59Z,bug,CLOSED,1,6,https://github.com/pytorch/pytorch/issues/93469,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子不支持框架特性,,,,,Exception: Invoking operators with non-Fake Tensor inputs in FakeTensorMode is not yet supported.
inductor cpu crash failure torchbench model mobilenet quantized qat resnet quantized qat,failure found latest torchinductor cpu performance dashboard issues refresh test log sw info sw commit master commit commit ada bca ec cce dc commit ce db cfa torchbench dfe benchmark commit dfe db bf cd ec torchaudio audio commit adece bb audio commit fcd torchtext text commit ce acb efe text commit efeba cb vision commit acf cbe ffd vision commit ffd abf da detail info reference dashboard issues logs details summary summary details minified repro benchmarks dynanmo torchbench py performance float dcpu inductor skip dashboard mobilenet quantized qat,2022-11-18T17:18:23Z,"bug, triaged, oncall: pt2",OPEN,0,10,https://github.com/pytorch/pytorch/issues/93430,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,模型出错,,,,,model mobilenet_v2_quantized_qat & resnet50_quantized_qat崩溃
output causes segmentation,output tensor initialized passed compile segfault observed allocating tensor invalid sample reproduce logs minified repro versions nanme version summary tensors dynanmic neural networks strong gpu acceleration org author team author email packages org license bsd location jthakur packages requires filelock jinja networkx nvidia cublas cu nvidia cuda cupti cu nvidia cuda nvrtc cu nvidia cuda runtime cu nvidia cudnn cu nvidia cufft cu nvidia curand cu nvidia cusolver cu nvidia cusparse cu nvidia nccl cu nvidia nvtx cu sympy triton typing extensions required torchaudio triton chen weiwen,2023-04-11T09:47:02Z,"bug, triaged, oncall: pt2, module: aotdispatch, module: inductor",OPEN,1,5,https://github.com/pytorch/pytorch/issues/98835,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子执行出错,,,,,empty output shape 导致torch.compile的Segmentation fault
changing module attributes retrigger compilation,compiling method execution flow depends module attribute changing attribute outside method retrigger compilation however replacing module attribute training global variable behaves expected recompile changes seems say mutating attributes supported org started reading updating attributes repro script output logs response minified repro response versions version debug cuda rocm os macos gcc version collect clang version clang cmake version collect libc version version default mar clang bit runtime platform macos bit cuda available cuda runtime version cuda cuda module loading set gpu models configuration cuda nvidia driver version cuda cudnn version cuda hip runtime version miopen runtime version xnnpack available cpu intel core tm cpu ghz versions libraries pip flake pip flake junit report pip mypy pip mypy extensions pip numpy pip pip numpy pypi pypi pypi pypi pypi pypi chen weiwen,2023-04-12T09:48:44Z,"high priority, bug, triaged, oncall: pt2, module: dynamo",OPEN,0,9,https://github.com/pytorch/pytorch/issues/98928,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,模型执行出错,,,,,改变模型属性没有重新触发编译
accuracy aot eager mobilenet quantized qat fails accuracy,expecting aot eager really close nantive model mobilenet quantized qat fails accuracy minifier helping going dive deeper couple questions something special model affect accuracy long ago relaxed accuracy another similar model resnet quantized qat cosine similarity think relaxed tighten back model fails accuracy well benchmarks dynanmo torchbench py training accuracy device cuda backend aot eager float mobilenet quantized qat logs response minified repro response,2022-11-23T07:21:31Z,"bug, triaged, oncall: pt2, module: dynamic shapes, module: aotdispatch",OPEN,0,9,https://github.com/pytorch/pytorch/issues/93445,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,模型精度问题,,,,,mobilenet_v2_quantized_qat精度问题
multiplying cuda tensor another tensor yields unexpected result,multiplying cuda tensor another tensor yields unexpected result reproduce steps reproduce behavior generate following random matrices must minimal example returns expected behavior return environment version debug cuda os ubuntu lts gcc version homebrew gcc cmake version collect version cuda available cuda runtime version gpu models configuration gpu geforce ti nvidia driver version cudnn version collect versions libraries pip numpy pip lightning pip pip torchkge pip blas mkl libblas mkl libcblas mkl liblapack mkl liblapacke mkl mkl py cuda cudnn lightning pypi pypi torchkge dev develop py cu,2019-08-08T16:31:10Z,"module: dependency bug, module: cuda, triaged",OPEN,0,9,https://github.com/pytorch/pytorch/issues/24016,"version, mkl, py, cuda, pip, pypi, numpy, gpu, pypi_pypi, cudnn",7,算子执行出错,,,,,张量相乘出错
serialization,concise description text reproduce steps reproduce behavior test py sample messages traces provide well expected behavior concise description expected happen environment copy paste output environment collection script raw githubusercontent master collect env py checklist manually script version db os linux windows installed pip source source command compiling source setup py version cuda cudnn version gpu models configuration information additionanl context add context problem,2020-01-16T14:51:42Z,"high priority, module: dependency bug, module: serialization, triaged",CLOSED,0,9,https://github.com/pytorch/pytorch/issues/32289,"reproduce, behavior, expected, concise, description, context, environment, concise_description, message, sample",14,框架出错,,,,,Python 3.8 serialization error
logs dynanmo aot shows pattern matcher py usage aotautograd,define repro py viable strict shows forward graph also joint graphs containing ops related bmm permute details summary output summary details huang also script similarly saw extraneous ops traced graphs,2023-04-10T20:06:55Z,"bug, triaged, oncall: pt2",CLOSED,0,9,https://github.com/pytorch/pytorch/issues/98778,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子异常,,,,,AotAutograd异常
train compiled fairseq hubert model got dynanmo exc backendcompilerfailed debug wrapper raised dynanmicoutputshapeexception tensor,add model compile model fairseq facebookresearch fairseq fairseq cli train py img alt images githubusercontent fd train hubert model got dynanmo exc backendcompilerfailed debug wrapper raised dynanmicoutputshapeexception tensor logs tools fairseq fairseq fairseq fairseq models hubert hubert py graph break apply mask mask indices numpy mask indices device tools ananconda envs myenv packages dynanmo eval frame py catch bugs return callback frame cache hooks tools ananconda envs myenv packages dynanmo convert frame py convert frame result inner convert frame cache hooks tools ananconda envs myenv packages dynanmo convert frame py fn return fn kwargs tools ananconda envs myenv packages dynanmo convert frame py convert frame assert return compile tools ananconda envs myenv packages dynanmo py wrapper func kwargs tools ananconda envs myenv packages dynanmo convert frame py compile transform object transform tools ananconda envs myenv packages dynanmo bytecode transformation py transform object transformations instructions options tools ananconda envs myenv packages dynanmo convert frame py transform tracer tools ananconda envs myenv packages dynanmo symbolic convert py super tools ananconda envs myenv packages dynanmo,2023-03-24T09:30:51Z,"triage review, bug, oncall: pt2, module: dynamic shapes",CLOSED,0,9,https://github.com/pytorch/pytorch/issues/97511,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,模型训练出错,,,,,
nightlies fail simple linanlg solve undefined symbol sgebak,latest failed torchbench drq train cuda test benchmark actions runs jobs version minimal reproduction add unit test versions broken dev cu previous dev cu good,2023-02-13T18:47:55Z,"high priority, triage review, bug, module: cuda, module: linear algebra",CLOSED,0,9,https://github.com/pytorch/pytorch/issues/94751,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,算子出错,,,,,"torch.linalg.solve() call with ""undefined symbol"
segment cusolverdndestroy,cusolverdn import pybind segment cusolverdncreate cusolverdndestroy note demo success segfault disappear segfault appear thread provide information test py import import example segfault disappear libcusolver say upgarde cuda may fix upgrade cuda segfault usual concise description reproduce steps reproduce behavior paste nanme bind cudevice compile bind cudevice compile paste nanmed test py test py bind cudevice compile test py core dump info sample messages traces provide well expected behavior happen concise description expected happen environment version os linux centos installed pip source source command compiling source version cuda cudnn version gpu models configuration information additionanl context add context problem cusolver base cublas cusparse reference cusolver docs nvidia cuda archive cusolver cusolverdnhandle,2020-06-23T08:46:41Z,"module: dependency bug, needs reproduction, module: crash, module: cuda, triaged",OPEN,0,9,https://github.com/pytorch/pytorch/issues/40425,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子出错,,,,,cusolverDnDestroy()段错误
mkldnn broken,source fails mkl dnn wbug warning thrown reproduce works mkldnn expected behavior builds environment collecting environment information version debug cuda os fedora release twenty nine gcc version gcc red hat cmake version version version cuda available cuda runtime version gpu models configuration gpu geforce rtx nvidia driver version cudnn version collect versions libraries pip numpy blas mkl mkl mkl include mkl fft py mkl random py hd dba additionanl context,2019-06-28T17:58:28Z,"high priority, module: dependency bug, triaged, module: mkldnn",CLOSED,0,9,https://github.com/pytorch/pytorch/issues/22346,"version, mkl, py, cuda, pip, pypi, numpy, gpu, pypi_pypi, cudnn",7,模型出错,,,,,Build with MKLDNN broken
tp fatal internanl compiler,try application new features currently bad supported compiler throw bugs versions libtorch msvc modules project msbuild msvc found several compile definitions resurrect functions deprecated compiler thrown output module remove deprecated functions several places provide request,2022-01-14T15:18:12Z,"module: dependency bug, module: windows, module: cpp, triaged",OPEN,0,9,https://github.com/pytorch/pytorch/issues/71309,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,模型编译出错,,,,,
fused addmm linear causing performance regression xla backend,forcing reshape flattening addmm instead add mm linear layer causing performance regression xla backend pr introduced fused addmm linear performance ms us pr reverted without reshape performance ms us change reshape regression reference model vit model input possible force fused addmm non xla backend xla backend generally sensitive wide tensors flattened backend undo op input reshaped help versions collecting environment information version debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version collect libc version glibc version default sep gcc bit runtime platform linux gcp glibc cuda available cuda runtime version collect gpu models configuration collect nvidia driver version collect cudnn version collect hip runtime version miopen runtime version xnnpack available versions libraries pip mypy extensions pip numpy pip numpydoc blas mkl mkl mkl service py mkl fft py hd mkl random py numpy py hf numpy base py numpydoc pyhd,2022-08-30T02:28:40Z,"bug, triaged, module: xla, module: linear algebra",CLOSED,2,1,https://github.com/pytorch/pytorch/issues/84244,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架后端性能下降,,,,,Fused addmm path in linear causing performance regression in XLA backend
inductor gives obscure fx graph compiled returns tuple,triggers assert inductor logs minified repro,2022-10-30T01:15:39Z,"bug, triaged",OPEN,0,9,https://github.com/pytorch/pytorch/issues/93593,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,模型出错,,,,,Inductor gives obscure error when FX graph to be compiled returns tuple
cpu inductor ir warning fallbackkernel cumsum,trying compile unixcoder model variation bert huggingface transformers cpu version version dev cpu performing model compile model default well reduce overhead machine ram encounter provided idea logs minified repro response,2022-12-22T15:38:38Z,"bug, oncall: pt2",CLOSED,0,9,https://github.com/pytorch/pytorch/issues/93495,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,模型编译出错,,,,,Using FallbackKernel: aten.cumsum
tensordot working dtype lower element given axis,tensordot seems failing one element given axis dtype lower happens providing explicit lists dimensions following runtimebug expected scalar long found works completely float dtypes also works multiple elements given axis instance following works versions,2022-09-05T06:18:47Z,"bug, triaged, module: linear algebra, actionable",OPEN,2,0,https://github.com/pytorch/pytorch/issues/84530,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,算子出错,,,,,tensordot当轴中只有 1 个元素时，不适用于 dtype int32 及更低版本
ppc le test py test failing test qr,running ubuntu ppc le running test py test fails test qr test fails running virtual machine unittest test testtorch test qr qr qr qr big matrix fail test qr test testtorch traceback ubuntu builder jenkins test common py wrapper fn kwargs ubuntu builder jenkins test test py test qr assertequal qr prec ubuntu builder jenkins test common py assertequal asserttensorsequal ubuntu builder jenkins test common py asserttensorsequal assertlessequal err prec message assertionbug less equal ran test failed failures fails running virtual machine baremetal systems seem exhibit failure also particular seems affect failure randn comment big matrix change randn cases test passes random failures someone point randn figure,2017-11-15T16:28:51Z,"todo, module: dependency bug, triaged, module: POWER",CLOSED,0,8,https://github.com/pytorch/pytorch/issues/3716,"test, circleci, job, ci, workflow, disabled, failure, pipeline, flaky, failing",12,模型在特定环境下失败,,,,,test_torch.py test failing in test_qr
inductor compilation breaks multiprocessing,inductor compilation breaks following default settings logs dynanmo succeed dynanmo optimize eager succeed aot succeed dynanmo optimize aot eager succeed inductor succeed dynanmo optimize inductor succeed minified repro,2022-10-20T20:46:26Z,"bug, oncall: pt2",CLOSED,0,8,https://github.com/pytorch/pytorch/issues/93563,"target, test, py, linux, cuda_target, linux_libcudnn, ci, jenkins, build, rocm",3,模型编译出错,,,,,多进程下编译失败
incremental supported windows fact,source ninja officially supported due ninja ninja ninja issues incremental supported windows rebuilds fact rerun setup py install windows far sccache accelerate rebuild nothing changed still takes minutes rerun setup py install cpu ssd hdd takes minutes reproduce steps reproduce behavior clone source windows rerun setup py install rebuild expected behavior edit compile cycle windows take much small change leaf rebuild finished soon linux additionanl context exception message think ninja fails parse visual compiler option rebuilding,2021-06-11T02:33:46Z,"module: dependency bug, module: build, module: windows, triaged",CLOSED,1,4,https://github.com/pytorch/pytorch/issues/59859,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,模型在特定环境下失败,,,,,Incremental build isn't supported on Windows
oom oom forever,assume inductor batch sizes model oom ooms problem ooms also oom even though supposed potentially dynanmo reset might help oom issues fixed master dynanmo init py,2022-10-27T20:18:43Z,"bug, triaged, oncall: pt2",CLOSED,0,8,https://github.com/pytorch/pytorch/issues/93583,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,框架资源调度不合理,,,,,内存耗尽后会一直耗尽
put bugs,msg likely running fallbacks tensor instead graph breaking indexing operations masks,2022-11-14T22:44:49Z,"bug, triaged, oncall: pt2",OPEN,0,8,https://github.com/pytorch/pytorch/issues/93421,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,算子出错,,,,,index_put布尔值索引出错
ddp inductor profiler crashes toy model,running set commands planned demo encountered work around aot eager broken inductor benchmarks dynanmo py toy model ddp dynanmo optimize ddp dynanmo inductor profile note crash happens without ddp graph split optimizer repro disables simplify works aot eager benchmarks dynanmo py toy model ddp dynanmo optimize ddp dynanmo aot eager profile versions clean whole env collecting environment information version git bf debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version ubuntu cmake version version libc version glibc version default jan gcc bit runtime platform linux aws glibc cuda available cuda runtime version cuda module loading set lazy gpu models configuration gpu nvidia gpu nvidia nvidia driver version cudnn version probably one following cuda targets linux libcudnn cuda targets linux libcudnn adv infer cuda targets linux libcudnn adv train cuda targets linux libcudnn cnn infer cuda targets linux libcudnn cnn train cuda targets linux libcudnn ops infer cuda targets linux libcudnn ops train cuda targets linux libcudnn cuda targets linux libcudnn adv infer cuda targets linux libcudnn adv train cuda targets linux libcudnn cnn infer,2023-01-20T17:49:36Z,"high priority, bug, module: crash, triaged, oncall: profiler, oncall: pt2, module: inductor",OPEN,0,8,https://github.com/pytorch/pytorch/issues/92691,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,模型出错,,,,,DDP+inductor+profiler crashes on toy model
cpu implemented numel overflows float,documented implementation cce ade ec nantive reduceops cpp bit us quansight labs numpy interop discussion versions master,2023-01-02T18:58:32Z,"bug, module: cpu, triaged, module: reductions",CLOSED,0,8,https://github.com/pytorch/pytorch/issues/91597,"new, thread, org, version, tutorial, assert, cpu, master, doc, internanl",6,算子资源调度不合理,,,,,torch.mean在float16上溢出
cudnn convolutions,following simple script gist vlasenkov fca ca crashes following traceback reproduce single gpu expected behavior script finishes environment,2019-03-15T08:56:39Z,"module: dependency bug, module: cudnn, triaged",OPEN,0,8,https://github.com/pytorch/pytorch/issues/18053,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子执行错误,,,,,使用3d convolutions时cuDNN出错
libth recognize intel mkl default location,building sources libth find intel mkl default location opt intel mkl include includes opt intel mkl intel worked around adding paths cmake prefix good libth searched default default system blas library linux tremendously slower intel mkl easy notice libth find mkl,2017-02-02T06:29:09Z,"todo, module: dependency bug",CLOSED,0,7,https://github.com/pytorch/pytorch/issues/673,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,框架出错,,,,,无法识别默认位置的MKL
probably allowing mutations happen fake tensor variabletracker,variabletracker supposed immutable checkpointing rollback instruction translation fails create graph break however indirectly store fake tensor variabletracker fake tensor treated immutably particular nodes fake tensor nodes may modify metadata input fake tensor logic appropriately duplicates fake tensors safely modifications hypothetical checkpoint symbolic evaluator squeeze mutates fake tensor discover rollback subsequent incorrect metadata input squeeze discussing jansel unlikely happen practice today rollback right generating graph break point going actually insert nodes graph good correct checkpointing seriously future generally probably sort lint sure variabletrackers actually mutated going turn real bugs immediately corrupt chen weiwen logs response minified repro response,2022-11-04T17:05:50Z,"bug, triaged, module: dynamo",OPEN,0,7,https://github.com/pytorch/pytorch/issues/93610,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,算子异常,,,,,We probably are allowing mutations to happen on fake tensor in VariableTracker
ubuntu install support gpu sources,description building cxx object third party protobuf cmake cmakefiles libprotobuf protobuf generated message table driven ninja stopped subcommand failed caffe installed source command compiling source os version version cuda cudnn version gpu models configuration gcc version compiling source log base ml chengming setup py install building wheel building version cmake gninja dbuild dbuild test dcmake release dcmake install prefix ml dcmake prefix ml ananconda dnumpy include ml ananconda packages numpy core include dpython executable ml ananconda bin dpython include ml ananconda include dpython library ml ananconda libpython dtorch version duse numpy ml cxx compiler identification gnu compiler identification gnu working cxx compiler bin working cxx compiler bin works detecting cxx compiler abi info detecting cxx compiler abi info done detecting cxx compile features detecting cxx compile features done working compiler bin working compiler bin works detecting compiler abi info detecting compiler abi info done detecting compile features detecting compile features done forcing particular blas found performing test compiler works performing test compiler works success performing test support glibcxx performing test support glibcxx success performing test caffe exception ptr supported performing test caffe exception ptr supported success exception ptr supported performing test,2020-08-27T07:03:06Z,"module: dependency bug, module: build, module: cuda, triaged",CLOSED,0,7,https://github.com/pytorch/pytorch/issues/43691,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架安装问题,,,,,ubuntu18.04 install pytorch
omp threads work intended cases,concise description built mkl omp threads set desired number openmp threads greater half number available cores provided useful hyperthreading available cores physical cores reproduce steps reproduce behavior clone current master branch numpy mkl create snippet script omp threads filenanme py number greater half number available cores mkl threads difference either output desired output means users might misled believing omp threads set desired number threads sample messages traces provide well expected behavior set threads environment variable omp threads behaved similar manner environment master branch source version gitc debug os ubuntu lts ubuntu lts gcc version cmake version version libc version glibc libc version glibc version bit runtime platform linux generic ubuntu bionic linux generic glibc versions libraries pip numpy even without numpy mkl launchpad net ubuntu source intel mkl openmp libgomp additionanl context maybe behavior already known find related info,2021-06-15T01:09:47Z,"module: dependency bug, module: docs, triaged, module: openmp, module: multithreading",CLOSED,0,7,https://github.com/pytorch/pytorch/issues/60000,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子出错,,,,,在某些情况下OMP_NUM_THREADS不按预期工作
embeddingbag allows ranges,version embeddingbag allow pass set however version embeddingbag allows range indices without alert informations plus forward api backward api segmentaion without logs intended result versions pip install pip install,2021-12-20T03:36:20Z,"module: dependency bug, triaged",OPEN,0,7,https://github.com/pytorch/pytorch/issues/70170,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子出错,,,,,EmbeddingBag允许range超出索引范围
cudnn batchnorm batch limit eval channel,works batch training input channel instead might related,2019-11-13T20:00:25Z,"module: dependency bug, module: cudnn, module: cuda, triaged",CLOSED,0,7,https://github.com/pytorch/pytorch/issues/29744,"nn, output, forward, input, layer, weight, float, embedding, padding, batch",9,算子功能不完善,,,,,CuDNN batchnorm has batch size limit for eval with channel 
caffe critical cannot caffe,issues guidelines limit issues reports feature requests question help support visit forums discuss org submitting feature request preface feature request submitting report include following information caffe caffe os ubuntu version installed pip source source version cuda cudnn version gpu models configuration nvidia ti gcc version compiling source cmake version command compiling source cmake sudo install versions libraries addition including following information also helpful us diagnose problem script reproduce try provide minimal test possible messages traces context around trying try import core caffe got bugs another information think must help understand problem thanks help,2018-04-04T02:18:37Z,"module: dependency bug, caffe2",CLOSED,0,7,https://github.com/pytorch/pytorch/issues/6259,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,框架出错,,,,,CRITICAL:root:Cannot load caffe2.python. Error
aot autograd hf bigbird output compiledfunctionbackward view modified inplace,following repro fails logs traceback scratch anijain work failures bigbird py module opt fn backward scratch anijain work dynanmo eval frame py fn return fn kwargs scratch anijain work failures bigbird py fn unsqueeze runtimebug output compiledfunctionbackward view modified inplace view created inside custom function input returned autograd logic handle view inplace override custom backward associated custom function leading incorrect gradients behavior forbidden fix cloning output custom function minified repro import import dynanmo def bmm nd inp inp ndim fast nd matrix multiplication faster replacement einsum bhqk bhkd bhqd return bmm inp reshape inp inp reshape inp view inp ndim inp ndim inp ndim dynanmo skip def fn bmm nd unsqueeze return randn requires grad randn requires grad ref fn backward opt fn dynanmo optimize aot eager fn opt fn backward,2022-11-17T07:06:51Z,"bug, triaged, ezyang's list, module: aotdispatch",OPEN,0,7,https://github.com/pytorch/pytorch/issues/93425,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子实现逻辑错误,,,,,pytorch自动求导有问题
batch cholesky runtimebug gpu pos def,trying perform batch cholesky decomposition singular matrix gpu tensor nanns returned instead raising runtimebug cpu reproduce expected behavior behavior consistent cpu gpu runtimebug raised environment master fbcode,2019-02-05T22:13:19Z,"module: dependency bug, triaged, module: third_party",CLOSED,0,7,https://github.com/pytorch/pytorch/issues/16780,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,框架出错,,,,,batch mode torch.cholesky does not raise RuntimeError on GPU if not pos. def.
gumbel softmax produces nann cpu,nanns sometimes output running cpu find exact pattern sampling tensor long enough nann appear output cuda works versions,2023-05-16T21:53:46Z,"bug, module: cpu, triaged",CLOSED,0,7,https://github.com/pytorch/pytorch/issues/101620,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,算子出错,,,,,gumbel_softmax在CPU上导致NaN
exporting operator col im onnx opset version supported,converted model format onnx format received operator col im supported import cvnets import model options opts import segmentation eval arguments def onnx opts segmentation eval arguments model model opts model eval onnx model mobilevit onnx data randn onnx export model data onnx opset version constant folding input nanmes output nanmes return nanme onnx traceback sunseeker project robot seg mobilevit seg demo py module onnx sunseeker project robot seg mobilevit seg demo py onnx onnx export model data onnx opset version constant folding input nanmes opt ananconda envs mobilevit packages onnx init py export return export opt ananconda envs mobilevit packages onnx py export export opt ananconda envs mobilevit packages onnx py export graph params model graph opt ananconda envs mobilevit packages onnx py model graph graph optimize graph opt ananconda envs mobilevit packages onnx py optimize graph graph jit pass onnx graph operator export opt ananconda envs mobilevit packages onnx init py symbolic function return symbolic function kwargs opt ananconda envs mobilevit packages onnx py symbolic function symbolic,2022-09-01T09:43:54Z,"module: onnx, triaged, topic: bug fixes",CLOSED,0,7,https://github.com/pytorch/pytorch/issues/84408,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,模型转换出错,,,,,.pt模型转换为onnx时，不支持运算符col2im。
cuda matrix vector product mv causes illegal memory access,mv causes illegal memory access multiplying matrix elements note matrix fit likely cublas either cublas fixed multiple calls cublassgemv reproduce note gpu example environment master mar cuda,2019-03-11T23:37:44Z,"module: dependency bug, module: cuda, triaged, module: 64-bit, module: cublas",OPEN,1,3,https://github.com/pytorch/pytorch/issues/17897,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子资源调度不合理,,,,,CUDA 大矩阵向量积导致非法内存访问
torchdynanmo inline modified nn modules forward fails huggingface accelerate,raised running huggingface bloom model accelerate support model applying dynanmo optimize example model taken conference demo youtube watch vbtgzl iraw note standard version without arguments device offload works requirements logs minified repro,2022-12-16T01:43:34Z,"high priority, bug, triaged, oncall: pt2",OPEN,0,7,https://github.com/pytorch/pytorch/issues/93484,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,模型出错,,,,,TorchDynamo doesn't inline modified nn.Modules forward - Fails with Huggingface Accelerate
assert based dynanmo tests fail test rewrite assert msg test rewrite assert without msg,typical backtrace logs response minified repro response,2022-11-24T14:30:43Z,"bug, triaged",CLOSED,0,7,https://github.com/pytorch/pytorch/issues/93449,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架出错,,,,,Assert based dynamo tests fail on Python 3.9
caffe uses ffmpeg functions deprecated ffmpeg gone,caffe video video decoder several parts ffmpeg api deprecated ffmpeg org doxygen deprecated removed ffmpeg release nanturally makes caffe part unbuildable ffmpeg also debian bugs debian org cgi bin bugreport cgi partial fix many changes mechanical implement example patch older representative changes needed git head seems take care lot however replacing avcodec decode video friends newer avcodec receive frame probably takes care thought versions behavior present current master head writing although partial patch older version,2022-02-03T11:50:46Z,"module: dependency bug, caffe2, triaged",OPEN,1,3,https://github.com/pytorch/pytorch/issues/72254,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,算子已被弃用,,,,,FFMPEG已经被弃用
dynanmo inference tensors track version counter inference llama,transformers repro log versions version gitb debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version version libc version glibc version default mar gcc bit runtime platform linux generic glibc cuda available cuda runtime version cuda cuda module loading set gpu models configuration cuda nvidia driver version cuda cudnn version cuda hip runtime version miopen runtime version xnnpack available cpu architecture cpu op bit bit byte little endian address sizes bits physical bits virtual cpu cpu thread core core socket socket numa vendor genuineintel cpu family model model nanme intel xeon platinum cpu ghz stepping cpu mhz cpu mhz cpu mhz bogomips cache mib cache mib cache mib cache mib numa cpu numa cpu vulnerability itlb multihit affected vulnerability tf affected vulnerability mds affected vulnerability meltdown affected vulnerability mmio stale data mitigation cpu buffers smt vulnerable vulnerability retbleed affected vulnerability spec store bypass mitigation speculative store bypass disabled via prctl seccomp vulnerability spectre mitigation usercopy swapgs barriers pointer sanitization vulnerability spectre mitigation enhanced ibrs ibpb conditionanl rsb filling pbrsb eibrs sw sequence vulnerability srbds affected vulnerability tsx async abort affected flags fpu vme de pse tsc msr pae mce cx apic sep mtrr pge mca cmov pat pse clflush dts acpi mmx fxsr sse sse ss,2023-05-11T02:52:13Z,"bug, oncall: pt2",CLOSED,0,6,https://github.com/pytorch/pytorch/issues/101151,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架出错,,,,,"Error ""Inference tensors do not track version counter"" in inference_mode w/ llama7b"
cat unexpected behavior,expect following,2017-01-28T22:33:40Z,"high priority, module: dependency bug",CLOSED,0,6,https://github.com/pytorch/pytorch/issues/627,"behavior, result, reproduce, expected, cpu, numpy, expected_behavior, index, different, detail",13,算子出错,,,,,torch.cat出现无法预料的行为
links pr ci runs commit missing pr,next commit ci commit missing dev infra,2021-09-17T21:25:40Z,"high priority, module: dependency bug, module: ci",CLOSED,0,6,https://github.com/pytorch/pytorch/issues/65246,"test, circleci, job, ci, workflow, disabled, failure, pipeline, flaky, failing",12,框架版本迭代问题,,,,,
macos ios ci failing ssh connect host port operation timed,seems due ssh able reach affects binanry normal ci workflows checkout update homebrew circleci pipelines workflows adf baae jobs circleci pipelines workflows jobs circleci pipelines workflows jobs circleci pipelines workflows adf baae jobs circleci pipelines workflows jobs dev infra,2021-10-13T15:00:55Z,"high priority, module: dependency bug, module: ci, triaged, ci: sev",CLOSED,0,6,https://github.com/pytorch/pytorch/issues/66552,"test, circleci, job, ci, workflow, disabled, failure, pipeline, flaky, failing",12,服务器连接失败,,,,,"MacOS and iOS CI failing with ""ssh: connect to host github.com port 22: Operation timed out"""
tensor scatter accepts bogus input,valgrind reports reading tensor,2017-05-25T15:16:40Z,"high priority, module: dependency bug",CLOSED,0,6,https://github.com/pytorch/pytorch/issues/1653,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,算子异常,,,,,Tensor.scatter_ accepts bogus input where index size > src size
runtimebug cudnn status supported,found runtime reproducing folowing,2017-05-26T19:14:35Z,"module: dependency bug, module: cudnn, triaged",CLOSED,0,6,https://github.com/pytorch/pytorch/issues/1664,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,框架资源调度不合理,,,,,CUDNN_STATUS_NOT_SUPPORTED
dense backward ignores unspecified elements sparse inputs,description historical reasons dense backward sparse inputs implements masked semantics contradicts current interpretation sparse tensors sparse tensors semantically equivalent strided tensors sparse format considered merely memory optimization define mask operations sparse tensors masked semantics tensors currently implemented masked future sparse deprecated breaks autograd sparse tensors dense method required operations resulting sparse tensors example example recommendation example gradcheck non masked semantics fails succeeds masked semantics possible solutions solution implemented introduces global context defines operations backward implementations interpret unspecified elements sparse tensors introduce masked kw argument default dense enanbles explicit control semantics dense method example following examples succeed discussion solutions pros cons discussions bc breaking default semantics operations nanmespace switch non masked semantics cannot avoided pursuing idea considering sparse tensors semantically equivalent strided tensors fortunantely tensor operations nanmespace sparse tensors already implement non masked semantics example failure expected transposing indexing non masked semantics indexing op circumvents sparse output supported gradcheck yet exception current dense future perspective masked,2023-02-25T11:23:34Z,"bug, module: sparse, module: bc-breaking, module: autograd, triaged",CLOSED,0,6,https://github.com/pytorch/pytorch/issues/95550,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,算子出错,,,,,torch.to_dense忽略稀疏输入中未指定的元素
sleef failure usage bin mkalias vector vector fp vector mangled isa extension,gnu log people freebsd org yuri py failure log rc freebsd,2019-05-12T02:32:30Z,"module: dependency bug, module: build, triaged, module: sleef",CLOSED,0,6,https://github.com/pytorch/pytorch/issues/20404,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架出错,,,,,Sleef 构建失败
pack padded sequence fails dynanmo due requiring non fake nd argument,pack padded sequence op takes nd argument tensor integers current implementation fails dynanmo trying fake nd argument converted fake tensor repro stacktrace failure occurs ce bbe nantive packedsequence cpp logs response minified repro response,2023-01-10T05:03:16Z,"bug, triaged, oncall: pt2, mlperf",OPEN,0,6,https://github.com/pytorch/pytorch/issues/93501,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子出错,,,,,_pack_padded_sequence 在 dynamo 中失败 
valgrind chokes rdrand gcc libstdc,attempted valgrind past ran bugs kde org show cgi apparently culprit march nantive going valgrind track testing indeed resolves making easy turn march nantive valgrind,2018-04-18T13:59:12Z,"module: dependency bug, module: cpu, triaged",CLOSED,0,6,https://github.com/pytorch/pytorch/issues/6705,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,框架出错,,,,,valgrind在rdrand 上被阻塞
torchinductor illegal memory access forever,hit trying reproduce sure exactly new one repro running nantive got running dynanmo inductor got actually batch search inductor keeps failing even batch less think generated triton change input changes logs response minified repro response,2022-11-01T05:25:49Z,"bug, triaged",CLOSED,0,6,https://github.com/pytorch/pytorch/issues/93597,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子资源调度不合理,,,,,非法内存访问
add source,add fail backward gradient succeed without exception versions,2023-01-18T20:35:44Z,"bug, triaged, actionable",CLOSED,0,6,https://github.com/pytorch/pytorch/issues/92576,"backward, gradient, sparse, version, matrix, focus, run_focus, run, pas, one",15,算子出错,,,,,index_add_ should error if shape of source is wrong
fake tensor fails codegen model,logs tiger packages dynanmo eval frame py userwarning tensorfloat tensor cores float matrix multiplication available enanbled consider setting set float matmul precision high tensorfloat tensor cores float matrix multiplication available enanbled traceback tiger packages dynanmo py return target kwargs tiger packages subclasses fake tensor py dispatch func kwargs tiger packages ops py return op kwargs tiger packages prims common wrappers py fn result fn kwargs tiger packages prims common wrappers py fn result fn bound arguments tiger packages refs init py lambda expected predicate got pred dtype tiger packages prims common init py exc runtimebug expected predicate got uint exception direct following exception traceback tiger packages dynanmo py fake lambda tx output kwargs nnmodule tiger packages dynanmo py wrap fake exception return fn tiger packages dynanmo py lambda lambda tx output kwargs nnmodule tiger,2022-12-22T11:33:54Z,"bug, oncall: pt2",CLOSED,0,6,https://github.com/pytorch/pytorch/issues/91306,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,模型出错,,,,,codegen模型出错
inductor accuracy failure pegasusforcausallm maybe bmm related,applying plus fixes perturbs output aotautograd sufficiently inductor generate accuracy failure worry repro logs minified repro gist ezyang ae ec,2022-10-30T02:13:59Z,"bug, triaged",CLOSED,0,6,https://github.com/pytorch/pytorch/issues/93595,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,模型精度问题,,,,,AOTAutograd 的输出产生精度故障
addition batch csr tensors produces incorrect invalid csr tensor,description example expected result place addition produces garbage well system info master,2023-02-06T15:01:12Z,"bug, module: sparse, triaged, module: correctness (silent)",OPEN,1,2,https://github.com/pytorch/pytorch/issues/94183,"new, thread, org, version, tutorial, assert, cpu, master, doc, internanl",6,算子异常,,,,,CSR 张量相加会产生不正确且无效的 CSR 张量
archive org failed dataset downloads,feature upon failure download built dataset downloader try downloading archive org motivation day host mnist dataset resulting service unanvailable attempting download feature mnist dataset class since common hosts go variety reasons good idea alternante means downloading data directly convenient interface provided datasets package upon failure download dataset default dataset downloader attempt download one several backup locations one reliable backup location archive org snanpshots internet taken regular basis example labels test set mnist dataset found web archive org web yann lecun exdb mnist labels idx ubyte gz alternantives possible alternantives include rather rely hosts team choose host datasets dataset data available peer peer fashion bittorrent similar protocol alternantive seem feasible since rely significant amount resources second alternantive seems challenging implement originanl proposal additionanl context dev infra,2021-03-12T12:59:16Z,"module: dependency bug, triaged, enhancement",OPEN,0,5,https://github.com/pytorch/pytorch/issues/53891,"behavior, result, reproduce, expected, cpu, numpy, expected_behavior, index, different, detail",13,框架接口下载数据集报错,,,,,
dynanmo accuracy minifier seem catch,following program appear trigger accuracy minifier troubleshooting docs org docs master dynanmo troubleshooting accuracy debugging neither seem catch versions version dev cpu debug cuda rocm os debian gnu linux rodete gcc version debian clang version cmake version version libc version glibc version gcc bit runtime platform linux rodete amd glibc cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda hip runtime version miopen runtime version xnnpack available versions libraries pip numpy rc pip dev cpu pip dev cpu collect chen weiwen,2022-12-07T17:47:07Z,"bug, oncall: pt2, module: dynamo",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/90397,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子异常,,,,,Accuracy minifier doesn't seem to catch bug
onnx exporting adaptive pool onnx,trying export model gabrielmittag nisqa onnx following bugs warnings segmentation lts runtimebug unsupported onnx export operator adaptive pool since output factor input free request support submit request export latest opset available version respectively replicate made fork miccio dk nisqa export script called export py script similarly predict py accepts input parameters versions environment environment,2022-04-06T23:10:47Z,"bug, module: crash, module: onnx, triaged, onnx-triaged",OPEN,0,5,https://github.com/pytorch/pytorch/issues/75383,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,算子转换错误,,,,,将 Adaptive_max_pool2d 导出到 ONNX 时出错
debug torchscript fastnlp bert,later jit fails tries jit script sounds missing ops perhaps easy fix prevent torchscript based backends working models look familar,2022-03-23T03:54:35Z,"bug, oncall: pt2",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/93742,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,模型出错,,,,,fastNLP_Bert中出现TorchScript 错误
missing lapack,description installing jetson xavier agx steps wget nvidia box shared static phqe cbhqjohwtvxorrwnmrnfx whl cp cp linux aarch whl pip install numpy cp cp linux aarch whl sudo apt install libjpeg dev zlib dev git clone branch vision cd sudo setup py install cd cd git clone recursive branch export nccl export export cuda arch export version export number sudo apt install pip cmake sudo pip install setuptools sudo pip install requirements txt pip install scikit pip install ninja setup py bdist wheel simple functions solve lu receving example import randn lu pivots lu traceback stdin module xavier packages functionanl py lu result lu info pivot pivot bugs infos runtimebug lu lapack library found compilation system info version debug cuda os ubuntu lts gcc version ubuntu linanro ubuntu cmake version version version cuda available cuda runtime version gpu models configuration collect nvidia driver version collect cudnn version aarch linux gnu libcudnn versions libraries pip numpy pip pip collect caffe installed pip source described command compiling source described,2020-01-17T08:30:15Z,"module: dependency bug, module: build, triaged",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/32344,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架异常,,,,,缺少LAPACK 库汇编
getitem returned overloadpacket instead opoverload dispatch,caught testing tags torchdispatchmode versions master,2022-07-15T18:53:36Z,"bug, triaged, module: __torch_dispatch__",OPEN,0,5,https://github.com/pytorch/pytorch/issues/81565,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,算子异常,,,,,__getitem__ is returned as an OverloadPacket instead of an OpOverload in __torch_dispatch__
macbook gpu support mps results random conversion casting tensor longtensor,simple test output random numbers example cast tensor longtensor cpu output versions stable mac default macbook macbook pro chip monterey,2022-07-15T19:07:36Z,"bug, triaged, module: mps",CLOSED,1,1,https://github.com/pytorch/pytorch/issues/81567,"new, thread, org, version, tutorial, assert, cpu, master, doc, internanl",6,框架出错,,,,,张量转换为 LongTensor 类型时，Macbook M1 GPU 错误
bugs permute function mps,class fcn nn module def init super fcn init conv nn conv gap nn adaptiveavgpool fc nn linear def forward batch conv gap view batch fc log softmax return nanme model fcn mps model randn permute mps shows assertion failed mapit jitvaluetypes function getstatictype mpsruntime project model randn mps work well versions version dev debug cuda rocm os macos gcc version collect clang version clang cmake version version libc version version packaged mar clang bit runtime platform macos bit cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda hip runtime version miopen runtime version xnnpack available versions libraries pip numpy pip dev pip torchaudio dev pip dev dev pypi pypi torchaudio dev pypi pypi dev pypi pypi,2022-07-15T16:00:43Z,"bug, triaged, module: mps",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/81557,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子出错,,,,,使用 MPS 的排列函数出错
compile bring better performance even lower compile possible reason,response logs response minified repro response,2022-12-07T17:00:43Z,"bug, oncall: pt2",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/93472,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子异常,,,,,torch.compile 并没有带来更好的性能，甚至还低于不编译
cuda gdb crash,minimal extension cuda gdb crashes reproduce put together minimal docker container capturing andyljones cuda gdb particular command andyljones cuda gdb master sh calls script andyljones cuda gdb master bad cu expected behavior minimal container also contrasting good cu andyljones cuda gdb master good cu script crash cuda gdb environment inside container additionanl context got example starting extension la tutorial org tutorials advanced cpp extension pulling bits easy read find nvidia cuda gdb issues daniilidis neural renderer issues people reporting one june one august assume due something provide specs magic combinantion libraries causing lot help script generate something rolling minimal example back shared libs apparently changed since gave trying going number reported cuda gdb internanls changed bit since repo nvidia cuda gdb updated yrs ago,2019-08-11T15:02:33Z,"module: dependency bug, module: cuda, triaged",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/24159,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,框架出错,,,,,当最小的 PyTorch 扩展在 cuda-gdb 中运行时会崩溃
compile fails broadcast distributions,description broadcast distributions causes compile function fail assertionbug op returned non tensor dtype function built function default dtype example system info version dev cu debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version version libc version glibc version gcc bit runtime platform linux generic glibc cuda available cuda runtime version collect cuda module loading set lazy gpu models configuration gpu nvidia pcie gpu nvidia pcie gpu nvidia pcie gpu nvidia pcie gpu nvidia pcie gpu nvidia pcie nvidia driver version cudnn version collect hip runtime version miopen runtime version xnnpack available versions libraries pip mypy extensions pip numpy rc pip dev cu pip torchaudio dev cu pip torchmetrics pip torchtriton pip dev cu collect chen weiwen,2022-12-10T16:26:09Z,"bug, module: distributions, oncall: pt2, module: dynamo",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/90623,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子出错,,,,,torch.compile 失败
extremely slow fusion create cycle nannogpt generate,nannogpt generate benchmark compiles extremely slowly looking py spy top looks spend lot fusion create cycle noticed model nodes probably triggering bad asymptotics versions chen weiwen,2023-05-31T16:23:21Z,"bug, triaged, oncall: pt2, module: inductor",OPEN,0,5,https://github.com/pytorch/pytorch/issues/102622,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,算子性能异常,,,,,nanogpt_generate 上的 will_fusion_create_cycle 速度极慢
gtk segfault ubuntu,setup segfault exit gtk loaded nn backtrace important point linux gnu libllvm gets loaded gtk snippet reproduce versions version cu debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version ubuntu cmake version version libc version glibc version apr gcc bit runtime platform linux generic glibc cuda available cuda runtime version gpu models configuration gpu nvidia geforce rtx laptop gpu nvidia driver version cudnn version probably one following linux gnu libcudnn linux gnu libcudnn adv infer linux gnu libcudnn adv train linux gnu libcudnn cnn infer linux gnu libcudnn cnn train linux gnu libcudnn ops infer linux gnu libcudnn ops train hip runtime version miopen runtime version xnnpack available versions libraries pip mypy pip mypy extensions pip numpy pip cu collect,2022-05-01T07:44:05Z,"module: dependency bug, triaged, has workaround",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/76631,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架特殊环境下出错,,,,,Ubuntu 22.04 上的 GTK4 段错误
clang hits internanl compiling source macos,getting segmentation building source reproduce macosx deployment target clang cxx clang setup py install expected behavior successful segmentation environment additionanl context maxunpoolkernel crash txt files maxunpoolkernel crash txt maxunpoolkernel sh txt files maxunpoolkernel sh txt,2021-09-23T20:54:51Z,"high priority, module: dependency bug, module: build, triaged, module: macos",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/65578,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,框架编译错误,,,,,在 MacOS 上从源代码编译 PyTorch 时 clang-1300 遇到内部错误
cuda memory unpooling,hello tested following import numpy np import import nn nn import nn functionanl import models models autograd import variable import optim optim class net nn module def init super net init conv nn conv kernel padding conv nn conv kernel padding def forward conv pool kernel stride return indices unpool kernel stride conv return model net model cuda optimizer optim sgd model parameters lr momentum model train batch np zeros dtype float data variable tensor batch data data cuda output model data memory usage gpu increasing runtimebug cuda runtime memory comment unpooling stable memory usage missing something memory unpooling layer ubuntu cuda tested got result help,2017-01-30T16:35:01Z,"high priority, module: dependency bug",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/642,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,框架资源调度不合理,,,,,CUDA内存泄露
speedup inductor backend,simplified example following performance gain observed dynanmo triton installed machine validated triton working expected however dynanmo seems triton accelerate matmul operation dynanmo optimization running increased seconds seconds logs bugs encountered performance bad output side tensor device cuda dtype float minified repro response,2022-11-04T21:52:01Z,"bug, oncall: pt2",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/93611,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,框架性能异常,,,,,No speedup using inductor backend
found pybind dec ref gil released,updating pybind pr new version added assert sure gil held inc ref dec ref object underlying reference count seems failing one tests actions runs jobs since gil held invalid reference counts develop corrupt gc lead memory leaks crashes versions ran master actions runs jobs,2022-12-21T16:40:44Z,"bug, triaged, module: pybind, shadow review",OPEN,0,5,https://github.com/pytorch/pytorch/issues/91251,"new, thread, org, version, tutorial, assert, cpu, master, doc, internanl",6,框架异常,,,,,Potential bug found with pybind11 dec_ref while gil released
dynanmo graph compilation invoked np sqrt,find np sqrt lead graph compilation math sqrt brief search issues find realted think might one one minimal example reproduce message got ce versions chen weiwen,2022-12-19T14:55:16Z,"bug, triaged, oncall: pt2, module: dynamo",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/91101,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,算子异常,,,,,np.sqrt可能会导致图形重新编译
rocm test failures upgrade,rocm ci upgrade tests regressed specifically fft tests test spectral ops py test grid sample test nn py keep passing ci signanl disable temporarily raised track regression reproduce steps reproduce behavior install rocm test test nn py test test spectral ops py additionanl context amd,2021-03-23T19:59:50Z,"high priority, module: dependency bug, module: rocm, triaged",OPEN,0,5,https://github.com/pytorch/pytorch/issues/54535,"target, test, py, linux, cuda_target, linux_libcudnn, ci, jenkins, build, rocm",3,框架异常,,,,,ROCm4.1升级期间测试失败
segfault nn functionanl conv macos catalinan,nn functionanl conv produces segmentation float data enough padding concise description reproduce steps reproduce behavior sample messages traces provide well process users identifier version nantive parent process bash responsible terminanl date os version mac os report version bridge os version awake since boot seconds since wake seconds system integrity protection enanbled crashed thread dispatch queue apple thread exception exc bad access sigsegv exception codes kern invalid address fc cdc exception note exc corpse notify terminantion signanl segmentation terminantion reason nanmespace signanl xb terminanting process exc handler vm regions near fc cdc linkedit rwx sm cow users dylib malloc fc cdc fc rw rwx sm prv thread crashed dispatch queue apple thread libsystem platform dylib fff ceedd platform bzero variant haswell libtorch cpu dylib void mkldnn impl cpu jit gemm convolution im col float mkldnn impl cpu jit gemm conv conf const float const float libtorch cpu dylib da mkldnn impl cpu gemm convolution fwd execute forward const operator const lambda mkldnn impl cpu anonymous nanmespace im pos const mkldnn impl cpu anonymous nanmespace im pos mkldnn impl cpu anonymous nanmespace im pos mkldnn impl cpu anonymous nanmespace im pos const operator mkldnn impl cpu anonymous nanmespace im pos const mkldnn impl cpu anonymous nanmespace im pos mkldnn impl cpu anonymous nanmespace im pos,2020-06-10T23:04:39Z,"high priority, module: dependency bug, module: crash, module: convolution, module: macos, module: mkldnn",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/39824,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子出错,,,,,MacOS Catalina 上 torch.nn.function.conv2d 上的 SegFault
compile failure due old cmake,trying working power rhel system cmake compile hit following searching turned following describing problem success newer cmake gflags gflags issues switched cmake compile worked suggest bumping minimum cmake avoid future thanks,2017-12-12T23:45:42Z,"module: dependency bug, module: build, triaged",CLOSED,1,1,https://github.com/pytorch/pytorch/issues/4141,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,框架编译错误,,,,,由于旧的 cmake 导致编译失败
test cudart register work rocm,patch fails rocm guessing behavior difference cudahostregister rocm equivalent works,2020-09-23T14:33:15Z,"module: dependency bug, module: rocm, triaged",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/45202,"target, test, py, linux, cuda_target, linux_libcudnn, ci, jenkins, build, rocm",3,算子异常,,,,,test_cudart_register doesn't work on ROCm
macosx cuda os failed operation supported os,hi compiled master tree ebdec fd ffb cd becc days ago sources macosx following command successfull however tutorial org tutorials beginner blitz tensor tutorial sphx glr beginner blitz tensor tutorial py ends gpu nvidia gt installed cuda cudnn works perfectly theano clang info anyone know receive hints appreciated thanks,2017-07-10T01:42:35Z,"module: dependency bug, needs reproduction",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/2029,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,框架异常,,,,,MacOSX、CUDA、操作系统调用失败
sparse mask method ignores masked elements sparse compressed input tensors,description current behavior sparse compressed inputs contradicts sparse mask org docs generated tensor sparse mask tensor sparse mask note returned sparse tensor indices sparse tensor mask even corresponding values zeros well current behavior strided sparse coo inputs example expected result meet documented specification behavior coo strided inputs system info version,2023-08-17T15:09:50Z,"bug, module: sparse, triaged, module: correctness (silent)",OPEN,0,5,https://github.com/pytorch/pytorch/issues/107381,"backward, gradient, sparse, version, matrix, focus, run_focus, run, pas, one",15,算子异常,,,,,sparse_mask method ignores masked-in elements of sparse compressed input tensors
convtranspose extremely slow gpu even slower cpu,hi confused nn convtranspose extremely slow running gpu even slower cpu reproduce steps reproduce behavior test trans conv py gpu testing omp threads cuda visible devices test trans conv py cpu testing omp threads cuda visible devices test trans conv py output gpu test average cost ms cpu test average cost ms however running everything works expected gpu test average cost ms optimizations done apply sample messages traces provide well nvprof output cudnn detail dgrad alg contributes kernel cost long debug accelerate convtranspose suggestion appreciated thanks expected behavior concise description expected happen gpu version convtranspose much faster comparable faster cpu version environment version os linux linux installed pip source pip command compiling source version cuda cudnn version cpu models cpu intel xeon platinum cpu ghz gpu models configuration nvidia smi output lspci output information additionanl context add context problem,2020-07-14T02:40:13Z,"module: dependency bug, module: performance, module: cudnn, module: convolution, triaged",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/41382,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子性能异常,,,,,ConvTranspose1d 在 GPU (T4) 上非常慢，甚至比 CPU 还慢
lowrankmultivariatenormal creates illegal memory access magma spotrf batched,reported forum discuss org lowrankmultivariatenormal throws cuda eliaz thanks reporting lowrankmultivariatenormal distribution distribution logits every pixel distribution squared images even following work notebook set also different gpus results works cpus reproduce reproduce added eliaz well since illegal memory access always triggered multiple iterations added reproduced dev cuda cuda gdb output seems point cholesky factorization icl cs utk edu projectsfiles magma doxygen magma potf ga aee,2020-07-14T10:15:10Z,"module: dependency bug, module: distributions, module: cuda, triaged, module: linear algebra",CLOSED,0,5,https://github.com/pytorch/pytorch/issues/41394,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子资源调度不合理,,,,,LowRankMultivariateNormal 在 magma_spotrf_batched 中创建非法内存访问
set data permits requires grad integer tensor,however supposed illegal integer tensor requires grad versions master,2022-04-19T00:54:17Z,"module: autograd, triaged, topic: bug fixes",CLOSED,1,0,https://github.com/pytorch/pytorch/issues/76008,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,算子异常,,,,,set data permits requires_grad=True on integer tensor
unexpected assertionbug export tensor numel,hi got unexpected result playing export api repro outputs following try repro produces following,2022-09-22T20:03:23Z,"bug, triaged, oncall: pt2",OPEN,0,4,https://github.com/pytorch/pytorch/issues/93686,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,框架错误,,,,,导出tensor.numel时出现断言错误
compiled mish function slower eager cpu,wanted test benefits compile function access powerful gpu speed computations cpu tested inductor match hadnwritten mish activation org docs stable generated nn mish however compiled function slower handwritten eager function builtin mish function results input tensor cores laptop physical cores expected behaviour cuda affected thanks result input elements input tensor elements functions take ms except inductor takes ms gen environment version dev cpu cpu intel core hq cpu ghz os ubuntu lts logs response minified repro response,2023-01-01T11:14:12Z,"bug, triaged, oncall: pt2, module: cpu inductor",OPEN,0,4,https://github.com/pytorch/pytorch/issues/93498,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子性能异常,,,,,torch.compiled mish 函数比 eager 慢 5 倍
pooling inner loop non contiguous memory read write pool performance cpu backend,timm ese vovnet dw model pooling poor performance caused non contiguous memory read write logs response minified repro generated pooling inner loop non contiguous memory read write,2023-01-04T13:52:39Z,"bug, triaged, oncall: pt2",CLOSED,0,4,https://github.com/pytorch/pytorch/issues/93500,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,算子异常,,,,,max_pooling's inner loop has non-contiguous memory read-write which get a pool performance for CPU backend
instructions building asan,following instructions master md building asan fails cmake problem bd cbb ae fa aaebc md plain cxx flags spelled shared libasan missing fix replace hand seems work ci bd cbb ae fa aaebc jenkins asan sh speculations follow related misspelling cxx flags scripts picks cxxflags environment dev infra,2022-04-19T09:58:36Z,"module: docs, module: ci, triaged, topic: bug fixes",OPEN,1,0,https://github.com/pytorch/pytorch/issues/76028,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,框架出错,,,,,使用 ASAN 构建 PyTorch 存在错误
runtimebug one variables needed gradient modified inplace operation,runtimebug one variables needed gradient modified inplace operation cuda halftensor version expected version instead hint enanble anomaly detection find operation failed compute gradient autograd set detect anomaly logs inductor ir warning devicecopy inductor ir warning devicecopy inductor ir warning devicecopy inductor ir warning devicecopy inductor ir warning devicecopy inductor ir warning devicecopy inductor ir warning devicecopy inductor ir warning devicecopy inductor compile fx warning skipping cudagraphs due multiple devices inductor compile fx warning skipping cudagraphs due multiple devices inductor compile fx warning skipping cudagraphs due multiple devices inductor compile fx warning skipping cudagraphs due multiple devices inductor compile fx warning skipping cudagraphs due multiple devices inductor compile fx warning skipping cudagraphs due multiple devices inductor compile fx warning skipping cudagraphs due multiple devices inductor compile fx warning skipping cudagraphs due multiple devices tiger packages cuda graphs py userwarning cuda graph ususally means graph attempted captured device stream triggered internanlly cuda cudagraph cpp super cudagraph capture tiger packages cuda graphs py userwarning cuda graph ususally means graph attempted captured device stream triggered internanlly cuda cudagraph cpp super cudagraph capture tiger packages cuda graphs py userwarning cuda graph ususally means graph attempted,2023-01-17T09:31:16Z,"bug, triaged, oncall: pt2",OPEN,0,4,https://github.com/pytorch/pytorch/issues/93510,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,算子异常,,,,,RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
inductor cpu torchbench model lennanrd jones performance regression ww,compare torchinductor cpu performance dashboard issues ww performance regression torchbench model lennanrd jones ww bellow ww ww batch speedup inductor eager batch speedup inductor eager speedup ratio eager ratio inductor ratio ww sw info sw commit master commit fac commit fac da ab commit fab ff cdc ee torchbench benchmark commit db torchaudio ecc audio commit ecc ca dd audio commit ace fa torchtext text commit efd bc cfde fc text commit fc eef ae efe vision commit efed fcbbaa vision commit afb adc torchdata data commit cd af fea fa cecd data commit afa bef ww sw info sw commit master commit fac commit fac da ab commit fab ff cdc ee torchbench,2023-01-12T08:51:24Z,bug,CLOSED,0,4,https://github.com/pytorch/pytorch/issues/93504,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,算子性能异常,,,,,Torchbench model lennard_jones performance regression > 10% on ww02.3 
compile dynanmo exc unsupported setattr userdefinedobjectvariable yolov,minimal repro stacktrace interesting move model outer def different thrown looks guess problem compile work well nn sequential therefore recognize inner module mutable variable caused graph break decorate inner module model model dynanmo optimize eager nopython model model inner module marked correctly mutable graph break versions version gitc bd debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version version libc version glibc version default gcc bit runtime platform linux generic glibc cuda available cuda runtime version cuda module loading set lazy gpu models configuration gpu nvidia geforce rtx nvidia driver version cudnn version probably one following linux gnu libcudnn linux gnu libcudnn adv infer linux gnu libcudnn adv train linux gnu libcudnn cnn infer linux gnu libcudnn cnn train linux gnu libcudnn ops infer linux gnu libcudnn ops train hip runtime version miopen runtime version xnnpack available chen weiwen,2023-05-11T08:03:32Z,"bug, triaged, oncall: pt2, module: dynamo",OPEN,0,4,https://github.com/pytorch/pytorch/issues/101168,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子异常,,,,,Unsupported: setattr(UserDefinedObjectVariable) for yolov7
assertionbug unknown expression,master logs response minified repro response,2022-10-26T13:50:52Z,"bug, triaged",OPEN,0,4,https://github.com/pytorch/pytorch/issues/93580,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,测试用例报错,,,,,AssertionError: Unknown expression s2
cublaszgemm tridedbatched produces incorrect result non contiguous cases trans,expected produce tensor filled ones note imaginanry part inputs yet produces cublaszgemm tridedbatched called trans un materialized conjugation pr,2021-08-27T17:56:59Z,"module: dependency bug, module: cuda, triaged, module: complex, module: cublas, module: linear algebra",CLOSED,0,4,https://github.com/pytorch/pytorch/issues/64103,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,算子错误,,,,,cublasZgemm{S/C}tridedBatched produces incorrect result for some non-contiguous cases when trans='c' 
inductor seg inductor segfaulting amp models,following repro segfaults note flaky might multiple times logs traceback scratch anijain work fail triton py module triton fused scatter add new zeros scatter add new zeros unsqueeze expand async compile triton scratch anijain work inductor codecache py triton return kernel source scratch anijain work inductor codecache py kernel kernel precompile scratch anijain work inductor triton ops autotune py precompile launchers scratch anijain work inductor triton ops autotune py listcomp precompile config warm cache scratch anijain work inductor triton ops autotune py precompile config binanry triton compile scratch anijain work triton triton compiler py compile asm shared kernel nanme compile fn signanture device constants configs warps stages scratch anijain work triton triton compiler py compile nanme asm shared mem triton gen compile ttir backend module device warps stages extern libs runtimebug internanl triton ptx codegen segmentation core dumped minified repro ctypes import void long import import random import strided strided device inductor codecache import asynccompile ops assert stride dynanmo guards assert stride async compile asynccompile import triton import triton language tl inductor triton ops autotune import grid import cuda getcurrentrawstream cuda stream triton fused scatter add new zeros scatter add new zeros unsqueeze expand async compile triton import triton import triton language tl,2022-11-19T02:04:36Z,"bug, triaged, oncall: pt2",OPEN,0,4,https://github.com/pytorch/pytorch/issues/93434,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,模型出错,,,,,Inductor segfaulting with few AMP models
aotautograd fails simple view transpose,revealed minifying test pytest generated test adapter hub adapter transformers py test issues repro explanantion compiling running repro causes aotautograd compile repro bugs interestingly either linear inplace division removed occur,2023-05-30T20:51:20Z,"bug, triaged, oncall: pt2, module: functorch, module: aotdispatch",OPEN,0,4,https://github.com/pytorch/pytorch/issues/102564,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子执行失败,,,,,AOTAutograd 在简单视图/转置情况下失败
partitioner generates useless constant symint edges forward backwards,suppose backwards tracing symbolic however later discovered fact constant added parameter fx graph still passes symbolic around happen know constant partitioner partitioner tell symint constant generate edge forwards backwards transfer constant totally unnecessary symbolic shapes branch currently hacking around commits ff eefd fc forces constant nodes const propagated partition logs response minified repro response,2022-11-23T04:24:14Z,"bug, triaged, oncall: pt2",OPEN,0,4,https://github.com/pytorch/pytorch/issues/93443,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架接口功能不完善,,,,,前向传播反向传播功能完善
batch needs input checks,right batch accepts weight running shapes complain checks nn master batchnormalization lua probably moved cuda side,2016-12-30T10:53:12Z,module: dependency bug,CLOSED,1,0,https://github.com/pytorch/pytorch/issues/378,"nn, output, forward, input, layer, weight, float, embedding, padding, batch",9,算子功能不完善,,,,,Batch normalization需要输入维度检查
torchbench moco runtimebug tensors must cuda dense,tb model moco logs look failure op seems model explicitly calling ops rather calling ddp think support minified repro response,2022-11-02T18:28:13Z,"bug, triaged",OPEN,0,4,https://github.com/pytorch/pytorch/issues/93604,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,模型出错,,,,,TorchBench - moco - RuntimeError: Tensors must be CUDA and dense
support setattr arbitrary provided types tracing,dynanmo already support patching nn module attribute outside forward model initialization cases detectrons rcnn model patch nn module attribute forward method internanl fburl vvekrxl dynanmo support right logs traceback stdin module shunting cpython install importlib init py reload bootstrap exec spec module frozen importlib bootstrap exec frozen importlib bootstrap externanl exec module frozen importlib bootstrap frames removed shunting learn misc py module gm guards dynanmo export mymodule inputs graph tracing symbolic shunting dynanmo eval frame py export result traced opt kwargs shunting nn modules module py impl return forward kwargs shunting dynanmo eval frame py forward return dynanmo ctx orig mod forward kwargs shunting dynanmo eval frame py fn return fn kwargs shunting dynanmo eval frame py catch bugs return callback frame cache hooks shunting dynanmo convert frame py fn return fn kwargs shunting dynanmo py wrapper func kwargs shunting dynanmo convert frame py convert frame assert return compile shunting dynanmo convert frame py compile transform object transform shunting,2023-01-17T22:33:23Z,"bug, triaged, oncall: pt2",OPEN,0,4,https://github.com/pytorch/pytorch/issues/93511,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子功能不完善,,,,,Dynamo目前不支持在前向方法中修补 nn.Module 属性
minifier test fails stderr stdout subprocess calls printed,captures output nothing ensures output printed something else fails logs response minified repro response,2022-12-17T04:55:49Z,"bug, triaged",OPEN,1,0,https://github.com/pytorch/pytorch/issues/93488,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子异常,,,,,"If minifier test fails, stderr/stdout of subprocess calls is not printed"
set grad enanbled respected running web server,set grad enanbled respected part web server flask fastapi repro filenanme go localhost even though set grad enanbled set set grad enanbled work outside context web server though shown assert statement seeing condensed block try convince set grad enanbled supposed work exactly sure certainly bite someone else butt lead mysterious cuda memory bugs think helping users either preventing happening educating users happen versions,2022-07-25T19:11:55Z,"module: dependency bug, module: autograd, triaged, actionable",OPEN,0,4,https://github.com/pytorch/pytorch/issues/82145,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,算子异常,,,,,set_grad_enabled not respected when running on a web server
dynanmo cond properly propagate guards,following fails torchhigherorderoperatorvariable function propagate guard pred constantvariable result hardcoded resulting fx graph recompilation occurs different given chen weiwen,2023-06-28T20:17:04Z,"bug, triaged, topic: not user facing, module: dynamo",CLOSED,1,0,https://github.com/pytorch/pytorch/issues/104372,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子执行不一致报错,,,,,cond does not properly propagate guards
mkldnn segmentation backward pass cpu conv layer,segmentation indefinite hanging occurs conv layer varying kernel sizes based investigation occurs backward pass related mkldnn back optimizing cpu operations reproduce snippet shows behavior expected behavior expect execute without cpu environment intel core cpu ghz additionanl context problem occur done gpu turning mkldnn backend via backends mkldnn enanbled example executes without embedding example produces seg lower integer executes running snippet mkldnn verbose following produced config,2020-10-02T17:04:15Z,"high priority, module: dependency bug, module: crash, triaged, module: mkldnn",CLOSED,0,4,https://github.com/pytorch/pytorch/issues/45746,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子出错,,,,,具有 Conv1D 层的 CPU 上向后传递时的 MKLDNN 分段错误
vision maskrcnn fails torchdynanmo dynanmic shapes typebug maximum argument position must tensor valuebug input output must number spatial dimensions got input spatial dimensions output,fails actually fixed patch fix different logs gist db eba fa minified repro minifier work,2022-10-23T14:59:48Z,"bug, triaged, oncall: pt2",CLOSED,0,4,https://github.com/pytorch/pytorch/issues/93570,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,模型出错,,,,,maximum()和输入和输出必须具有相同数量的空间维度
torchdynanmo handle tree correctly,allowed functions allows tracing tree function guaranteed return tensor,2022-06-10T17:37:20Z,"bug, oncall: pt2",CLOSED,0,4,https://github.com/pytorch/pytorch/issues/93762,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,框架问题,,,,,torchdynamo 无法正确处理 tree_map
pip requirement installs incorrect cuda version,description installing pip requirements provided installs cuda installed command install cuda requirements txt files requirements txt inline example side installed command system info version cu debug cuda os microsoft windows enterprise gcc version collect cmake version collect version cuda available cuda runtime version gpu models configuration gpu geforce rtx nvidia driver version cudnn version program files nvidia gpu computing toolkit cuda bin cudnn dll versions libraries pip numpy pip cu pip cu collect,2020-03-02T14:49:18Z,"module: dependency bug, oncall: binaries, releng, triaged",OPEN,0,4,https://github.com/pytorch/pytorch/issues/34064,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架安装出错,,,,,pip安装错误的CUDA版本
bounds read copy impl due unsafe fbgemm apis,repro details repro copy impl number calls fbgemm float tofloat simd machine goes dispatches finanlly mitigation pass fbgemm functions dst enough space calling notes currently planning verify fbgemm apis different paths copy impl versions master dc bb bf edb,2022-11-05T16:31:37Z,"high priority, bug, module: crash, module: cpu, triaged, module: complex",CLOSED,0,4,https://github.com/pytorch/pytorch/issues/88543,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,算子资源调度不合理,,,,,copy_impl由于不安全的fbgemmAPI导致读入越界
onnx export fails resolve conj op tensor slice printed,exporting module onnx fails module involves tensor slice maybe also view passed export fails message stating resolve conj op available appears printing view invokes resolve conj causes onnx exports fail even cases branch extraneous far understand onnx optimiser remove kinds extraneous branches fails pruning example observed output think understand resolve conj invoked causing onnx export failure feels unintentionanl side effect potentially avoided pruning graph invoking onnx optimiser jit script exporting onnx model succeeds jit fails way versions test test,2022-03-02T02:07:48Z,"bug, module: onnx, triaged, onnx-triaged",CLOSED,0,4,https://github.com/pytorch/pytorch/issues/73619,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,框架转换出错,,,,,ONNX导出失败
trying extract concrete symbolic,logs minified repro sure still submit minified repro since example provided already pretty small minified examples seem work torchdynanmo repro aot says input graph fail tester torchdynanmo repro dynanmo fails nanme defined collect env instead,2022-12-16T22:35:19Z,"bug, triaged",CLOSED,0,4,https://github.com/pytorch/pytorch/issues/93487,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架特性执行报错,,,,,Trying to extract a concrete int out of a symbolic int -torch.compile
problems clamp clamp clamp minimum maximum,repro clamp propagate nanns maximum propagates nanns second clamp promote clamp tensor bugs relu propagates nanns correctly luckily systematically running opinfo tests inductor,2022-07-17T01:14:40Z,bug,CLOSED,0,4,https://github.com/pytorch/pytorch/issues/93784,"backward, gradient, sparse, version, matrix, focus, run_focus, run, pas, one",15,算子出错,,,,,Problems with clamp/clamp_min/clamp_max/minimum/maximum
performance issues initial multivariatenormal log pdf,initialising multivariatenormal model scale tril input log prob extremely slow running repro note must fresh instance following results note log prob around slower successive calls details summary repro summary details model initialised covariance matrix input difference successive times pronounced difference supplying scale tril covariance matrix latter cholesky performed somehow performing cholesky cuda object seems reduce calculation log pdf particular initialising models quite frequently making calls log prob discarding model long punishing environment details summary environment information summary details,2019-08-08T08:39:51Z,"module: dependency bug, module: performance, module: distributions, triaged",CLOSED,0,4,https://github.com/pytorch/pytorch/issues/24008,"behavior, result, reproduce, expected, cpu, numpy, expected_behavior, index, different, detail",13,算子性能异常,,,,,首次调用 MultivariateNormal.log_pdf() 时出现性能问题
graph breaks huggingface stable diffusion,tracking graph breaks huggingface stable diffusion going outputs explain api trying understand different graph breaks avoid number graphs match number graph break reasons example script shows graphs vs reasons function userdefinedobjectvariable source defined class wrapping tensor attribute supported dynanmo data dependent operator scalar dense default recommended work around symbolic number help supporting inside dynanmo tensor numpy numpy logs minified repro,2022-12-05T19:30:10Z,"bug, triaged",OPEN,0,4,https://github.com/pytorch/pytorch/issues/93465,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子出错,,,,,HuggingFace Stable Diffusion 中图形中断
inductor cpu crash failure torchbench model hf bigbird,failure found latest torchinductor cpu performance dashboard issues refresh test log sw information sw commit master commit commit db fc aaa bc commit cbf torchbench benchmark commit bff torchaudio audio commit debd deae audio commit bbf ce decd torchtext ebcfed text commit ebcfed fc text commit vision commit acba fcf eafc vision commit cbeeb fbba logs minified repro,2022-12-01T03:46:21Z,"bug, triaged",CLOSED,0,4,https://github.com/pytorch/pytorch/issues/93460,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,模型出错,,,,,torchbench 模型 hf_BigBird 崩溃
segfault together import cv,reproduce lower repro replace py logs one,2020-08-29T09:06:58Z,"high priority, module: dependency bug, module: crash, module: build, triaged",CLOSED,0,4,https://github.com/pytorch/pytorch/issues/43834,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,框架异常,,,,,import cv2出现段错误
wav vec model trying inference,compile second downloaded model facebook huggingface huggingface co facebook wav vec lv took example usage provide added compile also another custom audio sample earlier strangely ran slowly compared originanl model prediction result logs warnings traceback minified repro try minifier huggingface example add compile assume reproduce problem versions working wsl ubuntu lts dev cpu torchaudio dev cpu dev cpu,2022-12-05T14:18:16Z,"bug, triaged",OPEN,0,4,https://github.com/pytorch/pytorch/issues/93464,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,模型出错,,,,,推理时出错
tensor gather bound indices gpu,gather scatter kernels missing asserts,2017-04-26T19:56:23Z,"high priority, module: dependency bug",CLOSED,0,4,https://github.com/pytorch/pytorch/issues/1368,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,算子功能不完善,,,,,Tensor.gather() doesn't check for out-of-bound indices on GPU
segmentation core dumped import,concise description reproduce steps reproduce behavior import segmentation core dumped import cv import traceback dji packages init py import importbug dji packages libtorch cpu undefined symbol sgesdd gdb import starting program bin import thread debugging libthread db enanbled host libthread db library aarch linux gnu libthread db sample messages traces provide well concise description expected happen environment version os linux ubuntu installed pip source nvidia whl version cuda cudnn version information nvidia tx additionanl context add context problem,2021-01-26T05:56:17Z,"high priority, module: dependency bug, module: crash, module: build, triaged, module: arm, module: jetson",CLOSED,0,4,https://github.com/pytorch/pytorch/issues/51092,"reproduce, behavior, expected, concise, description, context, environment, concise_description, message, sample",14,框架异常,,,,,导入torch时段错误
googlefnet failed amp,command fails logs dynanmo succeed dynanmo optimize eager succeed aot succeed dynanmo optimize aot eager succeed inductor succeed dynanmo optimize inductor succeed minified repro,2022-10-19T09:47:16Z,"bug, triaged, oncall: pt2",OPEN,0,3,https://github.com/pytorch/pytorch/issues/93549,"target, test, py, linux, cuda_target, linux_libcudnn, ci, jenkins, build, rocm",3,模型出错,,,,,GoogleFnet failed to load with amp
ideep compile ideep,got expected vecwise binanry op recently likely commit causing commit cdf version ideep commit miss lambda auto images githubusercontent cea ebf intel ideep dfeddd dc ebf ceb edfd include ideep fast math hpp images githubusercontent fd ef ce environment version master os linux centos ubuntu installed pip source source command compiling source cmake ninja gcc version cuda cudnn version,2019-06-17T01:48:30Z,"module: dependency bug, triaged, module: mkldnn",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/21850,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,框架异常,,,,,编译错误
remove redundant memory copy hf multi attention submodule cpu mkl prepack,redundant memory copy hf multi attention submodule cpu logs output minified repro,2022-12-19T09:29:44Z,"bug, triaged, oncall: pt2, module: cpu inductor",OPEN,0,3,https://github.com/pytorch/pytorch/issues/93489,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子资源调度不合理,,,,,a redundant memory copy for HF multi-attention submodule for CPU path
minifier transforms accuracy failure runtime failure pegasusforcausallm,checkout ab bff df minifier gist ccca bf ed ab hack minifier test fails accuracy gist caf dff however finished minifying got repro issues longer fails accuracy hits assert generating asserts useful prefer blocked fixing assert failure minify accuracy failure logs response minified repro response,2022-10-30T01:25:13Z,"bug, triaged",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/93594,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架出错,,,,,Minifier 将精度失败转化为运行时失败
missing filelock requirement compile cpu,follow getting started org started getting started official try compile function cpu inputs test cuda happens dev cpu version pip install numpy torchaudio force reinstall download org whl cpu logs import complaining import filelock lost originanl minified repro response,2023-01-01T11:20:33Z,"bug, oncall: pt2",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/93499,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架异常,,,,,无法导入filelock
assertionbug tracing,seeing assertion failure without good message attached works remove model compile model version dev cu logs work torchdynanmo examples venv packages dynanmo eval frame py userwarning tensorfloat tensor cores float matrix multiplication available enanbled consider setting set float matmul precision high warnings warn dynanmo eval frame debug skipping init contextlib py dynanmo eval frame debug skipping enter contextlib py dynanmo eval frame debug skipping init contextlib py dynanmo eval frame debug skipping enter contextlib py dynanmo eval frame debug skipping enanble dynanmic work torchdynanmo examples venv packages dynanmo eval frame py dynanmo symbolic convert info torchdynanmo start tracing forward dynanmo symbolic convert debug starts work torchdynanmo examples compile py dynanmo symbolic convert debug fast dynanmo symbolic convert debug mask initialized nnmodulevariable dynanmo symbolic convert debug pop jump constantvariable dynanmo symbolic convert debug starts work torchdynanmo examples compile py dynanmo symbolic convert debug fast dynanmo symbolic convert debug causal attn mask nnmodulevariable dynanmo symbolic convert debug function usermethodvariable function torchcausalattention causal attn mask ff nnmodulevariable dynanmo symbolic convert debug inlining object causal attn mask ff ab work torchdynanmo examples compile py global isinstance fast mask global tensor function pop jump,2022-12-21T22:07:52Z,"bug, oncall: pt2",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/93494,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,模型异常,,,,,模型编译时出现断言错误
compilation terminanted gentoo security reason,questions help pyyaml considered unsafe gentoo avoid gentoo develop team disccusion bugs gentoo org note tracker help form closed set listed resources available website org resources primary means support discussion forum discussion forum discuss org,2019-07-07T03:30:13Z,"module: dependency bug, module: build",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/22567,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,框架特殊环境下出错,,,,,Pytorch 编译在 Gentoo 上终止
runtimebug cudnn cudnn status execution failed,cudnn cuda running ti nvidia drivers cudnn cuda running ti nvidia drivers reproduce script resnet lstm py script test py versions collecting environment information version debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version collect libc version glibc version default mar gcc bit runtime platform linux microsoft standard debian buster sid cuda available cuda runtime version gpu models configuration gpu nvidia geforce ti nvidia driver version cudnn version collect hip runtime version miopen runtime version versions libraries pip numpy pip pip torchaudio pip blas mkl cudatoolkit bc ffmpeg hf mkl mkl service py mkl fft py hd mkl random py numpy pypi pypi py cuda cudnn mutex cuda torchaudio py cu py cu,2022-04-08T07:26:46Z,"module: dependency bug, module: cudnn, module: cuda, module: convolution, triaged",OPEN,0,3,https://github.com/pytorch/pytorch/issues/75500,"version, mkl, py, cuda, pip, pypi, numpy, gpu, pypi_pypi, cudnn",7,框架出错,,,,,cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
moco fails without suppress bugs runtimebug tensors must cuda dense,patch turn suppression benchmarks dynanmo torchbench py accuracy backend eager training moco fails logs gist minified repro minifier work captured dynanmo graph operations process groups initialized,2022-10-21T15:16:22Z,"bug, triaged, oncall: pt2",OPEN,0,3,https://github.com/pytorch/pytorch/issues/93566,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架异常,,,,,moco fails without suppress errors: RuntimeError: Tensors must be CUDA and dense 
nn adaptivemaxpool returns identical results within batch,nn adaptivemaxpool output global pooling layernorm returns identical results within batch follows approximately cases elements data identical think incorrect example may act versions version cu debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version collect libc version glibc version default mar gcc bit runtime platform linux generic debian buster sid cuda available cuda runtime version cuda module loading set lazy gpu models configuration gpu nvidia rtx gpu nvidia rtx gpu nvidia geforce rtx gpu nvidia rtx gpu nvidia rtx nvidia driver version cudnn version probably one following linux gnu libcudnn linux gnu libcudnn adv infer linux gnu libcudnn adv train linux gnu libcudnn cnn infer linux gnu libcudnn cnn train linux gnu libcudnn ops infer linux gnu libcudnn ops train hip runtime version miopen runtime version xnnpack available cpu architecture cpu op bit bit byte little endian cpu cpu thread core core socket socket numa vendor authenticamd cpu family model model nanme amd epyc core processor stepping cpu mhz cpu mhz cpu mhz bogomips virtualization amd cache cache cache,2023-08-24T03:25:25Z,"high priority, triage review, bug, module: nn, module: cuda, triaged",OPEN,0,3,https://github.com/pytorch/pytorch/issues/107842,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子出错,,,,,nn.AdaptiveMaxPool2d 在批次内返回相同的结果
cudnn implementation ctcloss handle gradients subsequent operations,cudnn implementation ctcloss handle gradients subsequent operations reproduce expected behavior expect gradients zeroed multiply loss cudnn implementation ctcloss following output gradients affected mul cudnn implementation environment version debug cuda os ubuntu lts gcc version ubuntu ubuntu cmake version version version cuda available cuda runtime version collect gpu models configuration gpu geforce nvidia driver version cudnn version probably one following linux gnu libcudnn cuda targets linux libcudnn versions libraries pip numpy pip pip torchgeometry rc pip blas mkl mkl mkl fft py hd dba mkl random py hd dba py cuda cudnn py cu,2019-09-08T13:19:17Z,"module: dependency bug, module: cudnn, triaged",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/25833,"version, mkl, py, cuda, pip, pypi, numpy, gpu, pypi_pypi, cudnn",7,框架出错,,,,,CTCLoss的 CUDNN 不处理后续操作的梯度
nn embedding cuda many resources requested launch,looks nn embedding backwards work batch think switches sorting algorithms something runs registers repro running,2016-12-29T19:33:25Z,module: dependency bug,CLOSED,0,3,https://github.com/pytorch/pytorch/issues/371,"nn, output, forward, input, layer, weight, float, embedding, padding, batch",9,算子异常,,,,,"nn.Embedding CUDA ""too many resources requested for launch"""
torchinductor tests attempt access internet,tests somewhere without outbound internet connection logs response minified repro response,2022-11-24T16:36:32Z,"bug, triaged",OPEN,0,3,https://github.com/pytorch/pytorch/issues/93450,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架网络连接问题,,,,,torchinductor tests attempt to access internet
fails compile clang xcode,stackoverflow questions defined move constructor disables implicit copy constructor brief log gist anonymous,2017-01-06T16:31:07Z,module: dependency bug,CLOSED,0,3,https://github.com/pytorch/pytorch/issues/416,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架编译出错,,,,,pytorch无法在clang6.0上编译
rid zombie processes multiprocessing pool,multiprocessing pool speed nn inference import multiprocessing context forkserver def parallel predict predict func sequences predicted cluster ids pool pool workers maxtasksperchild pool imap func functools partial predict func iterable sequences chunksize item tqdm total len sequences ncols predicted cluster ids append item pool close pool terminante pool join return predicted cluster ids note imap able show progress bar tqdm note forkserver spawn luck cannot methods interact poorly cuda note maxtasksperchild chunksize sequence sequences spawns new process note adding removing pool terminante pool join makes difference note predict func method class created also pass whole model parallel predict change anything everything works except fact memory cpu gpu everything works expected htop monitor memory usage notice every process spawn pool zombie uses memory cleared keep space still parallel predict return correct result goes script structured way validation multiple times next parallel predict called zombies add htop screenshot images githubusercontent cfe cb,2019-09-17T11:55:41Z,"module: dependency bug, oncall: distributed, module: multiprocessing, triaged",OPEN,0,3,https://github.com/pytorch/pytorch/issues/26344,"new, thread, org, version, tutorial, assert, cpu, master, doc, internanl",6,框架资源调度不合理,,,,,zombie processes
rethink exception rethrowing strategy,things longer sense new regime think changing remarked suppressing bugs default log stderr makes sense essentially suppress log stderr otherwise rethrowing exceptions torchdynanmointernanlbug similar classes annoying means default uninformative bottom scroll find useful consider rethrowing rethrowing also benchmark fails report accurate exception info useful know contexts minification occur exception thrown context add extra context saying hey minifier minified repro tension bullet logs minified repro,2022-10-23T15:07:42Z,"bug, triaged, oncall: pt2",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/93571,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,框架功能不完善,,,,,异常重新抛出策略
inductor cpu torchbench model hf reformer performance regression ww,compare torchinductor cpu performance dashboard issues ww performance regression torchbench model hf reformer ww bellow ww ww batch speedup inductor eager batch speedup inductor eager speedup ratio eager ratio inductor ratio ww sw info sw commit master commit dbc commit dbc dca edcf commit fa dc torchbench ef benchmark commit ef ca ce eaac torchaudio ecc audio commit ecc ca dd audio commit ace fa torchtext text commit efd bc cfde fc text commit fc eef ae efe vision commit efed fcbbaa vision commit afb adc torchdata data commit cd af fea fa cecd data commit afa bef dynanmo benchmarks fac commit fac da ab ww sw info sw commit master commit commit,2023-01-20T05:57:40Z,"bug, triaged, oncall: pt2, module: cpu inductor",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/93513,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,模型性能下降,,,,,[CPU] Torchbench model hf_Reformer performance regression > 10% on ww03.4
segfault shutdown,seeing sporadic crash one unit tests due pythreadstate returning null gil held reproduce steps reproduce behavior install pytest checkout froody fairscale tree crash execute pytest test autograd function pipelinestyle multiprocess break done wait crash example backtrace expected behavior test passes proposed fix replace pybind pybind ffd fea include pybind pybind environment version debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version version version bit runtime cuda available cuda runtime version gpu models configuration gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla nvidia driver version cudnn version collect hip runtime version miopen runtime version versions libraries pip numpy pip pip pg pip torchaudio pip blas mkl cudatoolkit bb mkl mkl service py mkl fft py mkl random py fdb numpy py aff numpy base py hfa py cuda cudnn pg pypi pypi torchaudio py py cu additionanl context debugging gdb,2020-10-31T21:50:33Z,"module: dependency bug, module: crash, triaged, module: pybind, module: third_party",OPEN,0,3,https://github.com/pytorch/pytorch/issues/47161,"version, mkl, py, cuda, pip, pypi, numpy, gpu, pypi_pypi, cudnn",7,框架出错,,,,,torch1.7 关闭期间出现段错误
torchdynanmo needs graph break resize calls aotautograd based backends,repro fails even partitioner handled correctly possible mutate metadata input arguments aotautograd invocation since aotautograd creates copies tensors inputs force resize unconditionanlly graph break seems bad cases aotautograd involved,2022-07-11T14:12:36Z,"bug, triaged, oncall: pt2, module: aotdispatch",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/93780,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,框架异常,,,,,torchdynamo needs to graph break for resize_ calls to AOTAutograd based backends
conv output gpu kernel many zeros,kernel many zeros masked conv conv output gpu reproduce test function convolve cpu output expected gpu output varies depending channel count sample messages traces provide well environment version debug cuda os ubuntu lts gcc version ubuntu ubuntu cmake version collect version cuda available cuda runtime version collect gpu models configuration gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla nvidia driver version cudnn version collect versions libraries pip numpy pip pip blas mkl mkl mkl service py mkl fft py mkl random py hd py cuda cudnn py cu,2020-03-30T04:23:05Z,"module: dependency bug, module: numerical-stability, module: cuda, module: convolution, triaged",OPEN,0,3,https://github.com/pytorch/pytorch/issues/35655,"version, mkl, py, cuda, pip, pypi, numpy, gpu, pypi_pypi, cudnn",7,算子计算错误,,,,,当卷积核有许多零时，GPU 上的 conv2d 输出错误
import failed,installed pip alpine container gave following importbug loading shared library ld linux directory needed packages cpython linux gnu ipython importbug relocating packages libcudnn strtod internanl symbol found,2017-05-23T04:47:53Z,module: dependency bug,CLOSED,0,3,https://github.com/pytorch/pytorch/issues/1618,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,框架导入失败,,,,,
problems training models torchdynanmo,try torchdynanmo help train model faster encounter issues torchdynanmo pip install version model nvidia problem previous observe beginning costs lot vs middle catch acceleration effects couple epochs torchdynanmo leads longer training results almost compared normally know right miss something bigger enlarge area torchdynanmo optimize nvfuser covers facing bugs training process got stuck info lot got stuck think problem maybe raised operations update stuck spends lot evaluate begin actual training waited half hour proceeded environment cu torchdynanmo torchtext cu tqdm typing extensions,2022-08-02T13:23:45Z,"bug, oncall: pt2",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/93796,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,模型训练出错,,,,,使用 torchdynamo 训练模型时遇到问题
encode numpy dependency,reproduce steps reproduce behavior install snippet expected behavior concise description expected happen logs environment version os linux mac osx installed pip source pip command compiling source version cuda cudnn version gpu models configuration information beginner tutorial saving loading models org tutorials beginner saving loading models additionanl context met installed numpy ran without bugs,2020-01-16T15:16:39Z,"high priority, module: dependency bug, oncall: binaries, releng, triaged, module: numpy, quansight-nack",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/32291,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,框架导入问题,,,,,PyTorch-1.4.0 doesn't encode numpy dependency
add support cuda available cuda current device,chen weiwen,2022-06-23T19:40:10Z,"bug, triaged, oncall: pt2, module: dynamo",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/93769,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,框架功能不完善,,,,,添加对torch.cuda.is_available()和 的支持torch.cuda.current_device()
onnx cse pass export pollutes information,description issues found recently adding cse pass onnx export cse pass update information common subexpressions different modules eliminanted updating information left result appears users modules accessing internanl information module belongs onnx passes creating duplicated constants cse pass issues illustrated example observe onnx graph constant onnx constant still appeared times layers constant output layers constant output layers constant output information incorrect constants considered belonging layers considered belonging layers layers layers respectively breaks nanming function export depends information figure nodes originanl module comparison graph cse disabled export proposal cse update common ancestor whenever finds nodes common sub expression example scopes nodes hence common ancestor ideally moved upward graph reflect structural changes well alternantive solution onnx exporter additionanl pass following cse update information based scopes versions master git,2022-10-10T19:54:52Z,"bug, module: onnx, triaged, module: regression",OPEN,0,3,https://github.com/pytorch/pytorch/issues/86627,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,框架异常,,,,,[ONNX] CSE pass in export pollutes Scope information
look ci passes slow tests disabled tests around env variables,sev issues windows ci disabled tests grows big breaks limit windows env variable length chars moment ci maintains following lists different env variables slow tests slow tests flaky disabled tests disabled tests revisit way env variables better safer way passing information around limits keep mind windows vs runner upper limit chars env variable length learn sev number ubuntu bionic deducted xargs show limits number mac os deducted getconf always limit dev infra versions versions,2022-08-15T19:19:31Z,"bug, module: ci, triaged, enhancement",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/83458,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,框架功能不完善,,,,,Look into how the CI passes the list of slow tests and disabled tests around using env variables
cudnn convolution add relu fails basic conditions,cudnn convolution add relu seems basic inputs reproduce results output expected behavior result output conv add relu environment additionanl context cudnn version,2021-08-06T21:37:36Z,"module: dependency bug, module: cudnn, triaged",OPEN,0,3,https://github.com/pytorch/pytorch/issues/62926,"behavior, result, reproduce, expected, cpu, numpy, expected_behavior, index, different, detail",13,算子执行失败,,,,,cudnn_volving_add_relu 在基本条件下失败 
inductor valuebug cannot bitcast data data,happens tensor works well change logs minified repro response,2022-11-02T21:32:55Z,"bug, triaged",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/93606,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,算子异常,,,,,无法将大小为 64 的数据类型位转换为大小为 32 的数据类型
illegal memory access nonzero method tensor,known nonzero method fails input tensor exceeds raising runtimebug however nonzero method also fails tensors slightly smaller causing irrecoverable illegal memory access reproduce decreasing one eliminantes expected behavior method either returns indices non elements raises runtimebug caught recovered additionanl context possibly related versions version cu debug cuda rocm os red hat enterprise linux server release maipo gcc version gcc clang version collect cmake version collect libc version glibc version platform linux el redhat maipo cuda available cuda runtime version gpu models configuration gpu tesla gpu tesla gpu tesla gpu tesla nvidia driver version cudnn version collect hip runtime version miopen runtime version xnnpack available versions libraries pip functorch pip numpy pip cu pip torchtext pip cu collect,2022-06-27T11:54:09Z,"module: dependency bug, module: crash, triaged, module: edge cases",OPEN,0,3,https://github.com/pytorch/pytorch/issues/80337,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子资源调度不合理,,,,,张量太大时进行非法内存访问
torchinductor cuda memory memory corruption debugging master,increase memory corruption bugs surfaced inductor tests mostly op tests ex torchdynanmo among current enanbled ops detected memory leaking compute sanitizer may leaks ops enanbled op info test torchdynanmo rmatmul float rmatmul float rmatmul float masked cumprod float masked cumprod float masked cumprod masked cumprod masked cumsum float masked cumsum float masked cumsum float masked cumsum masked cumsum masked logsumexp float masked logsumexp float masked logsumexp masked logsumexp masked float masked float allclose float allclose float allclose float argwhere argwhere float argwhere float argwhere float bernoulli float bernoulli float bincount bincount combinantions combinantions float combinantions float combinantions float combinantions combinantions copysign float copysign float copysign float corrcoef float corrcoef float cov float cov float cov float cov float float equal erf float erf float fmax float fmax float fmin float fmin float fmin float jiterator inputs outputs jiterator inputs extra float jiterator inputs extra float jiterator inputs extra float jiterator binanry float jiterator binanry jiterator binanry jiterator binanry return ref jiterator binanry return ref jiterator unanry float jiterator unanry float linanlg cholesky float linanlg inv float linanlg inv float linanlg float linanlg float linanlg qr float linanlg qr float logsumexp float nn functionanl scaled dot product attention float nn functionanl scaled dot product attention float nn functionanl scaled dot product attention float nn functionanl feature alpha dropout without train float nn,2022-09-27T08:12:47Z,"bug, module: cuda, triaged, oncall: pt2",OPEN,0,3,https://github.com/pytorch/pytorch/issues/93693,"nn, output, forward, input, layer, weight, float, embedding, padding, batch",9,框架资源调度不合理,,,,,CUDA内存泄漏/内存损坏
assertionbug dynanmo input output strict subset traced input output,trying torchdynanmo export huggingface transformers bertmodel let know api correctly version built master commit commit de logs minified repro pasting raw repro since torchdynanmo export edit versions transformers,2022-11-15T20:01:45Z,"bug, oncall: pt2",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/93422,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子出错,,,,,AssertionError: Dynamo input and output is a strict subset of traced input/output
cudnn batch kernel batchnorm bwtr nhwc semipersist gets blocked overlapping nccl reduce calls,cudnn channels format enanbled ddp backward pass cudnn batchnorm backward stalled overlapped nccl reduce sorry snippet repro screenshots profiler blocking batch backward images githubusercontent ee bbcd af neither nantive implementation nchw version cudnn kernel bn bw singleread specialized shows behavior technically seems cudnn know cudnn filing instead also reported nvidia nccl issues nvidia nccl issues versions,2022-06-25T00:19:56Z,"module: dependency bug, module: cudnn, triaged, module: nccl, module: memory format",OPEN,0,3,https://github.com/pytorch/pytorch/issues/80259,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子异常,,,,,Cudnn batch norm kernel被阻塞
slow training efficientnet updated,updated noticed slow training speed slower train efficientnet timm network training speed unanffected gpu cuda supported perhaps updating cuda quite weird anyone experienced versions downgraded speed back,2022-06-27T01:01:15Z,"module: dependency bug, module: performance, module: cudnn, module: cuda, triaged",CLOSED,0,3,https://github.com/pytorch/pytorch/issues/80310,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,框架性能异常,,,,,pytorch从1.8更新到1.10时，Efficientnet的训练速度缓慢
promotion accumulate operation differs eager cpp dynanmo,discovered looking cputests test defined result reduction op float tensor stored float scalar results slight accuracy discrepancy eager dynanmo generated db cdc ff edf test inductor test torchinductor py logs response minified repro response versions ci,2023-02-03T00:27:33Z,"bug, module: cpu, triaged, oncall: pt2",OPEN,0,2,https://github.com/pytorch/pytorch/issues/94017,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,算子精度差异,,,,,eager 和 dynamo 之间存在轻微的精度差异
tacotron one variables needed gradient modified inplace operation,even eager backend fails benchmarks dynanmo torchbench py training performance backend eager float tacotron logs cuda train tacotron common failed dynanmo one variables needed gradient modified inplace operation cuda floattensor output asstridedbackward version expected version instead hint enanble anomaly detection find operation failed compute gradient autograd set detect anomaly traceback scratch anijain work benchmarks dynanmo common py warmup fn model example inputs scratch anijain work dynanmo eval frame py fn return fn kwargs scratch anijain work benchmarks dynanmo torchbench py forward backward pass cloned inputs clone inputs inputs scratch anijain work benchmarks dynanmo torchbench py graph break forward backward pass grad scaler scale loss backward scratch anijain work tensor py backward autograd backward scratch anijain work autograd init py backward variable execution engine backward calls engine backward pass runtimebug one variables needed gradient modified inplace operation cuda floattensor output asstridedbackward version expected version instead hint enanble anomaly detection find operation failed compute gradient autograd set detect anomaly minified repro response,2022-11-22T18:29:20Z,"bug, triaged, oncall: pt2",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/93440,"backward, gradient, sparse, version, matrix, focus, run_focus, run, pas, one",15,框架异常,,,,,pytorch梯度计算出错
accuracy allenanilongformerbase minifier help minifies,benchmarks dynanmo huggingface py accuracy device cuda backend inductor training float allenanilongformerbase logs response minified repro response,2022-11-22T23:13:33Z,bug,CLOSED,0,2,https://github.com/pytorch/pytorch/issues/93441,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,测试用例执行不通过,,,,,
minifier work debertaforquestionannswering,ab bff df apply patch deberta failures accuracy aot eager minifier anything logs response minified repro response,2022-10-30T02:31:47Z,"bug, triaged",OPEN,0,2,https://github.com/pytorch/pytorch/issues/93596,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子执行失败,,,,,Minifier doesn't work on DebertaForQuestionAnswering 
store example fx meta,storing meta implies pass backend compiler actually strip example values fx graph done really internanl concept dynanmo propose store tensorvariable instead btw actually pass metadata backend presence metadata mutation misleading expect example logs response minified repro response,2022-11-04T15:41:12Z,"bug, triaged",OPEN,0,2,https://github.com/pytorch/pytorch/issues/93609,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,算子功能不完善,,,,,不要将 example_value 存储在 FX 节点元上 
torchdynanmo onnxrt backend generating fake tensor bugs,hi trying torchdynanmo onnxrt onnxrt cuda onnxrt cpu backends following resnet example described org tutorials intermediate dynanmo tutorial hitting dynanmo exc backendcompilerfailed onnxrt cuda raised exception convert tensors faketensors instantiate faketensormode allow non fake inputs found convolution default faketensor faketensor device meta cuda parameter containing tensor device cuda requires grad many examples torchbench hitting happen backends aot cudagraphs also force set faketensormode fake tensor py hit another missing something obvious version gitdf onnxrt version thinking something obvious missing dig detail logs dynanmo exc backendcompilerfailed onnxrt cuda raised exception convert tensors faketensors instantiate faketensormode allow non fake inputs found convolution default faketensor faketensor device meta cuda parameter containing tensor device cuda requires grad minified repro import import dynanmo dynanmo returns result running fn took fn seconds cuda events synchronization accurate measurements def timed fn start cuda enanble timing cuda enanble timing start record result fn record cuda synchronize return result start elapsed generates random input targets data model batch def generate data return randn float cuda randint cuda models import,2023-01-10T18:52:14Z,"bug, triaged, oncall: pt2",OPEN,0,2,https://github.com/pytorch/pytorch/issues/93502,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架后端出错,,,,,Torchdynamo 与 onnxrt 后端生成假张量错误
torchdynanmo export work tensor numel,logs dynanmo exc unsupported generic jump dynanmicshapevariable minified repro response,2022-10-24T18:28:18Z,"bug, oncall: pt2",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/93577,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子功能不完善,,,,,torchdynamo.export 不适用于tensor.numel
inductor cpu torchbench model soft actor critic performance regression ww,compare torchinductor cpu performance dashboard issues ww performance regression torchbench model soft actor critic ww bellow ww ww batch speedup inductor eager batch speedup inductor eager speedup ratio eager ratio inductor ratio ww sw info sw commit master commit fac commit fac da ab commit fab ff cdc ee torchbench benchmark commit db torchaudio ecc audio commit ecc ca dd audio commit ace fa torchtext text commit efd bc cfde fc text commit fc eef ae efe vision commit efed fcbbaa vision commit afb adc torchdata data commit cd af fea fa cecd data commit afa bef ww sw info sw commit master commit fac commit fac da ab commit fab ff cdc ee,2023-01-12T08:58:11Z,"bug, oncall: pt2, module: cpu inductor",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/93505,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,模型性能下降,,,,,[CPU] Torchbench model soft_actor_critic performance regression > 10% on ww02.3
dynanmo cpp backend works,signanl actions runs jobs actions runs jobs hud org commit bc bec aedff logs response minified repro response,2022-11-22T18:19:22Z,"bug, triaged, oncall: pt2",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/93439,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架后端问题,,,,,Dynamo cpp 后端仅适用于 g++12
addition hybrid csr tensors produces incorrect invalid csr tensor,description example expected result system info master,2023-02-06T15:19:42Z,"bug, module: sparse, triaged, module: correctness (silent)",OPEN,0,2,https://github.com/pytorch/pytorch/issues/94186,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,算子计算错误,,,,,添加混合 CSR 张量会产生不正确且无效的 CSR 张量
diffuser pipeline device attribute broken optimized model,hi adding tracking purposes solution potentially provided pr compiled model ddpmpipeline huggingface diffusers diffusers pipelines ddpm pipeline ddpm py device property correct result always return cpu huggingface diffusers ce bff diffusers pipeline py optimized model class inherent nn module simple repro pr solve makes compiled model nn module logs minified repro minifier work sure cases,2022-11-02T00:28:57Z,"bug, triaged",OPEN,0,2,https://github.com/pytorch/pytorch/issues/93602,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,模型异常,,,,,Diffuser pipeline device attribute broken when using optimized model
zeros csc layout throws internanl assert exception,description example expecting csc tensor metadata nnz version current master,2022-09-14T10:31:36Z,"bug, module: sparse",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/84998,"new, thread, org, version, tutorial, assert, cpu, master, doc, internanl",6,算子异常,,,,,具有 CSC 布局的 torch.zeros 会引发 INTERNAL ASSERT 异常
zeros csr layout produces tensor invalid crow indices,description example expected crow indices version current master,2022-09-14T10:27:21Z,"bug, module: sparse",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/84997,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,算子异常,,,,,具有 CSR 布局的 torch.zeros 生成具有无效 crow_indices 的张量
inductor cpu torchbench model vision maskrcnn performance regression ww,compare ww performance regression torchbench model vision maskrcnn torchinductor cpu performance dashboard issues ww bellow ww ww batch speedup inductor eager batch speedup inductor eager speedup ratio eager ratio inductor ratio ww sw info sw commit master commit commit da cb ab ca commit cdf efdee torchbench bbdc benchmark commit bbdc bf bda torchaudio ecc audio commit ecc ca dd audio commit ace fa torchtext text commit efd bc cfde fc text commit fc eef ae efe vision commit efed fcbbaa vision commit afb adc torchdata data commit cd af fea fa cecd data commit afa bef dynanmo benchmarks fac commit fac da ab ww sw info sw commit master commit fac commit fac da,2023-01-13T09:23:15Z,"bug, oncall: pt2, module: cpu inductor",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/93507,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,模型性能下降,,,,,[CPU] Torchbench 模型 Vision_maskrcnn 在 ww02.4 上性能回归 > 10%
fix dynanmo related debug failures,multiple test dynanmo tests asserts debug otherwise fail fixed script tests gist williamwen individual test stop running abort segfault continue skip failing test tests fix test test optim py test adagrad sparse ceval pyeval evalframedefault assertion failed reference tests also fail without test dynanmo test test tensor creation ops py test non writable buffer cpu fatal py checkslotresult slot getbuffer bytes succeeded exception set debug also fails debug test functorch test ops py test vmapvjpvjp linanlg tensorsolve cpu float assertionbug tensor likes close debug tests test indices cpu test indices cuda test test indexing py assertionbug scalars equal occurs entire test running individually passes fails locally release passes ci issues test test ops gradients py test fn grad linanlg det singular cpu float autograd gradcheck gradcheckbug jacobian mismatch output respect input debug test test sparse csr py test invalid input csr cpu assertionbug bit integer overflow nnz match cannot converted without overflow skipped ci,2023-06-01T00:06:24Z,"bug, triaged, release notes: dynamo",OPEN,0,2,https://github.com/pytorch/pytorch/issues/102673,"test, circleci, job, ci, workflow, disabled, failure, pipeline, flaky, failing",12,框架异常,,,,,多个PYTORCH_TEST_WITH_DYNAMO=1测试会在调试 Python 3.11 上引发断言
mmdet cannot inductor,hi guys going verify dynanmo mmdet branch getting bugs environmental information command follows eager eager verify works images githubusercontent efc bc bb bfcd modified following tensor tensor got following modifying images githubusercontent dfb ef modified following range tuple range got following modifying images githubusercontent fdc db eeed feda remove dtype parameters float instead mmlab mmdetection dev mmdet models modules prior generators anchor generator py mmlab mmdetection dev mmdet models dense heads base dense head py point works slow warning images githubusercontent bde bdd de ee aot eager slowly inductor images githubusercontent ee way train mmyolo inductor simple modification slow oom logs response minified repro response,2022-11-28T06:11:29Z,"bug, triaged",OPEN,0,2,https://github.com/pytorch/pytorch/issues/93454,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,框架运行失败,,,,,MMDet 3.x cannot run successfully in inductor mode
testing tensor work low high integral dtypes,fail ffb db fcbfb testing creation py,2023-03-07T09:09:06Z,"bug, triaged, module: testing",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/96179,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,算子功能不完善,,,,,torch.testing.make_tensor 不适用于整数 dtypes 的 low=high
copying tensors sometimes copy elements cuda graphs,copying tensors sometimes work cuda graphs occasionanlly elements copied repro gist tomconerlyanth fd dd ca eec versions tc collect env py collecting environment information version gitee debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version version libc version glibc version default sep gcc bit runtime platform linux amzn glibc cuda available cuda runtime version gpu models configuration gpu nvidia gpu nvidia gpu nvidia gpu nvidia gpu nvidia gpu nvidia gpu nvidia gpu nvidia nvidia driver version cudnn version probably one following linux gnu libcudnn linux gnu libcudnn adv infer linux gnu libcudnn adv train linux gnu libcudnn cnn infer linux gnu libcudnn cnn train linux gnu libcudnn ops infer linux gnu libcudnn ops train hip runtime version miopen runtime version xnnpack available versions libraries pip mypy pip mypy extensions pip numpy pip memlab pip gitee pip ops pip torchtyping blas mkl mkl mkl include mkl service py mkl fft py hd mkl random py mypy pypi pypi mypy extensions pypi pypi,2022-03-18T18:26:17Z,"high priority, module: dependency bug, triaged, module: correctness (silent), module: cuda graphs",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/74419,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架异常,,,,,使用 cuda 图时复制张量有时不会复制最后 N 个元素
function transpose convolution incorrect,function transposed convolution defined master jit functions py include output padding whereas function op defined org docs stable generated nn convtranspose nn convtranspose includes output padding looks versions observed latest torchmlir turn refers functions,2023-04-01T06:13:33Z,"bug, oncall: jit",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/98129,"nn, output, forward, input, layer, weight, float, embedding, padding, batch",9,算子输出结果错误,,,,,转置卷积的形状函数不正确
forward reverse gives gradgrad softsign,forward reverse gives gradgrad softsign obviously softsign well defined second gradient thus think fwd rev issues since rev rev pass gradgradcheck versions,2022-07-06T00:59:49Z,"bug, module: autograd, triaged",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/80926,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,算子出错,,,,,forward-over-reverse gives wrong gradgrad for Softsign
dynanmo generates invalid frame graph breaking due opacus cifar hooks,running torchbench opacus cifar today ci hooks silently ignored modules nn modules conv conv since allowed graph hooks traced dynanmo hook behavior might actually burned aot best shaky likely incorrect read carefully hooks trivial capture activations hook opacus opacus grad sample grad sample module py capture backprops hook opacus opacus grad sample grad sample module py related gh dynanmo skips hooks allowed modules pr fixes graph breaking allowed modules hooks pr desc discussion alternantive fixes graph breaking allowed modules hooks opacus fails looks invalid dynanmo frame generated checkout ghstack checkout repro compile debug benchmarks dynanmo torchbench py opacus cifar performance backend aot eager opacus log log gist wconstab db cdfff notable excerpts log graphbreak expected due change sure getattr fails unless becuase hook modify dynanmo copy module hook actually mutating module via setattr hook actually store stuff module finanlly bottom stackframe invalid versions pr chen weiwen,2023-03-27T23:43:27Z,"bug, triaged, module: dynamo, release notes: dynamo",OPEN,0,2,https://github.com/pytorch/pytorch/issues/97718,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架异常,,,,,Dynamo generates invalid frame when graph-breaking due to opacus_cifar10 hooks
compile backendcompilerfailed compile fn raised runtimebug,compile backendcompilerfailed compile fn raised runtimebug appears trying tracing tensor scalar dense default buging likely caused data dependent control flow similar sure means note uses custom lstm consists nested models multiple input parameters runtimebug traceback ananconda packages dynanmo output graph py compiler gm else compiled fn compiler fn gm fake example inputs logger logging info done compiler function nanme ananconda packages dynanmo debug py debug wrapper gm example inputs kwargs else compiled gm compiler fn gm example inputs kwargs ananconda packages init py compile fn model inputs cm return compile fn model inputs ananconda packages inductor compile fx py compile fx model example inputs inner compile force compile tiny graphs model example inputs ananconda packages dynanmo optimizations training py compiler fn gm example inputs nb cloned cg aot module simplified gm example inputs kwargs counters aot autograd ok ananconda packages functorch aot autograd py aot module simplified mod fw compiler bw compiler partition fn decompositions hasher static argnums aot config ananconda packages dynanmo py wrapper kwargs func kwargs latency ananconda packages functorch aot autograd py create aot dispatcher function flat fn flat aot config compiled,2022-12-09T12:40:07Z,"bug, triaged, oncall: pt2, module: inductor",OPEN,0,2,https://github.com/pytorch/pytorch/issues/90540,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架编译错误,,,,,torch.compile() _compile_fn 引发 RuntimeError
dynanmo failed module default forward function,module default forward function called module fail dynanmo traceback default dynanmo py module res module inp nn modules module py impl return forward kwargs dynanmo eval frame py forward return dynanmo ctx orig mod forward kwargs dynanmo eval frame py fn return fn kwargs dynanmo eval frame py catch bugs return callback frame cache dynanmo convert frame py convert frame result inner convert frame cache dynanmo convert frame py fn return fn kwargs dynanmo py wrapper func kwargs dynanmo convert frame py convert frame assert return compile dynanmo convert frame py compile transform object transform dynanmo bytecode transformation py transform object transformations instructions options dynanmo convert frame py transform tracer dynanmo symbolic convert py super dynanmo symbolic convert py dynanmo symbolic convert py getattr inst opnanme inst dynanmo symbolic convert py wrapper return inner fn inst dynanmo symbolic convert py function function fn dynanmo symbolic convert py function push fn function kwargs dynanmo,2022-12-07T06:54:10Z,"bug, triaged, oncall: pt2, module: dynamo",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/90361,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,接口执行出错,,,,,dynamo对于forward函数中具有默认参数的模块失败
add guards aliasing side effects,currently handle aliasing objects side effects facebookresearch torchdynanmo fcb torchdynanmo variables builder py guards checking aliasing relationships stay subsequent calls lead incorrect behavior example add set guards trigger recompile likely sort hash table checks quickly adding similar protection tensor aliasing likely also useful compiler backends allow generate relies input aliasing unchanged chen weiwen,2022-03-13T03:16:08Z,"bug, triaged, oncall: pt2, module: dynamo",OPEN,0,2,https://github.com/pytorch/pytorch/issues/93722,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,框架出错,,,,,需要对张量别名添加保护
inductor certain reductions triton compile,following causes triton compiler initially revealed failing test pytest generated test clovaai lffont py test issues appears particular think condition weak chen weiwen,2023-06-13T02:30:32Z,"bug, topic: not user facing, oncall: pt2, module: inductor, upstream triton",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/103481,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架编译出错,,,,,certain reductions cause triton compile error
inductor crash running model without returning results,function calls model return result causes crash happens inductor aot eager although points aotautograd logs minified repro,2022-11-24T00:43:31Z,"bug, triaged",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/93448,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,模型运行出错,,,,,"A function that calls a model but doesn't return the result causes a crash. Happens on inductor but not aot_eager, although the stack trace points to AOTAutograd."
tensor sparse handling indices incorrectly dynanmo fake tensor,sparse returning faketensor indices attribute logs response minified repro repro repro,2022-12-21T21:21:05Z,"bug, triaged, oncall: pt2, module: dynamic shapes",OPEN,0,2,https://github.com/pytorch/pytorch/issues/93493,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子执行出错,,,,,tensor.to_sparse() 在假张量下错误地处理索引
unanble nccl tests nccl,nccl gather failing nccl provided logs nccl debug info versions nccl cuda huang,2023-04-20T21:54:08Z,"bug, oncall: distributed",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/99669,"target, test, py, linux, cuda_target, linux_libcudnn, ci, jenkins, build, rocm",3,框架异常,,,,,无法使用 NCCL 2.12.12 运行 Torch 2.1.0（每晚）的 nccl 测试 
benchmark script giving meaningful traces,symbolic shapes branch gives logs dynanmo succeed dynanmo optimize eager succeed aot succeed dynanmo optimize aot eager succeed inductor succeed dynanmo optimize inductor succeed minified repro,2022-10-19T16:16:23Z,"bug, oncall: pt2",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/93550,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架异常,,,,,基准脚本再次没有给出有意义的错误跟踪
embedding op working expected,behavior renorm inside renorm weight specified however gpu input multi dimensionanl behavior expected also weight renormed less repo snippet one example output sometimes repo within runs versions latest night linux cuda,2022-07-19T19:02:22Z,"bug, module: cuda, triaged, module: norms and normalization, module: embedding",OPEN,0,2,https://github.com/pytorch/pytorch/issues/81717,"nn, output, forward, input, layer, weight, float, embedding, padding, batch",9,算子异常,,,,,使用 max_norm 的 PyTorch 嵌入操作未按预期工作
dynanmo guarding tensor locals,one example graph woof warf generates tensor match guard function generates tensor match guard cases guard completely unnecessary extra guards especially pernicious long function body graph breaks intermediate fragments guard inputs even broken subgraphs deal subset variables logs response minified repro response,2022-11-25T15:51:33Z,"bug, triaged",OPEN,0,2,https://github.com/pytorch/pytorch/issues/93452,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架异常,,,,,Dynamo 对 Tensor 局部变量的保护过度
jx nest base fails torchdynanmo dynanmic shapes typebug reshape argument must tuple symints found element float pos,fails logs gist ezyang db minified repro minifier work,2022-10-24T13:30:45Z,"bug, triaged, oncall: pt2",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/93572,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,算子执行失败,,,,,jx_nest_base参数shape必须是 SymInts 的元组，但在 pos 2 处找到了float 类型的元素
tensor nonzero tries allocate huge amount memory tensors gpu elements close,tensor nonzero fails gpu tensors larger tensors slightly smaller cuda memory exception raised huge amounts memory exabyte requested reproduce problem happen tensor slightly smaller cpu expected behavior return result result fits memory try allocate less memory environment additionanl context really seem one encounter however might hint source filed anyway free close sufficient,2021-02-08T14:17:19Z,"module: dependency bug, module: cuda, module: memory usage, triaged",OPEN,0,2,https://github.com/pytorch/pytorch/issues/51872,"behavior, result, reproduce, expected, cpu, numpy, expected_behavior, index, different, detail",13,算子资源调度不合理,,,,,Tensor.nonzero 尝试在 GPU 上为 num_elements 接近 INT_MAX 的张量分配大量内存
cublas status execution failed convtranspose backward fp inputs,helping integrate cuda amp ignite ignite asked debug cyclegan amp example saw weird nondeterministic bugs eventually nanrrowed repro convtranspose fp inputs shapes taken cyclegan fails illegal memory accesses slowconvtranspose dbackward repro involve cuda amp script think innocent know yet failure may may occur based input addresses sure distinguishes bad input addresses posting visibility similar issues keep digging related issues discusses nonzero output padding force convtranspose slow conv backend im col col im cublas gemm instead cudnn issues failing convtranspose fp inputs even shapes repro fails different symptoms fix commit ab af fce cfd one change replaced macro hardcoded character deep memory movement problems likely affected issues also cyclegan backward failure failure came cudnn resolved reproduce may work different system backward reliably fails iteration cuda launch blocking weight grad bias grad sums deterministic iterations cuda memcheck tool memcheck transpose py reports bunch invalid reads environment cublas loaded vanilla public cuda install based ish master version,2020-04-23T17:35:28Z,"high priority, module: dependency bug, triaged, module: cublas",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/37157,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子执行失败,,,,,CUBLAS_STATUS_EXECUTION_FAILED for ConvTranspose2d backward with FP16 inputs
runtimebug nccl processgroupnccl cpp unhandled system,came across runtimebug nccl processgroupnccl cpp unhandled system trying distribute neural network training gpus single according export nccl debug info reran noticed weird warnings debug output crash permission sudo chmod dev shm problem gone maybe handled better raising warning debug output users unhandled system know problem,2020-04-21T16:24:25Z,"module: dependency bug, oncall: distributed, triaged, module: nccl",OPEN,0,2,https://github.com/pytorch/pytorch/issues/37004,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,框架异常,,,,,"RuntimeError: NCCL error in ProcessGroupNCCL.cpp:290, unhandled system error"
cmake unusable archlinux,running example libtorch cmake bugs missing libc cuda find library required tries find arch package mananger installs either packager install correct place caffe targets cmake rigorous enough elf location thought proper modify caffe targets cmake find correctly circumstance provides variable something workaround reproduce environment version debug cuda os arch linux gcc version gcc cmake version version version cuda available cuda runtime version gpu models configuration gpu geforce nvidia driver version cudnn version libcudnn versions libraries pip numpy pip pip post collect additionanl context package install nanmed cuda arch,2019-05-21T12:49:29Z,"module: dependency bug, module: build, triaged",OPEN,0,2,https://github.com/pytorch/pytorch/issues/20753,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架特殊环境下出错,,,,,Torch 的 cmake 在 archlinux 中无法使用
clarify glog gflags,cmakelists txt contains dffc cmakelists txt basically says requires glog gflags however downloaded official via pip install cpu cpu download org whl stable checked glog include macros cmake macros contains undef glog cmake config string include caffe core macros confirms matches reported discussion issues issuecomment warning message cmake outdated since mistake particular ask glog recommend required tested background question compiling power seeing failures glog due floating point exceptions getting enanbled without able tell,2020-09-18T12:06:35Z,"module: dependency bug, module: build, triaged, small, module: build warnings, better-engineering",OPEN,0,2,https://github.com/pytorch/pytorch/issues/44948,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子执行失败,,,,,使用 GLog 时看到由于启用浮点异常而失败
test array adaptor test cuda array interface active device fails numba version,test array adaptor test cuda array interface active device fails numba version reproduce steps reproduce behavior test numba integration numba expected behavior tests question pass environment additionanl context test passes numba version also looks added couple days ago ananconda org numba numba,2019-06-03T02:36:12Z,"module: dependency bug, triaged, module: numba",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/21269,"behavior, result, reproduce, expected, cpu, numpy, expected_behavior, index, different, detail",13,算子执行失败,,,,,test_array_adaptor 和 test_from_cuda_array_interface_active_device 因 numba 版本 0.44.0 失败
solve gpu fails batch,solve gpu tensor fails batch tensors throws runtimebug cuda invalid configuration argument interestingly solve multiple times occasionanlly pass results reproducing reproduce steps reproduce behavior output execute multiple times environment additionanl notes snippet around runtime becomes similar runs cpu suspect relates,2019-06-11T18:10:05Z,"module: dependency bug, triaged",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/21643,"backward, gradient, sparse, version, matrix, focus, run_focus, run, pas, one",15,算子执行失败,,,,,当batch > 65535 时，GPU 中的 torch.solve 失败
rocm gemmnotransnotrans timeout,,2019-03-26T14:22:29Z,"high priority, module: dependency bug, module: rocm, triaged, module: deadlock",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/18477,"test, circleci, job, ci, workflow, disabled, failure, pipeline, flaky, failing",12,框架异常,,,,,ROCm GemmNoTransNoTrans 超时
undeclared dependency futurize caffe,seems undeclared dependency future org caffe library causes import bugs reproduce ubuntu install following packages venv create virtual environment running venv venv venv bin pip install tensorboard running venv bin pip install future fixes problem expected behavior future installed default environment seeing nantive linux installs wsl wsl info additionanl context issues,2019-08-25T19:03:31Z,"module: dependency bug, triaged, module: tensorboard",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/25163,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,框架依赖缺失,,,,,caffe2 中未声明对 python-futurize 的依赖
apex internanl assert failed,training fairseq internanl assert asks reproduce fairseq script expected behavior model train occur environment collecting environment information version bffc debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version version version bit runtime cuda available cuda runtime version collect gpu models configuration gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla gpu tesla nvidia driver version cudnn version collect hip runtime version miopen runtime version versions libraries pip msgpack numpy pip numpy pip lightning pip pip torchaudio dff blas mkl faiss cpu py df cpu libfaiss bc cpu magma cuda mkl mkl include mkl service py mkl fft py mkl random py msgpack numpy pypi pypi numpy py aff numpy base py hfa lightning pypi pypi pypi pypi torchaudio dff pypi pypi,2021-01-26T04:19:58Z,"module: dependency bug, triaged, module: assert failure, module: regression",OPEN,0,2,https://github.com/pytorch/pytorch/issues/51083,"version, mkl, py, cuda, pip, pypi, numpy, gpu, pypi_pypi, cudnn",7,框架异常,,,,,apex 内部断言失败
conv transpose returns different result input kernel mkldnn tensors,implemented upsampler conv transpose works correctly cpu gpu backend tested mkldnn output incorrect example upsampling tensor returns tensor input kernel converted mkldnn output tensor reproduce sample output expected behavior output stays mkldnn tensors environment copy paste output environment collection script raw githubusercontent master collect env py checklist manually collecting environment information version debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version version version bit runtime cuda available cuda runtime version collect gpu models configuration gpu geforce rtx design nvidia driver version cudnn version collect hip runtime version miopen runtime version versions libraries pip numpy pip pip torchaudio pip blas mkl cudatoolkit bb mkl mkl service py mkl fft py mkl random py numpy py aff numpy base py hfa py cuda cudnn torchaudio py py cu,2020-11-23T19:00:17Z,"high priority, module: dependency bug, module: convolution, triaged, module: mkldnn",OPEN,0,2,https://github.com/pytorch/pytorch/issues/48400,"version, mkl, py, cuda, pip, pypi, numpy, gpu, pypi_pypi, cudnn",7,算子返回值不一致,,,,,当输入和内核是 mkldnn 张量时，conv_transpose3d 返回不同的结果
sigill illegal instruction libtorch cpu callling backward function vm,concise description try fl project uses methods crashes calls backward function failed crashed illegal instruction core dumped ubuntu vm works windows normal non vm machine reproduce steps reproduce behavior little bit complicated reproduce behavior simply put attempted test fl algorithm qub blesson fedadapt fix bugs adjust configs set openstack vms master server clients edge devices create fresh venv installed packages following installing ran fl training algorithm everything works reaches function loss backward samples additionanl context section results following gdb debbuging output sample messages traces provide well expected behavior work vm less limited cpu instruction sets without instruction set without gpu support concise description expected happen environment collecting environment information version cpu debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version version libc version glibc version default sep gcc bit runtime platform linux generic glibc cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver,2021-11-15T14:50:51Z,"module: dependency bug, module: crash, module: cpu, triaged, module: mkldnn",OPEN,0,2,https://github.com/pytorch/pytorch/issues/68349,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架特殊环境下出错,,,,,在 Ubuntu VM 中调用pytorch“backward()”函数时失败并崩溃
intel omp multiprocessing assertion failure assertion failure linux util cpp,observe weird assertion happening hours training classifier intel omp one data loader workers fails downgrading intel omp seemingly fixes also found continuumio ananconda issues issues looks present discussed offline environment,2019-12-14T15:37:57Z,"module: dependency bug, module: multiprocessing, triaged",OPEN,0,2,https://github.com/pytorch/pytorch/issues/31289,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子执行失败,,,,,Intel OMP 多处理断言失败：z_Linux_util.cpp 断言失败
onnx output ceil pooling,output onnx exported following dummy model ceil avgpooling output output onnx bigger model remarks happens onnx opset version larger equal versions collecting environment information version cu debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version collect libc version glibc version default gcc bit runtime platform linux microsoft standard wsl glibc cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda hip runtime version miopen runtime version versions libraries pip numpy pip collect,2022-01-20T13:19:37Z,"bug, module: onnx, triaged, onnx-triaged",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/71549,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子输出结果错误,,,,,使用 ceil_mode AvgPooling2d 为虚拟模型导出的 ONNX 文件中的输出形状大小错误
incorrect output remainder function integer tensors,reproduction generated output interesting section look remainder long observe gets output fmod floating point tensor different results according docs remainder sign divisor weekend coming look fixing adding test avoid regression imagine looking around neighbourhood dd fbd cdcd bd dfc place start,2017-03-31T21:05:26Z,"high priority, module: dependency bug",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/1164,"new, thread, org, version, tutorial, assert, cpu, master, doc, internanl",6,算子输出结果错误,,,,,整数类型张量的余数函数输出不正确
logsoftmax numerically unstable cuda,sometimes gives inf,2017-02-12T16:05:18Z,"high priority, module: dependency bug",CLOSED,0,2,https://github.com/pytorch/pytorch/issues/725,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,算子结果不稳定,,,,,LogSoftmax 在 CUDA 上可能数值不稳定
simple function causing graph break,raised calling dynanmo optimize optimizing huggingface gpt enforcing single optimized graph whole model minimal repro transformers tpu available removed works expect dynanmo simple function logs response minified repro response,2022-12-16T18:42:16Z,"bug, triaged",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93486,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子错误,,,,,torch._dynamo.optimize优化 Huggingface GPT2 并为整个模型强制执行单个优化图时调用时会出现错误
aws efa network,opening collect side information needed running aws efa network aws side aws aws ofi nccl repo related bugs efa performance optimization settings aws aws ofi nccl issues default dataloaders crash efa must spawn aws aws ofi nccl issues getting around efa mpirun restrictions dataloader requires switching multiprocessing context spawn latest issues currently limits number workers lower desired,2019-08-02T04:48:55Z,"module: dependency bug, oncall: distributed, triaged",OPEN,0,1,https://github.com/pytorch/pytorch/issues/23721,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,框架异常,,,,,在 AWS EFA 网络上使用 PyTorch出现错误
assertionbug op returned non tensor function len,following fails since ops accesses jit operator registry instead registry possible access many non tensor functions mlir tests fail logs stacktrace gist silvasean ef fdbb minified repro,2022-11-18T15:14:59Z,"bug, oncall: pt2",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/93429,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,算子出错,,,,,AssertionError: torch.* op returned non-Tensor int call_function aten.len
conv memory usage,specific situations batch channels kernel memory usage kernel memory usage mb kernel memory usage reach reproduce test different situations environment version os ubuntu cuda version cudnn version,2019-09-27T03:23:44Z,"module: dependency bug, module: cudnn, module: memory usage, module: convolution, triaged",OPEN,0,1,https://github.com/pytorch/pytorch/issues/26950,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子资源调度不合理,,,,,pytorch 1.1.0的conv2d内存占用太大
performance regression interpolation kornia,compiling rescale function kornia geometry transform rescale conv interpolation dynanmo inductor performance regression cuda smaller constant regression cpu timing results eager vs dynanmo reproduce logs response minified repro response,2022-11-30T00:03:26Z,"bug, triaged",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93455,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,框架性能异常,,,,,Performance regression on interpolation in Kornia
inductor cpu accuracy failure torchbench model detectron fcos fpn,failure found latest torchinductor cpu performance dashboard refresh test issues sw information sw commit master commit blzheng commit bcc bbf abeff bdae blzheng commit fdcda torchbench dfe benchmark commit dfe db bf cd ec torchaudio audio commit adece bb audio commit fcd torchtext text commit ce acb efe text commit efeba cb vision commit acf cbe ffd vision commit ffd abf da detail info reference dashboard issues logs runtimebug storage calculation overflowed sizes minified repro benchmarks dynanmo torchbench py accuracy float dcpu inductor skip dashboard detectron fcos fpn batch,2022-11-17T08:08:38Z,"bug, triaged, oncall: pt2, module: cpu inductor",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93426,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,算子精度问题,,,,,[CPU] torchbench 模型中detectorron2_fcos_r_50_fpn的精度故障 
torchdynanmo capture specified cuda device,reported issues ddp currently requires setting default cuda device rank since otherwise dynanmo compiled program default device incorrectly non ranks,2022-12-14T17:24:37Z,bug,CLOSED,0,1,https://github.com/pytorch/pytorch/issues/93478,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,框架资源调度不合理,,,,,当前使用 DDP 需要为每个等级设置“默认 cuda 设备”，否则 dynamo 编译的程序将在非零等级上错误地使用默认设备 0
umbrella test failures returned non tensor output unimplemented,response logs response minified repro response,2022-12-14T21:35:37Z,"bug, triaged, oncall: pt2",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93479,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子结果出错,,,,,Umbrella issue for PyTorch test suite failures from torch.* returned non-Tensor output unimplemented
support getattr setattr properties tensor,tensor subclasses typically storing properties tensor supported tensorvariable graph breaks making applications tensor subclasses actually work logs response minified repro response,2022-12-01T04:42:19Z,"bug, triaged",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93461,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子功能不完善,,,,,Tensor 上的 getattr/setattr 用户属性
tensorwithtfoverridevariable unwraps early,tensorwithtfoverridevariable method unwraps immediately inlining function logical unwrapping truly happen super inside function disabletorchfunction means recursive calls function handled correctly right logs response minified repro response,2022-12-01T14:19:13Z,"bug, triaged",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93462,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子执行出错,,,,,TensorWithTFOverrideVariable 过早展开 
calling parent class method causes,gives following logs response minified repro response,2022-11-12T10:30:54Z,"bug, oncall: pt2",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/93416,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架异常,,,,,调用父类的方法会出错
strategy optimizing away transient dynanmic shapes device syncs,context xla issues key subtlety although principle perform optimizations sure early graph breaking immediately sync dynanmically sized output occurs may metadata associated tensors computed lazily since compute eagerly require sync also related unbacked symints logs response minified repro response,2022-12-05T22:31:25Z,"bug, triaged, oncall: pt2",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93466,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,算子功能不完善,,,,,Strategy for optimizing away transient dynamic shapes / device syncs
inductor cpu accurary failure timm model poolformer,failure found latest torchinductor cpu performance dashboard issues refresh test log sw information sw commit master commit commit db fc aaa bc commit cbf torchbench benchmark commit bff torchaudio audio commit debd deae audio commit bbf ce decd torchtext ebcfed text commit ebcfed fc text commit vision commit acba fcf eafc vision commit cbeeb fbba logs minified repro,2022-12-01T03:36:18Z,"bug, triaged",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/93459,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,模型精度问题,,,,,[CPU] timm_model poolformer_m36中的精度故障
gcc needs cmake cxx flags gnu,since gcc issues master system gcc met able fix explicitly adding stdlib adding dd ae aefd ff tools setup helpers cmake py maybe submit asmjit repo,2019-11-13T18:39:19Z,"module: dependency bug, module: build, triaged",OPEN,0,1,https://github.com/pytorch/pytorch/issues/29739,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,框架异常,,,,,"gcc 7.4 需要 CMAKE_CXX_FLAGS=""-std=gnu++11"""
accuracy minifier also work even exception raised,accuracy minifier model bugs optimized accuracy minifier nothing accuracy minification superset testing accuracy failures raised exceptions logs response minified repro response,2022-11-21T19:51:56Z,"bug, triaged, oncall: pt2",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93436,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,算子功能不完善,,,,,引发异常时精度缩小器不起作用
hub fails cache network crashes,hub fails cache network crashes trying model hub hub py fails cache network since parse repo info handle network timeout steps reproduce turn internet try following may following exception repo branch isl org midas example test another repo branch pair needed versions,2023-01-01T16:27:26Z,"bug, triaged, module: hub",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/91568,"cuda, device, libtorch, version, window, org, example, release, extension, system",5,算子执行失败,,,,,如果没有网络，torch.hub.load() 无法从缓存加载并崩溃
runtimebug quantile input tensor must either float double dtype,versions chen weiwen,2023-06-06T02:53:30Z,"bug, triaged, oncall: pt2, module: inductor",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/103054,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,算子功能不完善,,,,,quantile() 输入张量必须是 float 或 double dtype
nn parameter dispatch functionanlization,description following produces dispatch functionanlization revealed pytest generated test shinianzhihou changedetection py test issues,2023-06-05T17:35:56Z,"bug, triaged, module: aotdispatch",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/102989,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,算子异常,,,,,nn.Parameter dispatch error in functionalization
prevent pattern matches across mutation ops inductor grad fx passes,inductor grad fx passes inductor fx passes grad py unsafe respect mutation discard matches cross mutation op example repro reveals grad fx passes currently match across mutation ops add test test inductor test pattern matcher py unittest,2023-05-10T20:56:07Z,"bug, triaged, oncall: pt2",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/101124,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,算子fx执行不一致报错,,,,,Prevent pattern matches across mutation ops in Inductor pre-grad FX passes
runtime,hi simpletransformers library following versions collecting environment information version debug cuda rocm os ubuntu lts gcc version collect clang version collect cmake version collect libc version glibc version default jul gcc bit runtime platform linux generic debian buster sid cuda available cuda runtime version collect cuda module loading set gpu models configuration gpu nvidia geforce rtx nvidia driver version cudnn version collect hip runtime version miopen runtime version xnnpack available cpu architecture cpu op bit bit byte little endian cpu cpu thread core core socket socket numa vendor genuineintel cpu family model model nanme intel core tm cpu ghz stepping cpu mhz cpu mhz cpu mhz bogomips virtualization vt cache cache cache cache numa cpu flags fpu vme de pse tsc msr pae mce cx apic sep mtrr pge mca cmov pat pse clflush dts acpi mmx fxsr sse sse ss ht tm pbe syscall nx pdpe rdtscp lm constant tsc art arch perfmon pebs bts rep good nopl xtopology nonstop tsc cpuid aperfmperf pni pclmulqdq dtes monitor ds cpl vmx smx est tm ssse sdbg fma cx xtpr pdcm pcid sse sse apic movbe popcnt tsc deadline timer aes xsave rdrand lahf lm abm dnowprefetch cpuid invpcid single ssbd ibrs ibpb stibp tpr shadow vnmi flexpriority ept vpid,2023-04-25T17:16:43Z,"bug, triaged",OPEN,0,1,https://github.com/pytorch/pytorch/issues/99999,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,框架异常,,,,,使用 simpletransformers 库写入文件失败
randperm respect base,currently randperm returns clearly unexpected behavior needs return numbers range,2017-03-03T17:09:37Z,"high priority, module: dependency bug",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/911,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,算子返回值错误,,,,,"torch.randperm(10)没有返回[1, 10]内的数字"
vision maskrcnn failing periodic trunk,img alt images githubusercontent cabf bc ab ebc failures appear start note also failed bisect vision maskrcnn inductor earlier week looking dash looked drop revisions however bisecting found revs failing message changed point good db cde da bad cb af,2023-04-15T14:05:52Z,"bug, triaged, oncall: pt2",OPEN,0,1,https://github.com/pytorch/pytorch/issues/99230,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,算子执行失败,,,,,vision_maskrcnn failing in periodic/trunk
inductor tests metaprogramming breaks slow tests logic,set slow tests ci nanmes particular inductor dynanmic test slow opts special handling cd test echo test linear packed cpu dynanmic shapes cpp wrapper dynanmicshapescppwrappercputests slow tests json testrunner way skips fast tests runs slow tests observe following exception looks either testmethodnanme set properly copied metaprogrammed test class objects runtest method missing sure unfamiliar way set versions master today metaprogramming existing one inductor tests got flagged recently slow automatically stuck aforementioned json ci chen weiwen,2023-04-12T18:11:25Z,"bug, oncall: pt2, module: inductor",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/98954,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,框架异常,,,,,电感器测试元编程中断 SLOW_TESTS 逻辑
export onnx model custom op raises attributebug function object attribute function proto,exporting model custom op following instructions onnx org docs stable onnx fails attributebug traceback sample reproducing requirement pip install scatter convenient way provide custom op know needs handled via custom symbolic function running raises following versions collecting environment information version debug cuda rocm os centos gcc version gcc red hat clang version collect cmake version version libc version glibc version may gcc bit runtime platform linux amzn glibc cuda available cuda runtime version cuda module loading set lazy gpu models configuration gpu tesla nvidia driver version cudnn version probably one following libcudnn libcudnn adv infer libcudnn adv train libcudnn cnn infer libcudnn cnn train libcudnn ops infer libcudnn ops train hip runtime version miopen runtime version xnnpack available cpu architecture cpu op bit bit byte little endian cpu cpu thread core core socket socket numa vendor genuineintel cpu family model model nanme intel xeon cpu ghz stepping cpu mhz cpu mhz cpu mhz bogomips hypervisor vendor xen virtualization cache cache cache cache numa cpu flags fpu,2023-07-06T11:52:27Z,"high priority, bug, module: onnx, triaged, module: regression, onnx-triaged",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/104700,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,模型导出失败,,,,,Pytorch 2.0 中导出 ONNX 模型会引发 AttributeError：“function”对象没有属性“to_function_proto”
multinomial returns floats gpu,multinomial floattensor cuda normal returning floattensor gpu opposed longtensor cpu,2016-11-27T12:00:32Z,"high priority, module: dependency bug",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/261,"backward, gradient, sparse, version, matrix, focus, run_focus, run, pas, one",15,算子返回值错误,,,,,torch multinomial returns floats on GPU
cuda crossentropyloss,message assertion thindextensor target batch failed however everything put cuda raises runtimebug cuda runtime illegal memory access encountered guismay master thc generic thctensorcopy moreover restart notebook everything involving cuda illegal memory access,2016-11-29T14:49:42Z,"high priority, module: dependency bug",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/266,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子资源调度不合理,,,,,cuda CrossEntropyLoss引发非法内存访问
investigate graph breaks reported torchbench py,torchbench py look stats graph break section graphs breaks seem fixable function userdefinedobjectvariable method constantvariable constantvariable think coming inside nn function builtinvariable userdefinedobjectvariable chain easy add supporting itertools chain debug types things set break environment variable stats message investigate turn chen weiwen,2022-03-31T18:52:05Z,"bug, triaged, oncall: pt2, module: dynamo",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93744,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架异常,,,,,graph breaks reported by ./torchbench.py
minifier crash,minifier launcher gist ab fa acd addc fbb ded latest symbolic shapes branch writing torchdynanmo dynanmic shapes aot dynanmic shapes crashes logs response minified repro response,2022-11-08T04:20:42Z,"bug, triaged, oncall: pt2",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93613,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架异常,,,,,Minifier崩溃
turning minifier causes go away debertaformaskedlm,ab bff df oct breaking commit reverted benchmarks dynanmo huggingface py accuracy backend inductor training debertaformaskedlm fails torchdynanmo repro dynanmo benchmarks dynanmo huggingface py accuracy backend inductor training debertaformaskedlm passes logs internanlfb intern paste minified repro minifier work,2022-10-30T00:37:29Z,"bug, triaged",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93592,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,框架中错误消失,,,,,打开Minifier会导致错误消失（在 DebertaForMaskedLM 上）
eca botnext ts fails torchdynanmo dynanmic shapes sympy infinite loop,repro logs anyone even look minified repro minifier work,2022-10-24T17:09:43Z,"bug, triaged, oncall: pt2",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93576,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子资源调度不合理,,,,,eca_botnext26ts_256 fails with TORCHDYNAMO_DYNAMIC_ SHAPES=1: sympy infinite loop
minifier config variables replicate env vars,today remember minifier launcher py config vars env vars originanl script minifier record configs explicitly maybe changed logs response minified repro response,2022-10-24T15:39:34Z,"bug, triaged, oncall: pt2",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93574,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,框架功能不完善,,,,,Minifier should save config variables
sebotnet ts fails torchdynanmo dynanmic shapes sympy infinite loop,logs gist da ce minified repro minifier work looks works resulting repro actually trigger patching fix minifier commits dfd df abe minor improvements minifier commits fa minifier longer runs,2022-10-24T14:05:51Z,"bug, triaged, oncall: pt2",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93573,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子资源调度不合理,,,,,sebotnet33ts_256 失败，TORCHDYNAMO_DYNAMIC_SHAPES=1：sympy 无限循环
torchdynanmo export work specific constants,logs mnt xarfuse uid fc seed nspid cgpid ns torchdynanmo symbolic convert py mnt xarfuse uid fc seed nspid cgpid ns torchdynanmo symbolic convert py getattr inst opnanme inst mnt xarfuse uid fc seed nspid cgpid ns torchdynanmo symbolic convert py inner unimplemented generic jump typestr mnt xarfuse uid fc seed nspid cgpid ns torchdynanmo exc py unimplemented unsupported msg torchdynanmo exc unsupported generic jump dynanmicshapevariable dynanmo succeed dynanmo optimize eager succeed aot succeed dynanmo optimize aot eager succeed inductor succeed dynanmo optimize inductor succeed minified repro repro,2022-10-20T22:59:25Z,"bug, oncall: pt2",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/93564,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子功能不完善,,,,,torchdynamo.export() 不适用于 torch 特定常量
torchinductor input strided calls dont compose offset inputs,torchinductor compile operators select strided calls signanture strided tensor symint symint stride symint storage offset tensor input non storage offset offset ignored logs dynanmo succeed dynanmo optimize eager succeed aot succeed dynanmo optimize aot eager succeed inductor succeed dynanmo optimize inductor succeed minified repro,2022-10-20T20:35:56Z,"bug, triaged, oncall: pt2",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93562,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子输出错误,,,,,TorchInductor Input As_Strided Calls Dont Compose With Offset Inputs
speech transformer failing torchdynanmo dynanmic shapes runtimebug expand cudabooltype number sizes provided must greater equal number dimensions tensor,cannot minify exception occurs outside dynanmo compiled logs gist cee dd eed dynanmo succeed dynanmo optimize eager succeed aot succeed dynanmo optimize aot eager succeed inductor succeed dynanmo optimize inductor succeed minified repro minifier work example,2022-10-20T15:45:55Z,"bug, triaged, oncall: pt2",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93560,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子执行失败,,,,,expand提供的尺寸数量 (3) 必须大于或等于张量的维数 (4)
lowering conv bugs inductor config tune layout,lowering conv bugs inductor tune layout logs dynanmo succeed dynanmo optimize eager succeed aot succeed dynanmo optimize aot eager succeed inductor succeed dynanmo optimize inductor succeed minified repro,2022-10-19T21:23:16Z,"bug, oncall: pt2",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/93551,"target, test, py, linux, cuda_target, linux_libcudnn, ci, jenkins, build, rocm",3,算子错误消失,,,,,当 torch._inductor.config.tune_layout = True 时，降低 conv1d 错误
warning missing return statement non void function,gcc cuda probably configurations lots warnings nvcc unanble deal combinantion unlikely aka builtin expect dead eliminantion seen simple program combinantion features handled correctly host compiler reported nvidia developer nvidia nvidia,2019-06-05T17:59:49Z,"module: dependency bug, module: cuda",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/21418,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,算子返回值错误,,,,,非 void 函数末尾缺少 return 语句
minifier prints reductions incorrectly,steps reproduce produces repro must quoted work logs response minified repro response,2023-01-18T22:22:28Z,"bug, triaged",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/93512,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子输出结果错误,,,,,Minifier错误地打印缩减
inductor cpu huggingface model mobilebertforquestionannswering performance regression ww,compare ww performance regression huggingface model mobilebertforquestionannswering torchinductor cpu performance dashboard issues ww bellow ww ww batch speedup inductor eager batch speedup inductor eager speedup ratio eager ratio inductor ratio ww sw info sw commit master commit commit da cb ab ca commit cdf efdee torchbench bbdc benchmark commit bbdc bf bda torchaudio ecc audio commit ecc ca dd audio commit ace fa torchtext text commit efd bc cfde fc text commit fc eef ae efe vision commit efed fcbbaa vision commit afb adc torchdata data commit cd af fea fa cecd data commit afa bef dynanmo benchmarks fac commit fac da ab ww sw info sw commit master commit fac commit fac da,2023-01-13T09:36:52Z,"bug, triaged, oncall: pt2, module: cpu inductor",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/93508,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,模型性能下降,,,,,[CPU] Huggingface model MobileBertForQuestionAnswering performance regression > 10% on ww02.4
inductor cpu torchbench model hf albert performance regression ww,compare ww performance regression torchbench model hf albert torchinductor cpu performance dashboard issues ww bellow ww ww batch speedup inductor eager batch speedup inductor eager speedup ratio eager ratio inductor ratio ww sw info sw commit master commit commit da cb ab ca commit cdf efdee torchbench bbdc benchmark commit bbdc bf bda torchaudio ecc audio commit ecc ca dd audio commit ace fa torchtext text commit efd bc cfde fc text commit fc eef ae efe vision commit efed fcbbaa vision commit afb adc torchdata data commit cd af fea fa cecd data commit afa bef dynanmo benchmarks fac commit fac da ab ww sw info sw commit master commit fac commit fac da,2023-01-13T09:17:18Z,"bug, oncall: pt2, module: cpu inductor",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/93506,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,模型性能下降,,,,,[CPU] Torchbench model hf_Albert performance regression > 10% on ww02.4
inductor cpu torchbench model dlrm performance regression ww,compare torchinductor cpu performance dashboard issues ww performance regression torchbench model dlrm ww bellow ww ww batch speedup inductor eager batch speedup inductor eager speedup ratio eager ratio inductor ratio ww sw info sw commit master commit fac commit fac da ab commit fab ff cdc ee torchbench benchmark commit db torchaudio ecc audio commit ecc ca dd audio commit ace fa torchtext text commit efd bc cfde fc text commit fc eef ae efe vision commit efed fcbbaa vision commit afb adc torchdata data commit cd af fea fa cecd data commit afa bef ww sw info sw commit master commit fac commit fac da ab commit fab ff cdc ee torchbench ff,2023-01-12T04:52:08Z,"bug, oncall: pt2, module: cpu inductor",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/93503,"commit, image, rpc, githubusercontent, image_githubusercontent, benchmark, fc, inductor, ab, db",2,模型性能下降,,,,,[CPU] Torchbench model dlrm performance regression > 10% on ww02.3
setup py develop properly recompile version changes,comment originanlly posted torchdynanmo issues issuecomment looks setup py aggressive enough recompiling lead issues one linked comment,2022-05-16T16:27:05Z,"bug, oncall: pt2",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/93754,"reproduce, behavior, expected, concise, description, context, environment, concise_description, message, sample",14,框架编译出错,,,,,setup.py develop当 pytorch 版本更改时无法正确重新编译
catch bugs cell match nested cell,,2022-06-23T19:44:02Z,"bug, triaged, oncall: pt2",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93770,"test, circleci, job, ci, workflow, disabled, failure, pipeline, flaky, failing",12,算子功能不完善,,,,,如果 match_nested_cell 中的cell为空，则捕获值错误
reordering test test induces dynanmo failure,,2022-07-26T14:56:54Z,"bug, triaged, oncall: pt2",OPEN,0,1,https://github.com/pytorch/pytorch/issues/93791,"test, circleci, job, ci, workflow, disabled, failure, pipeline, flaky, failing",12,框架异常,,,,,PyTorch 测试套件中的重新排序测试会导致dynamo故障
batchnorm bessel correction batch,right makes everything nann,2017-04-27T17:47:47Z,module: dependency bug,CLOSED,0,1,https://github.com/pytorch/pytorch/issues/1381,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,算子输出结果错误,,,,,BatchNorm 不应对批量大小为 1 使用贝塞尔修正，会导致全部NaN
dynanmo aot improperly handles dupe via,simple repro new test test dynanmo test aot autograd py based existing test test dupe via dynanmo recompiles many param basically module accepts format forward deal duplicate correctly shows real life dynanmo instead forward since rather nnmodule impl accepts rather typical usercode forward accepts explicit dynanmo aot may handle localinputsource little differently account assertionbug raised expected behavior dynanmo guard failure triggers recompile versions bc af fed fd ed ca bb chen weiwen,2023-02-24T20:38:42Z,"bug, triaged, oncall: pt2, module: dynamo",OPEN,0,1,https://github.com/pytorch/pytorch/issues/95501,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子执行错误,,,,,dynamo+aot 通过 *args 不正确地处理重复参数 
cudnn batchnorm batch limit,cudnn status supported reported discuss org cudnn status supported bach batchnorm,2017-09-30T20:10:19Z,"high priority, module: dependency bug",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/2917,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,算子功能不完善,,,,,CuDNN batchnorm 有batch大小限制
showing profiler tutorials,doc blocks stopped showing profiler tutorial org tutorials intermediate tensorboard profiler tutorial steps block display doc pages downloaded ipython notebook img alt screen shot pm images githubusercontent ef,2022-07-04T19:39:10Z,"bug, module: docs",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/80846,"new, thread, org, version, tutorial, assert, cpu, master, doc, internanl",6,文档不完善,,,,,代码未显示在 Profiler 教程中
segfault new strided,looks insufficient input validation segfaults repro gdb core backtrace versions git commit collect,2022-07-28T17:47:18Z,"bug, module: crash, triaged, module: edge cases",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/82416,"backward, gradient, sparse, version, matrix, focus, run_focus, run, pas, one",15,算子出错,,,,,new_empty_strided的段错误
onnx topk export keepdim onnx,model images githubusercontent cd aea ae aff model images githubusercontent dbc df aad ef ffd export issues exporting keep efficient argmax combined onnxruntime cuda ep extra copy memory argmax another onnxruntime microsoft onnxruntime issues versions collecting environment information version debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version version libc version glibc version packaged default oct gcc bit runtime platform linux azure debian bullseye sid cuda available cuda runtime version gpu models configuration gpu tesla pcie nvidia driver version cudnn version probably one following linux gnu libcudnn linux gnu libcudnn adv infer linux gnu libcudnn adv train linux gnu libcudnn cnn infer linux gnu libcudnn cnn train linux gnu libcudnn ops infer linux gnu libcudnn ops train hip runtime version miopen runtime version xnnpack available versions libraries pip numpy pip pip torchlars pip blas mkl cudatoolkit bc ffmpeg hf mkl mkl service py mkl fft py mkl random py,2022-04-25T22:06:43Z,"bug, module: onnx, triaged, onnx-triaged",OPEN,0,1,https://github.com/pytorch/pytorch/issues/76344,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子资源调度不合理,,,,,使用topk导出max到onnx额外复制到内存
onnx scripted reshape incorrect dynanmically calculated,onnx export produces incorrect export reshape function scripting calculated dynanmically looks one arguments converted integer float instead onnx images githubusercontent bd ffda cd ede versions version cu debug cuda rocm os ubuntu lts gcc version ubuntu ubuntu clang version collect cmake version version libc version glibc version apr gcc bit runtime platform linux generic glibc cuda available cuda runtime version gpu models configuration gpu nvidia geforce rtx ti nvidia driver version cudnn version probably one following linux gnu libcudnn linux gnu libcudnn linux gnu libcudnn adv infer linux gnu libcudnn adv train linux gnu libcudnn cnn infer linux gnu libcudnn cnn train linux gnu libcudnn ops infer linux gnu libcudnn ops train hip runtime version miopen runtime version xnnpack available versions libraries pip mypy extensions pip numpy pip pip numpy pypi pypi pypi pypi pypi pypi,2022-06-02T12:38:50Z,"bug, module: onnx, triaged, onnx-triaged",OPEN,0,1,https://github.com/pytorch/pytorch/issues/78721,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子出错,,,,,如果动态计算形状，则 torch.onnx.export 在编写脚本后会生成不正确的 reshape 函数调用导出
validate sparse bsr tensor fails non trivial block cases,problem validate sparse bsr tensor take account specified blocksize interpreting bsr tensor versions master branch,2022-05-25T16:36:40Z,"module: sparse, topic: bug fixes",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/78264,"backward, gradient, sparse, version, matrix, focus, run_focus, run, pas, one",15,算子执行失败,,,,,torch._validate_sparse_bsr_tensor_args fails on non-trivial block size cases
failure guard fail fn callback raising null guard eval traceback tiger packages dynanmo guards py guard fail hook guard fn guard fail fn guardfail reason orig,compile class model def init def forward return process def process dynanmo guards failure guard fail fn callback raising null guard eval traceback tiger packages dynanmo guards py guard fail hook guard fn guard fail fn guardfail reason orig typebug nonetype object callable versions dynanmo guards failure guard fail fn callback raising null guard eval traceback tiger packages dynanmo guards py guard fail hook guard fn guard fail fn guardfail reason orig typebug nonetype object callable chen weiwen,2022-12-08T17:22:47Z,"high priority, triage review, bug, oncall: pt2, module: dynamo",CLOSED,0,1,https://github.com/pytorch/pytorch/issues/90480,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子异常,,,,,Guard_fail_fn 导致 Guard eval Traceback 出现 NULL 错误
elastic rendezvous matches machine hostnanme resolve hostnanmes,steps reproduce behavior launch kubernetes volcano loglevel info rdzv backend rdzv rdzv endpoint vc sh hosts nnodes echo hello rendezvous times since rank host realize master due insufficient hostnanme resolution hostnanme sh db kkt vd sh volcano gives addresss sh db kkt vd sh sh db kkt vd hosts resolve conf hostnanme information required realize addresses equivalent current logic sufficient efbe ee bae ff elastic rendezvous py may dns resolution address matches ip addresses versions os linux linux sh db kkt vd sh amzn smp wed aug utc gnu linux installed torchelastic pip source docker docker docker tag docker torchx pkgs container torchx tag dev command compiling source git commit installed source version cuda cudnn version gpu models configuration execution environment prem aws eks volcano information,2022-02-24T20:51:50Z,"triaged, module: elastic, oncall: r2p, topic: bug fixes",OPEN,0,1,https://github.com/pytorch/pytorch/issues/73380,"version, reproduce, behavior, source, model, environment, pip, expected, expected_behavior, installed",4,算子执行失败,,,,,_matches_machine_hostname无法完全解析主机名
adding batched csr tensors different sparsities produces invalid tensor,description adding batched csr tensors sparsities produces correct results also adding csr tensors without batch dimensions different sparsities produce correct results example expected result system info version,2023-08-18T19:45:41Z,"bug, module: sparse, triaged, module: correctness (silent)",OPEN,0,0,https://github.com/pytorch/pytorch/issues/107478,"new, thread, org, version, tutorial, assert, cpu, master, doc, internanl",6,框架异常,,,,,添加具有不同稀疏度的批量 CSR 张量会产生无效张量
conversion coo sparse dimensions csr dense specified fails,description specifying dense conversion leads expected result example expected behavior exception following result system info version,2023-08-18T11:32:03Z,"bug, module: sparse, feature, triaged",OPEN,0,0,https://github.com/pytorch/pytorch/issues/107451,"behavior, result, reproduce, expected, cpu, numpy, expected_behavior, index, different, detail",13,算子返回值错误,,,,,dense_dim未在转换中指定会导致无法预料的结果
mis annotated return grad embedding renorm also jit related,sub facing involves functions prefixed underscore grad embedding renorm mismatched typing return bdb dd eaaffbcd nn functionanl py function return anything modified purpose ps return anyway return got annotated tuple tensor tensor know changed way explain author pr occurrences grad embedding renorm quite sure whether related correct bdb dd eaaffbcd csrc api include nn functionanl embedding bdb dd eaaffbcd csrc jit runtime register special ops cpp bdb dd eaaffbcd csrc jit runtime serialized function registry cpp bdb dd eaaffbcd jit functions py versions master,2023-06-17T11:33:55Z,"bug, oncall: jit, module: typing, triaged",OPEN,0,0,https://github.com/pytorch/pytorch/issues/103800,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,算子返回值错误,,,,,F._no_grad_embedding_renorm_不返回任何内容
nn embedding acting strange,hi trying feature nn embedding noticed seems change way indexing works correct happens seen seems embedding corresponding instead requested misunderstanding something,2017-03-07T01:34:07Z,"high priority, module: dependency bug",CLOSED,0,0,https://github.com/pytorch/pytorch/issues/943,"nn, output, forward, input, layer, weight, float, embedding, padding, batch",9,算子出错,,,,,the max-norm feature of nn.Embedding seems to change the way indexing works
asarray respect set default device,asarray respecting set default device tensor creation functions ran process testing scipy progress support tensors comment scipy scipy issuecomment asarray function quite important allows writing library agnostic asarray one heavily functions test suites versions environment details details details originanl author believe asarray,2023-08-08T11:34:51Z,"bug, triaged, module: numpy",CLOSED,0,0,https://github.com/pytorch/pytorch/issues/106773,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,算子异常,,,,,torch.asarray does not respect set_default_device
getting indices sparse compressed tensor results tensors requires grad,description tensors floating point dtype require gradients example expected crow indices requires grad system info version,2023-08-12T10:47:14Z,"bug, module: sparse, triaged, module: correctness (silent)",CLOSED,0,0,https://github.com/pytorch/pytorch/issues/107083,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,算子异常,,,,,获取稀疏压缩张量结果张量的索引，requires_grad 为 True
sparse compressed tensor values autograd support implemented,description example expected behavior exception return exception message explained ddba aafa aaf tools autograd derivatives yaml defines sparse csr related leads calling sparse coo tensor specific indices system info version,2023-08-16T11:20:51Z,"bug, module: sparse, module: autograd, triaged",OPEN,0,0,https://github.com/pytorch/pytorch/issues/107286,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,框架功能不完善,,,,,未实现稀疏压缩张量值 autograd 支持
nllloss cuda tensors,nllloss seem inputs still returns runtimebug assertion input target input target input target failed mismatch got input target loss output cuda target cuda variable containing know losses affected,2017-02-12T07:10:46Z,"high priority, module: dependency bug",CLOSED,0,0,https://github.com/pytorch/pytorch/issues/722,"tensor, input, dimension, float, return, dtype, cuda, version, nann, element",18,算子功能不完善,,,,,NLLLoss2d 不检查 cuda 张量的形状
matrix multiplication performance regression additionanl dimension,unsqueeze unsqueeze times slower depending dimensions matrices reproducing happens machine nvidia geforce rtx ti difference execution times always item evaluates similar machine versions,2023-05-28T22:16:48Z,"module: dependency bug, triaged, module: cublas",OPEN,0,0,https://github.com/pytorch/pytorch/issues/102459,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,算子性能异常,,,,,Matrix multiplication performance regression in case of an additional dimension of size 1
update nt take account strides,summary since nt input contiguous numels input match buffer allow unanry ops transposed inputs also update nt construct output tensor stride offset information,2022-12-29T15:53:20Z,"bug, triaged, module: nestedtensor",OPEN,0,0,https://github.com/pytorch/pytorch/issues/91508,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,算子功能不完善,,,,,map_nt不检查输入是否连续，而仅检查输入的数字与缓冲区的数字是否匹配
intermittent inductor segfault ci test ops py,actions runs jobs filing tracking appears intermittent trunk confirmed retroactively showing week less day probably gonnan hard find unless info logs chen weiwen,2023-04-15T03:53:23Z,"bug, triaged, oncall: pt2, module: inductor",OPEN,0,0,https://github.com/pytorch/pytorch/issues/99222,"test, circleci, job, ci, workflow, disabled, failure, pipeline, flaky, failing",12,框架异常,,,,,intermittent inductor segfault in CI
opacus cifar fails dynanmo due hooks,adding support hooks allowed modules via graph breaks opacus cifar crashes dynanmo tracing benchmarks dynanmo torchbench py opacus cifar backend aot eager accuracy inference looks crash happening tracing exception handling relating hooks gradsamplemodule opacus chen weiwen,2023-04-14T21:04:26Z,"bug, triaged, oncall: pt2, module: dynamo",OPEN,0,0,https://github.com/pytorch/pytorch/issues/99201,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子执行失败,,,,,opacus_cifar10 由于hook而在 dynamo 中失败
gather tensor typebug,gather typebug tensors tensor gathered tensors otherwise silent gathered tensors equivalent tensor process following reproduces gpus nccl backend output follows initialized gather fp float actual gathered tensors fp half seen outputs gathered tensor skewed bugs warnings regarding makes silent suspect occurs fp tensors read fp types still think warning tested nvidia ngc output collect env py environment versions version debug cuda rocm os ubuntu lts gcc version gcc clang version collect cmake version collect libc version glibc version packaged dec gcc bit runtime platform linux generic glibc cuda available cuda runtime version gpu models configuration gpu nvidia geforce rtx ti gpu nvidia geforce rtx ti gpu nvidia geforce rtx ti gpu nvidia geforce rtx ti nvidia driver version cudnn version probably one following linux gnu libcudnn linux gnu libcudnn linux gnu libcudnn adv infer linux gnu libcudnn adv train linux gnu,2022-09-04T16:51:03Z,"bug, oncall: distributed, triaged",CLOSED,0,0,https://github.com/pytorch/pytorch/issues/84525,"version, cuda, runtime, gpu, pip, collect, linux, nvidia, gcc, ubuntu",17,算子功能不完善,,,,,torch.distributed.all_gather在错误类型的张量列表上应该提出一个TypeError
umbrella populate real cache export test fails,introduced skips logs response minified repro response,2022-12-14T21:42:03Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93480,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架异常,,,,,Umbrella issue for only populate real_value_cache in export test suite fails
umbrella weakref related dynanmo test failures,collected logs response minified repro response,2022-12-14T22:03:27Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93481,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架异常,,,,,Umbrella issue for weakref related Dynamo PyTorch test suite failures
guard crashes process null users ezyang dev metal csrc dynanmo eval frame,crashing processes makes sres unhappy crash propagate exception back annoying lookup function eval frame already uses null return something different,2022-12-15T10:53:05Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93483,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,接口执行报错,,,,,Error in guard code crashes process
triton autotuning cache clearing adds mb memory overhead,inductor autotuning invoke triton bench internanlly allocates mb tensor openani triton ecb triton testing py cache practical standpoint mb insignificant machine however rtx overhead additionanlly non linear factor throws memory usage stats small batch sizes big effect torchbench performance dashboard low batch sizes resnet compression autotuning without pass handle allocated memory autotuned kernel avoid additionanl allocation logs response minified repro response,2023-01-13T22:57:19Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93509,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,框架资源调度不合理,,,,,Triton 自动调整缓存清除增加了 256MB 内存开销
iter tensorvariable fail,dynanmo support iterating thru tensor variable far dynanmo fail even dimension tensorvariable static fix simple since unroll loop generated items calling getitem problem becomes complex dimension tensor variable dynanmic discussions shunting thoughts possible solutions rewrite explicitly let pass information indicating dimension dynanmic assume dimension static specialize compiling fx graph add guard guard set underlying problem handle loop potentially variable number iterations fail traced internanlfb fbsource ca eae efd bb cbbff fbcode vision fair detectron detectron export py lines logs response minified repro response,2023-01-20T23:08:25Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93514,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,框架功能不完善,,,,,Dynamo 不支持迭代张量变量
dynanmo scalar tensor item,rcnn model detectron support internanlfb fbsource ca eae efd bb cbbff fbcode vision fair detectron detectron export py lines logs traceback shunting learn dynanmo temp py module gm guards export inp graph tracing symbolic shunting dynanmo eval frame py export result traced opt kwargs shunting dynanmo eval frame py fn return fn kwargs shunting dynanmo eval frame py catch bugs return callback frame cache hooks shunting dynanmo convert frame py fn return fn kwargs shunting dynanmo convert frame py convert frame assert return compile shunting dynanmo py wrapper func kwargs shunting dynanmo convert frame py compile transform object transform shunting dynanmo bytecode transformation py transform object transformations instructions options shunting dynanmo convert frame py transform tracer shunting dynanmo symbolic convert py super shunting dynanmo symbolic convert py shunting dynanmo symbolic convert py getattr inst opnanme inst shunting dynanmo symbolic convert py wrapper return inner fn inst shunting dynanmo symbolic convert py function function fn shunting,2023-01-21T01:16:11Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93515,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架功能不完善,,,,,Dynamo 无法跟踪 'int(a_scalar_tensor.item())'
custom op cannot included fx graph captured torchdynanmo,custom op registered via registration mechanism cannot included fx graph captured torchdynanmo logs graph jit includes ipex convolution forward graph dynanmo include got warnning graph break inline skipfiles graph jit graph fx graph module mangle graphmodule tensor conv ctx classes ipex prepack convolutionopcontext prim constant object conv weight tensor prim constant tensor conv bias tensor prim constant cpufloattype tensor prim callmethod nanme data handle conv ctx module conv tensor prim profile profiled float strides requires grad device cpu seen tensor prim profile profiled float strides requires grad device cpu seen conv weight tensor prim profile profiled float strides requires grad device cpu seen conv bias tensor prim profile profiled long strides requires grad device cpu seen input tensor ipex convolution forward module conv jiayisun ananconda envs ipex packages ops py tensor prim profile profiled float strides requires grad device cpu seen input tensor relu jiayisun ananconda envs ipex packages nn functionanl py tensor prim profile profiled float strides requires grad device cpu seen prim profile return graph dynanmo jit backend torchdynanmo symbolic convert warning graph break inline skipfiles forward jiayisun gitlab frameworks ai ipex cpu release packages intel extension nn weight prepack py eval key forward conv conv graph fx graph module mangle,2022-10-20T02:46:56Z,"bug, triaged, oncall: pt2, module: cpu inductor",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93554,"py, nn, package, functionanl, module, nn_functionanl, conv, loss, inductor, ananconda",11,算子异常,,,,,自定义操作无法包含在 torchdynamo 捕获的 FX 图表中 
optforcausallm failing torchdynanmo dynanmic shapes unpack sequence assertionbug assert len items inst argval,logs gist db ae dbcf dynanmo succeed dynanmo optimize eager succeed aot succeed dynanmo optimize aot eager succeed inductor succeed dynanmo optimize inductor succeed minified repro minifier work minifier ran resulting program actually fail,2022-10-20T15:58:53Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93561,"target, test, py, linux, cuda_target, linux_libcudnn, ci, jenkins, build, rocm",3,框架异常,,,,,minifier不起作用
assertionbug abcmeta,turn suppression test sort test sort logs nan dynanmo succeed dynanmo optimize eager succeed aot succeed dynanmo optimize aot eager succeed inductor succeed dynanmo optimize inductor succeed minified repro nan,2022-10-21T04:17:55Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93565,"target, test, py, linux, cuda_target, linux_libcudnn, ci, jenkins, build, rocm",3,算子数值nan assert报错,,,,,AssertionError: ABCMeta
traceable tensor subclasses cannot actually aotautograd,patch fails problem function eliminanted inputs compiler directly pass tensor subclass even eliminanted input compiler also eliminante runtime input compiled function reconstruct subclasses output eager accidentally works tests trivial inner traced function identical outer function calls conventionanl function logs response minified repro response,2022-12-01T14:25:24Z,"bug, triaged",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93463,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,框架功能不完善,,,,,Traceable tensor subclasses cannot actually be used with AOTAutograd
way accuracy minifier one particular subgraph,accuracy minifier may stuck early subgraph may helpful ask minifier subgraphs one suspect problematic sort ablation find subgraphs failing accuracy discard ones accuracy minifier work logs response minified repro response,2022-11-30T03:55:04Z,"bug, triaged",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93456,"onnx, utm, source, caffe, export, model, doc, minifier, gh, quantized",19,框架性能异常,,,,,精度缩小器可能会卡在早期子图上
notimplementedbug autogradfunctionvariable constant xformers,failure seems happen facebookresearch xformers fd fc ae xformers ops memory efficient attention py logs repro,2022-11-16T10:51:08Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93423,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子异常,,,,,NotImplementedError: AutogradFunctionVariable() is not a constant
dynanmo allow graph work built operators,dynanmo allow graph allow operator lt fx graph optimize configuration prevent break example assert fail due breaks instead versions chen weiwen,2022-11-08T23:27:15Z,"bug, triaged, module: dynamo",CLOSED,0,0,https://github.com/pytorch/pytorch/issues/88708,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,算子功能不完善,,,,,torch._dynamo.allow_in_graph 不适用于 python 内置运算符
torchdynanmo properly setting input tracking symbolic guards view bases,convert real tensor fake tensor originanl tensor differentiable view also convert base fake tensor construct fake tensor also looks view dynanmic shapes means allocate symbolic sizes base well view turned circumstances producing guards base problematic dynanmo considers view new tensor inputs tracks symints assert fails typically dynanmoguardprinter ef bf dc dfc commits ef bf dc dfc problem less likely occur suppressing guards occur creating fake tensors since nanughty guard observed practice caused strided setup view performing bounds better fix properly especially since still failed find sx tests general idea whenever allocate symbolic integers tensor always set environment however difficult tell many inputs tensor actually base field also grad field may easiest fake tensor converter communicates information via callback also says attribute appropriate source tensor logs response minified repro response,2022-11-12T05:26:58Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93419,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,框架异常,,,,,torchdynamo 未正确设置视图基础的输入跟踪
ptx codegen race,ci log actions runs jobs logs response minified repro response,2022-11-25T14:47:23Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93451,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,框架对TVM不支持,,,,,
segmentation triton ptx codegen cudnn api eca halonext ts,running inductor cudnn api cudnn api enanbled segfault appears triton compilation repro command cuda launch blocking cudnn api enanbled timm models py amp training performance dashboard device cuda inductor eca halonext ts observed running without cudnn api enanbled may succeed reproduce torchinductor cleared logs minified repro minifier command torchdynanmo repro dynanmo cuda launch blocking cudnn api enanbled timm models py amp training performance dashboard device cuda inductor eca halonext ts created intermediate repro py script taking multiple hours without stopping stopped finished completely note torchinductor also cleared repro,2022-11-19T04:03:49Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93435,"memory, cuda, cudnn, process, gpu, model, convolution, training, version, batch",10,框架出错,,,,,使用 cuDNN V8 API 和 Triton PTX 代码生成中的分段错误eca_halonext26ts
accuracy minifier find spurious accuracy failures involving uninitialized memory,suppose graph suppose accuracy minifying removing accuracy failure uninitialized memory trigger difference oops quick dirty way remove non determinism uninitialized cuda memory sort proper fix logs response minified repro response,2022-11-21T20:36:26Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93437,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子异常,,,,,精度缩小器可以发现涉及未初始化内存的虚假精度故障
partitioner require functionanlized graph,partitioner require functionanlized graph allow us easily ablate functionanlization debugging issues caused functionanlization logs response minified repro response,2022-11-21T20:40:53Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93438,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,框架功能不完善,,,,,应该有一个不需要功能化图的分区器
aotautograd generates useless tangent inputs symint outputs,aotautograd graph returns joint forward backward take tangent argument useless also tangent specified specific symbolic fact always instead autograd engine annoying fix must determine tangents pass outside joint forwards backwards calculation actually needs tangents inside joint forward backwards hoist logs response minified repro response,2022-11-23T04:21:08Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93442,"nn, output, forward, input, layer, weight, float, embedding, padding, batch",9,算子异常,,,,,AOTAutograd generates useless tangent inputs for SymInt outputs
minifier try forward fails set fwd,explanantory logs response minified repro response,2022-10-24T15:53:16Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93575,"repro, log, response, minified, minified_repro, install, org, download, cu, log_response",0,算子功能不完善,,,,,"Minifier should try forward only, and if it fails set fwd_only=True"
testing tensor incorrectly clamps high integral inputs,low high clamped ffb db fcbfb testing creation py however upper limit randint exclusive thus iinfo dtype never sampled,2023-03-07T08:59:50Z,"bug, triaged, module: testing",CLOSED,0,0,https://github.com/pytorch/pytorch/issues/96178,"tensor, issue, operation, feature, add, non, split, function, method, contiguous",1,算子错误,,,,,torch.testing.make_tensor incorrectly clamps high for integral inputs
typo torchgen model py,hitting following dc bb torchgen model py exception valuebug invalid format specifier raised correct docs org library string format string syntax,2022-05-13T11:01:25Z,"triaged, module: codegen, topic: bug fixes",CLOSED,0,0,https://github.com/pytorch/pytorch/issues/77412,"function, grad, autograd, work, set, parameter, variable, requires, support, data",8,模型异常,,,,,torchgen/model.py中的拼写错误
nccl watchdog failed crash program collective times,processgroupnccl watchdog observed failing crash program collective times leads stuck without noticed hence wastes machine capacity example program hang py expected hang since rank elements reduce launch command output although log reports timeout claims take process seems torchx notified runtimebug caught torchx versions master today,2023-03-20T22:18:20Z,"bug, triaged, module: c10d",CLOSED,0,0,https://github.com/pytorch/pytorch/issues/97191,"new, thread, org, version, tutorial, assert, cpu, master, doc, internanl",6,框架资源调度不合理,,,,,NCCL watchdog failed to crash program when collective times out
desync debugger encounters tracemap,internanl reported following seems desync debugger missed log start collective hence logging completion failed find match versions master today huang,2023-03-21T18:17:42Z,"bug, oncall: distributed, triaged, module: c10d",OPEN,0,0,https://github.com/pytorch/pytorch/issues/97268,"new, thread, org, version, tutorial, assert, cpu, master, doc, internanl",6,框架异常,,,,,异步调试器遇到 TraceMap 错误
minifier dumps checkpoints actually reproduce,steps reproduce checkout bd eaf writing tip symbolic shapes branch checkout fda dfde torchdynanmo writing tip accuracy minifier branch intentionanlly introduce failure trigger try minify cd torchdynanmo torchdynanmo repro dynanmo show cpp stacktraces aot fx graphs joint torchdynanmo dynanmic shapes aot dynanmic shapes benchmarks torchbench py bert accuracy backend aot eager training produces checkpoint minifier ezyang checkpoints py however script runs without reproducing looks finanl repro py reproduce confused checkpoint,2022-09-27T20:43:41Z,"bug, triaged, oncall: pt2, module: minifier",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93694,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架出错,,,,,Minifier dumps checkpoints which don't actually reproduce the error
specialization drq guard nn modules,enanble guard nn modules facebookresearch torchdynanmo ef df ebf ff torchdynanmo config py drq ops captured graphs note torchbench py warmups set capture looks guard failing causing recompiles torchdynanmo guard fails facebookresearch torchdynanmo ef df ebf ff torchdynanmo guards py seems guard mod outputs keys set failing debug one figure going ideas perhaps alarm guard times strange perhaps generating incorrect guard perhaps guarding outputs seems something write torchdynanmo thinks perhaps actually guard perhaps real mutation convert unspecialized facebookresearch torchdynanmo ef df ebf ff torchdynanmo variables nn module py actually ways handling nn module dynanmo specialized unspecialized dynanmo sees nn modules created mutated transparently switch unspecialized handling treats module defined class allows mutation,2022-03-16T03:32:11Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93730,"dynanmo, module, py, compile, fn, model, convert, graph, guard, frame",16,框架异常,,,,,Over-specialization with drq and guard_nn_modules=True
issues custom types defining new,meta internanl example output,2022-07-22T23:59:58Z,"bug, triaged, oncall: pt2",OPEN,0,0,https://github.com/pytorch/pytorch/issues/93790,"new, thread, org, version, tutorial, assert, cpu, master, doc, internanl",6,框架出错,,,,,自定义类型定义的问题__new__